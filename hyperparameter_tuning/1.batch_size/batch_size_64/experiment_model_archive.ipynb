{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"gpuType":"V5E1","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13781504,"sourceType":"datasetVersion","datasetId":8772259}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"adc4842c","cell_type":"markdown","source":"# Hyperparameter Tuning - Batch Size 128 Experiment [Kaggle Codespace Version]\n\nAfter completing ablation experiments, we proceed to conduct hyperparameter tuning to further enhance the model's performance. The architecture is defined as per the 'experiment_model.py' file, and we utilise functions created in the 'experiment_utils.py' file to facilitate the training process.\n\nThe flow of this notebook is similar to that of previous components, but with modifications to accommodate the hyperparameter tuning requirements. The loss values are recorded for upcoming analysis, visualization and cross comparison.\n\nThis notebook is also incorporated to be able to run seamlessly in Kaggle's Codespace environment.","metadata":{"id":"adc4842c"}},{"id":"b67f9107","cell_type":"code","source":"################\n# KAGGLE SETUP #\n################\n\ndata_set_name = 'batch-size-128' # Input this dataset name\nkaggle_dir = f'/kaggle/input/{data_set_name}/'\n\nimport sys\nsys.path.insert(0, kaggle_dir)\n\nimport experiment_utils2 as fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:02.390551Z","iopub.execute_input":"2025-11-18T19:26:02.390744Z","iopub.status.idle":"2025-11-18T19:26:05.152902Z","shell.execute_reply.started":"2025-11-18T19:26:02.390720Z","shell.execute_reply":"2025-11-18T19:26:05.152132Z"}},"outputs":[],"execution_count":1},{"id":"477b41b4","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport optax\nimport sys\nimport jax\nimport time\nimport os\nimport numpy as np\nfrom pathlib import Path","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"477b41b4","outputId":"5447b332-c98d-48e5-9f32-dc33a2170433","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:05.154439Z","iopub.execute_input":"2025-11-18T19:26:05.155071Z","iopub.status.idle":"2025-11-18T19:26:05.158891Z","shell.execute_reply.started":"2025-11-18T19:26:05.155051Z","shell.execute_reply":"2025-11-18T19:26:05.158189Z"}},"outputs":[],"execution_count":2},{"id":"656e4403","cell_type":"markdown","source":"# Load the Experiment Setup","metadata":{"id":"656e4403"}},{"id":"1f18b286","cell_type":"markdown","source":"## Set the relevant directory paths & update.log file\n\nHere, we set the necessary directories and output paths for saving model checkpoints and training logs.","metadata":{"id":"1f18b286"}},{"id":"2168d1a9","cell_type":"code","source":"training_log_file = \"training_results.log\"\nvalidation_log_file = \"validation_results.log\"\ncheckpoint_file = \"checkpoint.pkl\"","metadata":{"id":"2168d1a9","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:05.160251Z","iopub.execute_input":"2025-11-18T19:26:05.160493Z","iopub.status.idle":"2025-11-18T19:26:05.181889Z","shell.execute_reply.started":"2025-11-18T19:26:05.160475Z","shell.execute_reply":"2025-11-18T19:26:05.181160Z"}},"outputs":[],"execution_count":3},{"id":"a89050a0","cell_type":"code","source":"if not os.path.exists(training_log_file):\n    fn.initialize_training_log(training_log_file)\n\nif not os.path.exists(validation_log_file):\n    fn.initialize_validation_log(validation_log_file)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a89050a0","outputId":"942f4710-a759-4a02-8bb6-06874e0968c8","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:20.360403Z","iopub.execute_input":"2025-11-18T19:26:20.361085Z","iopub.status.idle":"2025-11-18T19:26:20.365903Z","shell.execute_reply.started":"2025-11-18T19:26:20.361057Z","shell.execute_reply":"2025-11-18T19:26:20.365200Z"}},"outputs":[{"name":"stdout","text":"[initialize_training_log] Initialized training log file at training_results.log\n[initialize_validation_log] Initialized validation log file at validation_results.log\n","output_type":"stream"}],"execution_count":4},{"id":"da2df4b7","cell_type":"markdown","source":"## Load the experiment configurations\n\nPrior to running this notebook, the experiment configurations will be set in a 'config.json' file, which will be loaded to set the model hyperparameters and training settings.","metadata":{"id":"da2df4b7"}},{"id":"f290991b","cell_type":"code","source":"config = fn.load_config(f\"{kaggle_dir}config.json\")\n\nprint(f\"We will be conducting {config['description']}.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f290991b","outputId":"92cea26f-f947-47d1-e88e-8b979827c139","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:23.705358Z","iopub.execute_input":"2025-11-18T19:26:23.705640Z","iopub.status.idle":"2025-11-18T19:26:23.711158Z","shell.execute_reply.started":"2025-11-18T19:26:23.705617Z","shell.execute_reply":"2025-11-18T19:26:23.710437Z"}},"outputs":[{"name":"stdout","text":"[load_config] Loaded configuration from /kaggle/input/batch-size-128/config.json\nWe will be conducting  Hyperparameter tuning for Batch Size = 32.\n","output_type":"stream"}],"execution_count":5},{"id":"31bb4815","cell_type":"code","source":"# Load the seed\nseed = config['seed']\n\n# Model parameters\nvocab_size = config['model']['vocab_size']\nd_model = config['model']['d_model']\nn_heads = config['model']['n_heads']\nn_layers = config['model']['n_layers']\nmlp_ratio = config['model']['mlp_ratio']\nseq_len = config['model']['seq_len']\n\n# Training parameters\nloss_type = config['model']['loss_type']\ndropout_rate = config['model']['dropout']\nweight_decay = config['model']['weight_decay']\nlabel_smoothing = float(config['model']['label_smoothing'])\n\n# Mixed precision and other model settings\nuse_mixed_precision = config['model']['mixed_precision']\npos_encoding = config['model']['pos_encoding']\nattention_type = config['model']['attention_type']\n\n# Auxiliary loss settings\nuse_auxiliary_loss = config['model']['use_auxiliary_loss']\naux_heads = config['model']['aux_heads']\naux_weight = config['model']['aux_weight']","metadata":{"id":"31bb4815","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:25.690393Z","iopub.execute_input":"2025-11-18T19:26:25.690988Z","iopub.status.idle":"2025-11-18T19:26:25.696097Z","shell.execute_reply.started":"2025-11-18T19:26:25.690962Z","shell.execute_reply":"2025-11-18T19:26:25.695356Z"}},"outputs":[],"execution_count":6},{"id":"cf23122e","cell_type":"code","source":"# Throughput test parameters\nmax_test_iters = config['throughput']['max_test_iters']\nmax_test_time_in_seconds = config['throughput']['max_test_time_in_seconds']\ncompute_budget_hours = config['throughput']['compute_budget_hours']\n\n# Training settings\nval_fraction = config['training']['val_fraction']\nbatch_size = config['training']['batch_size']\nlearning_rate = config['training']['learning_rate']\nlr_schedule = config['training']['lr_schedule']\noptimizer_type = config['training']['optimizer']\ngrad_clip = config['training']['grad_clip']","metadata":{"id":"cf23122e","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:28.051568Z","iopub.execute_input":"2025-11-18T19:26:28.051899Z","iopub.status.idle":"2025-11-18T19:26:28.056657Z","shell.execute_reply.started":"2025-11-18T19:26:28.051876Z","shell.execute_reply":"2025-11-18T19:26:28.055965Z"}},"outputs":[],"execution_count":7},{"id":"4030d754","cell_type":"markdown","source":"# Loading the Data\n\nThe same text8 dataset is used, which has 100M characters of text data from Wikipedia articles. It contains only lowercase letters and spaces, and is already pre-split into 90M characters for training and 10M characters for testing.","metadata":{"id":"4030d754"}},{"id":"75d76b31","cell_type":"code","source":"# Read in training text file\nwith open(f\"{kaggle_dir}text8_train.txt\", 'r', encoding='utf-8') as f:\n    train_text = f.read()\nprint(f\"Training text loaded. Length: {len(train_text) :,} characters.\")\n\n# Inspect first 500 characters of training text\nprint(\"First 500 characters of training text:\")\nprint(train_text[:500])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75d76b31","outputId":"f7eef75b-29f8-4ea9-f9e0-7b219821686e","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:30.053855Z","iopub.execute_input":"2025-11-18T19:26:30.054497Z","iopub.status.idle":"2025-11-18T19:26:30.756446Z","shell.execute_reply.started":"2025-11-18T19:26:30.054470Z","shell.execute_reply":"2025-11-18T19:26:30.755708Z"}},"outputs":[{"name":"stdout","text":"Training text loaded. Length: 90,000,000 characters.\nFirst 500 characters of training text:\n anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n","output_type":"stream"}],"execution_count":8},{"id":"7fd37a2d","cell_type":"code","source":"chars = sorted(set(train_text)) # unique characters in training text\nchars_to_int = {ch: i for i, ch in enumerate(chars)} # char to int mapping\nint_to_chars = {i: ch for i, ch in enumerate(chars)} # int to char mapping\n\nprint(f\"Unique characters in training text: {len(chars)}\") # should be 27, including space (sanity check)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fd37a2d","outputId":"0ef808be-a050-47bc-950d-bdb25903339d","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:33.439543Z","iopub.execute_input":"2025-11-18T19:26:33.439814Z","iopub.status.idle":"2025-11-18T19:26:34.085334Z","shell.execute_reply.started":"2025-11-18T19:26:33.439792Z","shell.execute_reply":"2025-11-18T19:26:34.084401Z"}},"outputs":[{"name":"stdout","text":"Unique characters in training text: 27\n","output_type":"stream"}],"execution_count":9},{"id":"032e68f0","cell_type":"markdown","source":"We further split the training data into a training set and a validation set to monitor the model's performance during training, in accordance to the validation fraction specified in our configuration file (10%)","metadata":{"id":"032e68f0"}},{"id":"9a969636","cell_type":"code","source":"train_text, val_text = fn.split_train_val(train_text, val_fraction=val_fraction)\n\nprint(f\"Training text length: {len(train_text) :,} characters.\")\nprint(f\"Validation text length: {len(val_text) :,} characters.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a969636","outputId":"119b07fe-ee8e-46a5-f374-b2ec9a3f6c4f","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:35.921665Z","iopub.execute_input":"2025-11-18T19:26:35.921944Z","iopub.status.idle":"2025-11-18T19:26:36.011893Z","shell.execute_reply.started":"2025-11-18T19:26:35.921906Z","shell.execute_reply":"2025-11-18T19:26:36.011221Z"}},"outputs":[{"name":"stdout","text":"Training text length: 89,099,996 characters.\nValidation text length: 900,004 characters.\n","output_type":"stream"}],"execution_count":10},{"id":"1c28ac68","cell_type":"markdown","source":"# Model Initialisation","metadata":{"id":"1c28ac68"}},{"id":"7fe304ec","cell_type":"markdown","source":"## Model Setup\n\nWe intialise our model with the following parameters in accordance to our configuration file.\nBased on these parameters, our model has approximately ~4.2M parameters.","metadata":{"id":"7fe304ec"}},{"id":"d19f06bd","cell_type":"code","source":"# Define the model params\nrng = jax.random.PRNGKey(seed)\n\nmodel_obj, params, constants = fn.create_train_state(\n        rng,\n        vocab_size = vocab_size,\n        d_model = d_model,\n        n_heads = n_heads,\n        n_layers = n_layers,\n        mlp_ratio = mlp_ratio,\n        seq_len = seq_len,\n        dropout = dropout_rate,\n        aux_loss = use_auxiliary_loss,\n        num_aux_heads = aux_heads,\n        mixed_precision = use_mixed_precision,\n        attention_type = attention_type,\n        pos_encoding = pos_encoding\n)","metadata":{"id":"d19f06bd","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:38.933006Z","iopub.execute_input":"2025-11-18T19:26:38.933730Z","iopub.status.idle":"2025-11-18T19:26:47.812838Z","shell.execute_reply.started":"2025-11-18T19:26:38.933702Z","shell.execute_reply":"2025-11-18T19:26:47.812207Z"}},"outputs":[],"execution_count":11},{"id":"91839933","cell_type":"code","source":"total_params = fn.count_parameters(params)\n\nprint(f\"Total number of parameters in the model: {total_params :,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91839933","outputId":"ba0a9b96-0f75-4307-d875-9ec206b1b8e4","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:47.813941Z","iopub.execute_input":"2025-11-18T19:26:47.814119Z","iopub.status.idle":"2025-11-18T19:26:47.818715Z","shell.execute_reply.started":"2025-11-18T19:26:47.814105Z","shell.execute_reply":"2025-11-18T19:26:47.818180Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters in the model: 4,160,768\n","output_type":"stream"}],"execution_count":12},{"id":"e709b510","cell_type":"markdown","source":"We perform a sanity check by running a single forward pass with random input data to ensure the model is functioning as expected.","metadata":{"id":"e709b510"}},{"id":"bdc7a187","cell_type":"code","source":"# SANITY CHECK: Test the model forward pass\nB, T = 2, 8  # Batch size and sequence length\nbatch = jax.random.randint(key = rng, shape = (B, T), minval = 0, maxval = vocab_size)\n\nvariables = {\"params\": params, \"constants\": constants}\noutput = model_obj.apply(variables, batch, deterministic=False)\nprint(\"Logits shape:\", output[\"logits\"].shape)  # Expected: (B, T, vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdc7a187","outputId":"81237094-534d-4a66-9fad-6faac1bbd600","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:50.410152Z","iopub.execute_input":"2025-11-18T19:26:50.410711Z","iopub.status.idle":"2025-11-18T19:26:54.084412Z","shell.execute_reply.started":"2025-11-18T19:26:50.410689Z","shell.execute_reply":"2025-11-18T19:26:54.083541Z"}},"outputs":[{"name":"stdout","text":"Logits shape: (2, 8, 27)\n","output_type":"stream"}],"execution_count":13},{"id":"ec22aeee","cell_type":"markdown","source":"## Initialise the optimizer\n\nIn this section, we set up the optimizer for training our model in accordance to our configuration file.","metadata":{"id":"ec22aeee"}},{"id":"05d04750","cell_type":"code","source":"# Define the learning rate\nlearning_rate = learning_rate\n\n# Create the Optimizer and initialize it\nif optimizer_type == \"adam\":\n    optimizer = optax.adam(learning_rate)\nelif optimizer_type == \"sgd\":\n    optimizer = optax.sgd(learning_rate)\nelif optimizer_type == \"adamw\":\n    optimizer = optax.adamw(\n        learning_rate = learning_rate,\n        weight_decay = weight_decay\n    )\n\n# Add gradient clipping if specified\nif grad_clip is not None and grad_clip != \"none\":\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(grad_clip),\n        optimizer\n    )\n\n\nopt_state = optimizer.init(params)\n\nprint(\"Optimizer initialized:\", optimizer_type, \"with Learning Rate =\", learning_rate)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05d04750","outputId":"fb412264-79b9-4770-e4e6-ac448e4c992e","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:26:54.085605Z","iopub.execute_input":"2025-11-18T19:26:54.085889Z","iopub.status.idle":"2025-11-18T19:26:54.446341Z","shell.execute_reply.started":"2025-11-18T19:26:54.085868Z","shell.execute_reply":"2025-11-18T19:26:54.445647Z"}},"outputs":[{"name":"stdout","text":"Optimizer initialized: adam with Learning Rate = 0.001\n","output_type":"stream"}],"execution_count":14},{"id":"233c7fe3","cell_type":"markdown","source":"## Text encoding\n\nWe then encode the text data into integer format for model training. Each unique character is mapped to a unique integer index.","metadata":{"id":"233c7fe3"}},{"id":"17829ea8","cell_type":"code","source":"# Encode the train, val, test texts\ntrain_data = fn.encode(train_text, chars_to_int)\nval_data = fn.encode(val_text, chars_to_int)","metadata":{"id":"17829ea8","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:27:00.138862Z","iopub.execute_input":"2025-11-18T19:27:00.139343Z","iopub.status.idle":"2025-11-18T19:27:07.443852Z","shell.execute_reply.started":"2025-11-18T19:27:00.139311Z","shell.execute_reply":"2025-11-18T19:27:07.443079Z"}},"outputs":[],"execution_count":15},{"id":"153631ec","cell_type":"markdown","source":"## Determine maximum permissible training steps\n\nTaking into account possible compute limitations, we perform a preliminary calculation to determine the maximum number of training steps we can perform based on the throughput of our model and the total training time available. For this preliminary test, the default maximum training time is 60 seconds, and commpute budget hours is 2 hours.\n\nBased on the throughput calculated from the preliminary test, the estimated maximum no. of training steps we can perform within this compute budget is ~120,000. To ensure we keep within the budget, we set the maximum training steps to be 100,000.","metadata":{"id":"153631ec"}},{"id":"1312e527","cell_type":"code","source":"# Determining how many steps we can run in a reasonable time\nmax_iters = max_test_iters\nmax_time = max_test_time_in_seconds # in seconds\nmax_compute_time = compute_budget_hours # in hours\n\n_ , max_steps = fn.calculate_throughput(\n    max_test_iters = max_iters,\n    max_test_time = max_time,\n    model = model_obj,\n    params = params,\n    opt_state = opt_state,\n    optimizer = optimizer,\n    rng = rng,\n    batch_size = batch_size,\n    seq_len = seq_len,\n    compute_budget = max_compute_time,\n    train_data = train_data,\n    loss_type = loss_type,\n    aux_loss = use_auxiliary_loss,\n    aux_weight = aux_weight,\n    constants = constants,\n    label_smoothing = label_smoothing\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1312e527","outputId":"7961ac93-8c66-4d74-f71a-6787aa4ffad6","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:27:07.444906Z","iopub.execute_input":"2025-11-18T19:27:07.445154Z","iopub.status.idle":"2025-11-18T19:28:05.426574Z","shell.execute_reply.started":"2025-11-18T19:27:07.445137Z","shell.execute_reply":"2025-11-18T19:28:05.425754Z"}},"outputs":[{"name":"stdout","text":"Benchmark completed in 57.98 seconds.\nTotal tokens processed: 8192000\nThroughput: 141300.17 tokens/second\nEstimated max steps within compute budget: 124189.0\n","output_type":"stream"}],"execution_count":16},{"id":"ff66f55f","cell_type":"markdown","source":"# Model Training & Evaluation","metadata":{"id":"ff66f55f"}},{"id":"b839549c","cell_type":"markdown","source":"## Training the model\n\nNow, we proceed to train the model over the determined number of training iterations. During training, we monitor the training loss and periodically evaluate the model on the validation set to track its performance. We also make sure to record the time taken for training to ensure it stays within our compute budget.","metadata":{"id":"b839549c"}},{"id":"1262eda3","cell_type":"code","source":"iter_max = 100000\n\n# To track training and validation loss, as well as time taken\ntrain_loss_history = []\nval_loss_history = []\ntrain_step_history = list(range(iter_max))\nval_step_history = []\n\n# Load checkpoint if it exists\nparams, opt_state, constants, start_iter = fn.load_checkpoint(\n    checkpoint_file,\n    params,\n    constants,\n    opt_state\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1262eda3","outputId":"bc8a88d5-d946-4bf7-f70d-4ea509e7045b","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:28:35.234775Z","iopub.execute_input":"2025-11-18T19:28:35.235328Z","iopub.status.idle":"2025-11-18T19:28:35.242609Z","shell.execute_reply.started":"2025-11-18T19:28:35.235304Z","shell.execute_reply":"2025-11-18T19:28:35.241954Z"}},"outputs":[{"name":"stdout","text":"[load_checkpoint] No checkpoint found at checkpoint.pkl\n[load_checkpoint] Starting training as per usual.\n","output_type":"stream"}],"execution_count":17},{"id":"cd2b91d3","cell_type":"markdown","source":"We then train the model and log the training and validation losses for analysis.","metadata":{"id":"cd2b91d3"}},{"id":"c7cbde26","cell_type":"code","source":"if start_iter > 0:\n        print(f\"Resuming training from iteration = {start_iter}.\")\nelse:\n        print(\"Starting training from iteration = 0.\")\n\ntime_start = time.time()\n\nfor it in range(start_iter, iter_max):\n\n    # get a batch of data\n    inputs, targets = fn.get_batch(train_data, batch_size, seq_len)\n\n    # Perform a training step\n    rng, sub = jax.random.split(rng)\n    new_params, new_opt_state, metrics = fn.train_step(\n            model = model_obj,\n            params = params,\n            constants=constants,\n            opt_state = opt_state,\n            x = inputs,\n            y = targets,\n            tx = optimizer,\n            rng = sub,\n            loss_type = loss_type,\n            aux_loss = use_auxiliary_loss,\n            aux_weight = aux_weight,\n            label_smoothing = label_smoothing\n    )\n\n    # Update parameters and optimizer state\n    params = new_params\n    opt_state = new_opt_state\n\n    # Record training metrics\n    acc = metrics['acc']\n    loss = metrics['loss']\n    last_char_acc = metrics['acc_last']\n    train_time = time.time() - time_start\n\n    train_loss_history.append(loss)\n\n    fn.update_training_log(\n        log_path = \"training_results.log\",\n        step = it,\n        train_loss = loss,\n        train_time = train_time,\n        train_acc = acc,\n        last_char_acc = last_char_acc\n        )\n\n    log_every = max(1, iter_max // 100)\n\n    if (it % log_every) == 0 or (it == iter_max - 1): # Print every 1% of iterations\n\n        # Compute the loss on validation set\n        batch_size_val = batch_size\n        seq_len_val = seq_len\n        val_inputs, val_targets = fn.get_batch(val_data, batch_size_val, seq_len_val)\n\n        val_out = model_obj.apply({\"params\": params, \"constants\": constants}, val_inputs, deterministic=True)\n        val_logits = val_out[\"logits\"]\n        val_aux_logits = val_out.get('aux_logits', None)\n\n        val_loss, val_metrics = fn.loss_and_metrics(\n            logits = val_logits,\n            targets = val_targets,\n            loss_type = loss_type,\n            aux_loss = use_auxiliary_loss,\n            aux_logits = val_aux_logits,\n            aux_weight = aux_weight,\n            label_smoothing = label_smoothing\n        )\n\n        # Record validation loss and time\n        val_acc = val_metrics['acc']\n        last_char_acc_val = val_metrics['acc_last']\n        val_loss_history.append(val_loss)\n        time_elapsed = time.time() - time_start\n        val_step_history.append(it)\n\n        fn.update_validation_log(\n            log_path = \"validation_results.log\",\n            step = it,\n            val_loss = val_loss,\n            val_time = time_elapsed,\n            val_acc = val_acc,\n            last_char_val_acc = last_char_acc_val\n        )\n\n        fn.save_checkpoint(\n            checkpoint_path = checkpoint_file,\n            params = params,\n            constants = constants,\n            opt_state = opt_state,\n            step = it,\n            time_elapsed = time_elapsed\n        )\n\n        # Print training and validation metrics\n        print(f\"Iteration {it}, time elapsed: {time_elapsed:.2f} seconds\")\n        print(f\"\\t \\t Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")\n        print(f\"\\t \\t Training Acc: {acc:.4f}, Validation Acc: {val_acc:.4f}\")\n        print(f\"\\t \\t Last Char Training Acc: {last_char_acc:.4f}, Last Char Validation Acc: {last_char_acc_val:.4f}\")\n        print(\"-\" * 50)\n\nprint(f\"Training completed in {time.time() - time_start:.2f} seconds.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7cbde26","outputId":"8478b89e-dafe-4e69-8274-aa483af8ca9e","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:28:44.460000Z","iopub.execute_input":"2025-11-18T19:28:44.460246Z","iopub.status.idle":"2025-11-18T21:00:59.436311Z","shell.execute_reply.started":"2025-11-18T19:28:44.460229Z","shell.execute_reply":"2025-11-18T21:00:59.435634Z"}},"outputs":[{"name":"stdout","text":"Starting training from iteration = 0.\n[save_checkpoint] Saved checkpoint at step 0 to checkpoint.pkl\nIteration 0, time elapsed: 4.17 seconds\n\t \t Training Loss: 3.7112, Validation Loss: 4.5223\n\t \t Training Acc: 0.0587, Validation Acc: 0.1680\n\t \t Last Char Training Acc: 0.0938, Last Char Validation Acc: 0.2188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 1000 to checkpoint.pkl\nIteration 1000, time elapsed: 59.19 seconds\n\t \t Training Loss: 1.4405, Validation Loss: 1.4526\n\t \t Training Acc: 0.5544, Validation Acc: 0.5546\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 2000 to checkpoint.pkl\nIteration 2000, time elapsed: 114.19 seconds\n\t \t Training Loss: 1.3374, Validation Loss: 1.3332\n\t \t Training Acc: 0.5771, Validation Acc: 0.5833\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 3000 to checkpoint.pkl\nIteration 3000, time elapsed: 169.30 seconds\n\t \t Training Loss: 1.2828, Validation Loss: 1.3818\n\t \t Training Acc: 0.5988, Validation Acc: 0.5615\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 4000 to checkpoint.pkl\nIteration 4000, time elapsed: 224.54 seconds\n\t \t Training Loss: 1.3071, Validation Loss: 1.2751\n\t \t Training Acc: 0.5918, Validation Acc: 0.5997\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 5000 to checkpoint.pkl\nIteration 5000, time elapsed: 280.04 seconds\n\t \t Training Loss: 1.1891, Validation Loss: 1.2735\n\t \t Training Acc: 0.6257, Validation Acc: 0.6024\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 6000 to checkpoint.pkl\nIteration 6000, time elapsed: 335.57 seconds\n\t \t Training Loss: 1.2574, Validation Loss: 1.2510\n\t \t Training Acc: 0.6088, Validation Acc: 0.6007\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 7000 to checkpoint.pkl\nIteration 7000, time elapsed: 390.69 seconds\n\t \t Training Loss: 1.1859, Validation Loss: 1.2200\n\t \t Training Acc: 0.6267, Validation Acc: 0.6163\n\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 8000 to checkpoint.pkl\nIteration 8000, time elapsed: 445.77 seconds\n\t \t Training Loss: 1.2656, Validation Loss: 1.2128\n\t \t Training Acc: 0.5962, Validation Acc: 0.6162\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 9000 to checkpoint.pkl\nIteration 9000, time elapsed: 500.94 seconds\n\t \t Training Loss: 1.2515, Validation Loss: 1.2263\n\t \t Training Acc: 0.6053, Validation Acc: 0.6213\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 10000 to checkpoint.pkl\nIteration 10000, time elapsed: 556.14 seconds\n\t \t Training Loss: 1.1796, Validation Loss: 1.2645\n\t \t Training Acc: 0.6199, Validation Acc: 0.6010\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 11000 to checkpoint.pkl\nIteration 11000, time elapsed: 611.35 seconds\n\t \t Training Loss: 1.2535, Validation Loss: 1.2862\n\t \t Training Acc: 0.6127, Validation Acc: 0.6002\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 12000 to checkpoint.pkl\nIteration 12000, time elapsed: 666.79 seconds\n\t \t Training Loss: 1.1894, Validation Loss: 1.1128\n\t \t Training Acc: 0.6274, Validation Acc: 0.6434\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 13000 to checkpoint.pkl\nIteration 13000, time elapsed: 722.29 seconds\n\t \t Training Loss: 1.1526, Validation Loss: 1.1895\n\t \t Training Acc: 0.6335, Validation Acc: 0.6244\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 14000 to checkpoint.pkl\nIteration 14000, time elapsed: 777.39 seconds\n\t \t Training Loss: 1.1476, Validation Loss: 1.1826\n\t \t Training Acc: 0.6328, Validation Acc: 0.6207\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 15000 to checkpoint.pkl\nIteration 15000, time elapsed: 832.47 seconds\n\t \t Training Loss: 1.1135, Validation Loss: 1.1595\n\t \t Training Acc: 0.6510, Validation Acc: 0.6328\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 16000 to checkpoint.pkl\nIteration 16000, time elapsed: 887.48 seconds\n\t \t Training Loss: 1.1853, Validation Loss: 1.1891\n\t \t Training Acc: 0.6245, Validation Acc: 0.6294\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 17000 to checkpoint.pkl\nIteration 17000, time elapsed: 942.80 seconds\n\t \t Training Loss: 1.1891, Validation Loss: 1.1762\n\t \t Training Acc: 0.6232, Validation Acc: 0.6300\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 18000 to checkpoint.pkl\nIteration 18000, time elapsed: 998.05 seconds\n\t \t Training Loss: 1.0953, Validation Loss: 1.1967\n\t \t Training Acc: 0.6499, Validation Acc: 0.6194\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 19000 to checkpoint.pkl\nIteration 19000, time elapsed: 1053.21 seconds\n\t \t Training Loss: 1.1462, Validation Loss: 1.1231\n\t \t Training Acc: 0.6444, Validation Acc: 0.6455\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 20000 to checkpoint.pkl\nIteration 20000, time elapsed: 1108.24 seconds\n\t \t Training Loss: 1.1384, Validation Loss: 1.1524\n\t \t Training Acc: 0.6447, Validation Acc: 0.6378\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.4375\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 21000 to checkpoint.pkl\nIteration 21000, time elapsed: 1163.91 seconds\n\t \t Training Loss: 1.1067, Validation Loss: 1.1691\n\t \t Training Acc: 0.6510, Validation Acc: 0.6306\n\t \t Last Char Training Acc: 0.8438, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 22000 to checkpoint.pkl\nIteration 22000, time elapsed: 1219.27 seconds\n\t \t Training Loss: 1.0834, Validation Loss: 1.1490\n\t \t Training Acc: 0.6562, Validation Acc: 0.6395\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 23000 to checkpoint.pkl\nIteration 23000, time elapsed: 1274.46 seconds\n\t \t Training Loss: 1.1852, Validation Loss: 1.1084\n\t \t Training Acc: 0.6267, Validation Acc: 0.6503\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 24000 to checkpoint.pkl\nIteration 24000, time elapsed: 1329.54 seconds\n\t \t Training Loss: 1.1233, Validation Loss: 1.1114\n\t \t Training Acc: 0.6438, Validation Acc: 0.6405\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 25000 to checkpoint.pkl\nIteration 25000, time elapsed: 1384.58 seconds\n\t \t Training Loss: 1.1400, Validation Loss: 1.1412\n\t \t Training Acc: 0.6437, Validation Acc: 0.6390\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 26000 to checkpoint.pkl\nIteration 26000, time elapsed: 1439.99 seconds\n\t \t Training Loss: 1.0576, Validation Loss: 1.1502\n\t \t Training Acc: 0.6669, Validation Acc: 0.6368\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 27000 to checkpoint.pkl\nIteration 27000, time elapsed: 1495.58 seconds\n\t \t Training Loss: 1.1348, Validation Loss: 1.2582\n\t \t Training Acc: 0.6384, Validation Acc: 0.6121\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 28000 to checkpoint.pkl\nIteration 28000, time elapsed: 1550.94 seconds\n\t \t Training Loss: 1.1231, Validation Loss: 1.1373\n\t \t Training Acc: 0.6444, Validation Acc: 0.6399\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 29000 to checkpoint.pkl\nIteration 29000, time elapsed: 1606.18 seconds\n\t \t Training Loss: 1.1203, Validation Loss: 1.1178\n\t \t Training Acc: 0.6420, Validation Acc: 0.6493\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 30000 to checkpoint.pkl\nIteration 30000, time elapsed: 1661.24 seconds\n\t \t Training Loss: 1.1212, Validation Loss: 1.0949\n\t \t Training Acc: 0.6459, Validation Acc: 0.6534\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 31000 to checkpoint.pkl\nIteration 31000, time elapsed: 1716.54 seconds\n\t \t Training Loss: 1.1030, Validation Loss: 1.1728\n\t \t Training Acc: 0.6525, Validation Acc: 0.6328\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 32000 to checkpoint.pkl\nIteration 32000, time elapsed: 1771.60 seconds\n\t \t Training Loss: 1.0817, Validation Loss: 1.0865\n\t \t Training Acc: 0.6536, Validation Acc: 0.6562\n\t \t Last Char Training Acc: 0.8125, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 33000 to checkpoint.pkl\nIteration 33000, time elapsed: 1826.58 seconds\n\t \t Training Loss: 1.1563, Validation Loss: 1.1393\n\t \t Training Acc: 0.6410, Validation Acc: 0.6471\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 34000 to checkpoint.pkl\nIteration 34000, time elapsed: 1882.01 seconds\n\t \t Training Loss: 1.1438, Validation Loss: 1.1452\n\t \t Training Acc: 0.6388, Validation Acc: 0.6411\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 35000 to checkpoint.pkl\nIteration 35000, time elapsed: 1937.38 seconds\n\t \t Training Loss: 1.0872, Validation Loss: 1.1209\n\t \t Training Acc: 0.6553, Validation Acc: 0.6488\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 36000 to checkpoint.pkl\nIteration 36000, time elapsed: 1992.39 seconds\n\t \t Training Loss: 1.0714, Validation Loss: 1.1636\n\t \t Training Acc: 0.6571, Validation Acc: 0.6320\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 37000 to checkpoint.pkl\nIteration 37000, time elapsed: 2047.55 seconds\n\t \t Training Loss: 1.0780, Validation Loss: 1.1103\n\t \t Training Acc: 0.6565, Validation Acc: 0.6510\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 38000 to checkpoint.pkl\nIteration 38000, time elapsed: 2102.66 seconds\n\t \t Training Loss: 1.0993, Validation Loss: 1.1649\n\t \t Training Acc: 0.6548, Validation Acc: 0.6338\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 39000 to checkpoint.pkl\nIteration 39000, time elapsed: 2157.86 seconds\n\t \t Training Loss: 1.0622, Validation Loss: 1.1849\n\t \t Training Acc: 0.6608, Validation Acc: 0.6290\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 40000 to checkpoint.pkl\nIteration 40000, time elapsed: 2213.08 seconds\n\t \t Training Loss: 1.1199, Validation Loss: 1.1337\n\t \t Training Acc: 0.6451, Validation Acc: 0.6426\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 41000 to checkpoint.pkl\nIteration 41000, time elapsed: 2268.65 seconds\n\t \t Training Loss: 1.0724, Validation Loss: 1.1153\n\t \t Training Acc: 0.6622, Validation Acc: 0.6499\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 42000 to checkpoint.pkl\nIteration 42000, time elapsed: 2323.77 seconds\n\t \t Training Loss: 1.0791, Validation Loss: 1.1151\n\t \t Training Acc: 0.6599, Validation Acc: 0.6412\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 43000 to checkpoint.pkl\nIteration 43000, time elapsed: 2378.83 seconds\n\t \t Training Loss: 1.0640, Validation Loss: 1.1471\n\t \t Training Acc: 0.6667, Validation Acc: 0.6415\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 44000 to checkpoint.pkl\nIteration 44000, time elapsed: 2433.91 seconds\n\t \t Training Loss: 1.1378, Validation Loss: 1.1423\n\t \t Training Acc: 0.6445, Validation Acc: 0.6422\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 45000 to checkpoint.pkl\nIteration 45000, time elapsed: 2489.15 seconds\n\t \t Training Loss: 1.0757, Validation Loss: 1.1136\n\t \t Training Acc: 0.6570, Validation Acc: 0.6527\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 46000 to checkpoint.pkl\nIteration 46000, time elapsed: 2544.47 seconds\n\t \t Training Loss: 1.0495, Validation Loss: 1.0878\n\t \t Training Acc: 0.6692, Validation Acc: 0.6622\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 47000 to checkpoint.pkl\nIteration 47000, time elapsed: 2599.78 seconds\n\t \t Training Loss: 1.1403, Validation Loss: 1.1345\n\t \t Training Acc: 0.6368, Validation Acc: 0.6472\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 48000 to checkpoint.pkl\nIteration 48000, time elapsed: 2655.34 seconds\n\t \t Training Loss: 1.0978, Validation Loss: 1.0862\n\t \t Training Acc: 0.6517, Validation Acc: 0.6548\n\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 49000 to checkpoint.pkl\nIteration 49000, time elapsed: 2710.36 seconds\n\t \t Training Loss: 1.0466, Validation Loss: 1.0204\n\t \t Training Acc: 0.6617, Validation Acc: 0.6755\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 50000 to checkpoint.pkl\nIteration 50000, time elapsed: 2765.41 seconds\n\t \t Training Loss: 1.0210, Validation Loss: 1.1135\n\t \t Training Acc: 0.6776, Validation Acc: 0.6478\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 51000 to checkpoint.pkl\nIteration 51000, time elapsed: 2820.44 seconds\n\t \t Training Loss: 1.0257, Validation Loss: 1.0712\n\t \t Training Acc: 0.6757, Validation Acc: 0.6648\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 52000 to checkpoint.pkl\nIteration 52000, time elapsed: 2875.59 seconds\n\t \t Training Loss: 1.0538, Validation Loss: 1.0845\n\t \t Training Acc: 0.6678, Validation Acc: 0.6642\n\t \t Last Char Training Acc: 0.8125, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 53000 to checkpoint.pkl\nIteration 53000, time elapsed: 2930.81 seconds\n\t \t Training Loss: 1.0698, Validation Loss: 1.0933\n\t \t Training Acc: 0.6635, Validation Acc: 0.6533\n\t \t Last Char Training Acc: 0.8125, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 54000 to checkpoint.pkl\nIteration 54000, time elapsed: 2985.96 seconds\n\t \t Training Loss: 1.0497, Validation Loss: 1.1346\n\t \t Training Acc: 0.6630, Validation Acc: 0.6462\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 55000 to checkpoint.pkl\nIteration 55000, time elapsed: 3041.17 seconds\n\t \t Training Loss: 1.0281, Validation Loss: 1.0580\n\t \t Training Acc: 0.6746, Validation Acc: 0.6664\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.8438\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 56000 to checkpoint.pkl\nIteration 56000, time elapsed: 3097.00 seconds\n\t \t Training Loss: 1.0226, Validation Loss: 1.0702\n\t \t Training Acc: 0.6748, Validation Acc: 0.6592\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 57000 to checkpoint.pkl\nIteration 57000, time elapsed: 3152.51 seconds\n\t \t Training Loss: 1.0240, Validation Loss: 1.0559\n\t \t Training Acc: 0.6793, Validation Acc: 0.6656\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 58000 to checkpoint.pkl\nIteration 58000, time elapsed: 3207.93 seconds\n\t \t Training Loss: 1.0499, Validation Loss: 1.1378\n\t \t Training Acc: 0.6697, Validation Acc: 0.6389\n\t \t Last Char Training Acc: 0.8750, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 59000 to checkpoint.pkl\nIteration 59000, time elapsed: 3263.33 seconds\n\t \t Training Loss: 0.9852, Validation Loss: 1.0816\n\t \t Training Acc: 0.6901, Validation Acc: 0.6620\n\t \t Last Char Training Acc: 0.8438, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 60000 to checkpoint.pkl\nIteration 60000, time elapsed: 3318.74 seconds\n\t \t Training Loss: 1.0405, Validation Loss: 1.1089\n\t \t Training Acc: 0.6663, Validation Acc: 0.6477\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 61000 to checkpoint.pkl\nIteration 61000, time elapsed: 3374.23 seconds\n\t \t Training Loss: 1.0763, Validation Loss: 1.0696\n\t \t Training Acc: 0.6600, Validation Acc: 0.6572\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.4062\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 62000 to checkpoint.pkl\nIteration 62000, time elapsed: 3429.73 seconds\n\t \t Training Loss: 1.0258, Validation Loss: 1.1283\n\t \t Training Acc: 0.6791, Validation Acc: 0.6492\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 63000 to checkpoint.pkl\nIteration 63000, time elapsed: 3485.30 seconds\n\t \t Training Loss: 1.0724, Validation Loss: 1.0327\n\t \t Training Acc: 0.6576, Validation Acc: 0.6772\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 64000 to checkpoint.pkl\nIteration 64000, time elapsed: 3540.70 seconds\n\t \t Training Loss: 1.0220, Validation Loss: 1.1045\n\t \t Training Acc: 0.6707, Validation Acc: 0.6515\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 65000 to checkpoint.pkl\nIteration 65000, time elapsed: 3596.11 seconds\n\t \t Training Loss: 1.0321, Validation Loss: 1.0930\n\t \t Training Acc: 0.6703, Validation Acc: 0.6595\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 66000 to checkpoint.pkl\nIteration 66000, time elapsed: 3651.63 seconds\n\t \t Training Loss: 1.1053, Validation Loss: 1.0774\n\t \t Training Acc: 0.6600, Validation Acc: 0.6605\n\t \t Last Char Training Acc: 0.8125, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 67000 to checkpoint.pkl\nIteration 67000, time elapsed: 3707.05 seconds\n\t \t Training Loss: 1.0187, Validation Loss: 1.0603\n\t \t Training Acc: 0.6748, Validation Acc: 0.6667\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 68000 to checkpoint.pkl\nIteration 68000, time elapsed: 3762.53 seconds\n\t \t Training Loss: 1.0634, Validation Loss: 1.0491\n\t \t Training Acc: 0.6647, Validation Acc: 0.6705\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 69000 to checkpoint.pkl\nIteration 69000, time elapsed: 3818.29 seconds\n\t \t Training Loss: 1.0026, Validation Loss: 1.0772\n\t \t Training Acc: 0.6869, Validation Acc: 0.6626\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 70000 to checkpoint.pkl\nIteration 70000, time elapsed: 3873.72 seconds\n\t \t Training Loss: 1.0744, Validation Loss: 1.1123\n\t \t Training Acc: 0.6622, Validation Acc: 0.6534\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 71000 to checkpoint.pkl\nIteration 71000, time elapsed: 3929.24 seconds\n\t \t Training Loss: 1.0910, Validation Loss: 1.0563\n\t \t Training Acc: 0.6575, Validation Acc: 0.6732\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 72000 to checkpoint.pkl\nIteration 72000, time elapsed: 3984.86 seconds\n\t \t Training Loss: 1.0571, Validation Loss: 1.0891\n\t \t Training Acc: 0.6660, Validation Acc: 0.6593\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 73000 to checkpoint.pkl\nIteration 73000, time elapsed: 4040.36 seconds\n\t \t Training Loss: 1.0249, Validation Loss: 1.1741\n\t \t Training Acc: 0.6761, Validation Acc: 0.6294\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 74000 to checkpoint.pkl\nIteration 74000, time elapsed: 4096.01 seconds\n\t \t Training Loss: 1.0979, Validation Loss: 1.1338\n\t \t Training Acc: 0.6548, Validation Acc: 0.6531\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 75000 to checkpoint.pkl\nIteration 75000, time elapsed: 4151.72 seconds\n\t \t Training Loss: 1.0353, Validation Loss: 1.0380\n\t \t Training Acc: 0.6738, Validation Acc: 0.6699\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 76000 to checkpoint.pkl\nIteration 76000, time elapsed: 4207.18 seconds\n\t \t Training Loss: 1.0860, Validation Loss: 1.0585\n\t \t Training Acc: 0.6604, Validation Acc: 0.6729\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 77000 to checkpoint.pkl\nIteration 77000, time elapsed: 4262.72 seconds\n\t \t Training Loss: 1.0738, Validation Loss: 1.0794\n\t \t Training Acc: 0.6643, Validation Acc: 0.6636\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 78000 to checkpoint.pkl\nIteration 78000, time elapsed: 4318.10 seconds\n\t \t Training Loss: 1.0287, Validation Loss: 1.0508\n\t \t Training Acc: 0.6725, Validation Acc: 0.6650\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 79000 to checkpoint.pkl\nIteration 79000, time elapsed: 4373.69 seconds\n\t \t Training Loss: 0.9786, Validation Loss: 1.0944\n\t \t Training Acc: 0.6941, Validation Acc: 0.6572\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 80000 to checkpoint.pkl\nIteration 80000, time elapsed: 4429.35 seconds\n\t \t Training Loss: 1.0782, Validation Loss: 1.0867\n\t \t Training Acc: 0.6671, Validation Acc: 0.6604\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 81000 to checkpoint.pkl\nIteration 81000, time elapsed: 4485.08 seconds\n\t \t Training Loss: 1.0471, Validation Loss: 1.0550\n\t \t Training Acc: 0.6716, Validation Acc: 0.6694\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 82000 to checkpoint.pkl\nIteration 82000, time elapsed: 4540.58 seconds\n\t \t Training Loss: 1.0354, Validation Loss: 1.0637\n\t \t Training Acc: 0.6774, Validation Acc: 0.6710\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 83000 to checkpoint.pkl\nIteration 83000, time elapsed: 4596.05 seconds\n\t \t Training Loss: 1.0155, Validation Loss: 1.0954\n\t \t Training Acc: 0.6783, Validation Acc: 0.6589\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 84000 to checkpoint.pkl\nIteration 84000, time elapsed: 4651.33 seconds\n\t \t Training Loss: 1.0209, Validation Loss: 1.0551\n\t \t Training Acc: 0.6768, Validation Acc: 0.6647\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 85000 to checkpoint.pkl\nIteration 85000, time elapsed: 4706.64 seconds\n\t \t Training Loss: 1.0037, Validation Loss: 1.0541\n\t \t Training Acc: 0.6837, Validation Acc: 0.6661\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 86000 to checkpoint.pkl\nIteration 86000, time elapsed: 4761.88 seconds\n\t \t Training Loss: 0.9936, Validation Loss: 1.0522\n\t \t Training Acc: 0.6851, Validation Acc: 0.6724\n\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 87000 to checkpoint.pkl\nIteration 87000, time elapsed: 4817.21 seconds\n\t \t Training Loss: 1.0899, Validation Loss: 1.0665\n\t \t Training Acc: 0.6615, Validation Acc: 0.6670\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 88000 to checkpoint.pkl\nIteration 88000, time elapsed: 4872.46 seconds\n\t \t Training Loss: 1.0234, Validation Loss: 1.0724\n\t \t Training Acc: 0.6755, Validation Acc: 0.6649\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 89000 to checkpoint.pkl\nIteration 89000, time elapsed: 4927.84 seconds\n\t \t Training Loss: 1.0358, Validation Loss: 1.0953\n\t \t Training Acc: 0.6759, Validation Acc: 0.6595\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 90000 to checkpoint.pkl\nIteration 90000, time elapsed: 4983.33 seconds\n\t \t Training Loss: 1.0508, Validation Loss: 1.0449\n\t \t Training Acc: 0.6676, Validation Acc: 0.6680\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 91000 to checkpoint.pkl\nIteration 91000, time elapsed: 5038.44 seconds\n\t \t Training Loss: 1.0491, Validation Loss: 1.1151\n\t \t Training Acc: 0.6658, Validation Acc: 0.6522\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 92000 to checkpoint.pkl\nIteration 92000, time elapsed: 5093.55 seconds\n\t \t Training Loss: 1.0426, Validation Loss: 1.0486\n\t \t Training Acc: 0.6708, Validation Acc: 0.6743\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 93000 to checkpoint.pkl\nIteration 93000, time elapsed: 5148.58 seconds\n\t \t Training Loss: 1.0302, Validation Loss: 1.0474\n\t \t Training Acc: 0.6807, Validation Acc: 0.6654\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.9062\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 94000 to checkpoint.pkl\nIteration 94000, time elapsed: 5203.72 seconds\n\t \t Training Loss: 1.0804, Validation Loss: 1.0252\n\t \t Training Acc: 0.6643, Validation Acc: 0.6722\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 95000 to checkpoint.pkl\nIteration 95000, time elapsed: 5258.89 seconds\n\t \t Training Loss: 1.0037, Validation Loss: 1.0590\n\t \t Training Acc: 0.6836, Validation Acc: 0.6641\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 96000 to checkpoint.pkl\nIteration 96000, time elapsed: 5314.08 seconds\n\t \t Training Loss: 1.0223, Validation Loss: 1.0983\n\t \t Training Acc: 0.6802, Validation Acc: 0.6570\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 97000 to checkpoint.pkl\nIteration 97000, time elapsed: 5369.38 seconds\n\t \t Training Loss: 1.0301, Validation Loss: 1.0825\n\t \t Training Acc: 0.6724, Validation Acc: 0.6643\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 98000 to checkpoint.pkl\nIteration 98000, time elapsed: 5424.37 seconds\n\t \t Training Loss: 1.0703, Validation Loss: 1.0637\n\t \t Training Acc: 0.6733, Validation Acc: 0.6617\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 99000 to checkpoint.pkl\nIteration 99000, time elapsed: 5479.40 seconds\n\t \t Training Loss: 1.0106, Validation Loss: 1.0215\n\t \t Training Acc: 0.6761, Validation Acc: 0.6779\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 99999 to checkpoint.pkl\nIteration 99999, time elapsed: 5534.80 seconds\n\t \t Training Loss: 1.0883, Validation Loss: 1.0503\n\t \t Training Acc: 0.6570, Validation Acc: 0.6670\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5938\n--------------------------------------------------\nTraining completed in 5534.97 seconds.\n","output_type":"stream"}],"execution_count":18},{"id":"1f22e0a4","cell_type":"markdown","source":"## Plot the training and validation loss curves\n\nAfter training, we plot the training and validation loss curves to visualize the model's learning progress over time. This plot is saved to the specified output directory, and will be useful to identify if the model converges at first glance. Among all the ablation experiments for that category, we can also compare these curves to see how different configurations affect the learning dynamics.","metadata":{"id":"1f22e0a4"}},{"id":"f4cd9351","cell_type":"code","source":"# Plot training and validation loss curves\nplt.plot(train_step_history, train_loss_history, \"-\",label='Training Loss', color='blue')\nplt.plot(val_step_history, val_loss_history, \"-\", label='Validation Loss', lw = 2, color='red')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend(loc = \"upper right\")\nplt.title(\"Training and Validation Loss over Iterations\")\nplt.legend()\nplt.savefig(\"loss_curve.png\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"f4cd9351","outputId":"2cd068a4-34b3-40f0-de45-d4594dcd51e0","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T21:00:59.437550Z","iopub.execute_input":"2025-11-18T21:00:59.437962Z","iopub.status.idle":"2025-11-18T21:01:00.566164Z","shell.execute_reply.started":"2025-11-18T21:00:59.437934Z","shell.execute_reply":"2025-11-18T21:01:00.565343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":19},{"id":"11644d30","cell_type":"markdown","source":"## Testing the model on a given prompt\n\nWe finally test the trained model by generating text based on a given prompt. The model successfully generates a sequence of characters that continue from the prompt, and we save the generated text to a file for review.","metadata":{"id":"11644d30"}},{"id":"3767fd8e","cell_type":"code","source":"# Test the model on a given prompt\nprompt = \"the meaning of life is\"\nencoded_prompt = fn.encode(prompt, chars_to_int)\ncontext = encoded_prompt[None, :]\n\nB = 1\nseed = seed\ngenerate_len = 1000\nrng = jax.random.PRNGKey(seed)\n\noutput_indices = fn.generate_tokens(\n    model=model_obj,\n    params=params,\n    constants=constants,\n    rng=rng,\n    context=context,\n    length=generate_len,\n    block_size=64,\n    temperature=0.8,\n    sample=True,\n    pad_id=None,\n    deterministic=True\n)\n\noutput_indices = np.array(output_indices)  # Convert from JAX array to NumPy array\ngenerated_text = fn.decode(output_indices, int_to_chars)\n\nprint(\"Generated ID Shape:\", output_indices.shape)\nprint(\"Generated Text:\")\nprint(prompt + generated_text)\n\ngenerated_text_file = \"generated_text.txt\"\n\nwith open(generated_text_file, \"w\") as f:\n    f.write(prompt + generated_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3767fd8e","outputId":"0e84baab-cecb-4ecb-91fd-2232306e9d16","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T21:01:00.567085Z","iopub.execute_input":"2025-11-18T21:01:00.567416Z","iopub.status.idle":"2025-11-18T21:01:03.783330Z","shell.execute_reply.started":"2025-11-18T21:01:00.567396Z","shell.execute_reply":"2025-11-18T21:01:03.782597Z"}},"outputs":[{"name":"stdout","text":"Generated ID Shape: (1, 1000)\nGenerated Text:\nthe meaning of life is told to some tang already only what going to do it our distance from an opportunity to the new roof for most of the stars despite large enough a new orbit to hope that did not allow the effort to record to record but to retreat the eplleges in the boston passenger the station on november eight three zero zero zero zero zero zero zero zero zero zero internet users in cartesian microprocessors related to the gna crop flop a floppy disk for the standard the now exploded by lee play s man capper joe norfolk manne is mentioned as a vinian and u s in the older testament through the country based on part in the middle east florida estrand cross country due to the most part standing large population growth heavily distributed by migrant inflation this process for malouchampse there is about the sporting story in the white house and the recording house to break out their home keaton the most often the player will not occur as a form of playing the way to occur the second and moves on a single \n","output_type":"stream"}],"execution_count":20}]}