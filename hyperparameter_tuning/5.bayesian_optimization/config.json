{
  "experiment_name": "bayesian optimization for LR and decay weight",
  "description": "Hyperparameter tuning using Bayesian optimization for learning rate (and schedule) and weight decay.",
  "seed": 0,

  "model": {
    "vocab_size": 27,
    "d_model": 256,
    "n_heads": 4,
    "n_layers": 6,
    "mlp_ratio": 4,
    "seq_len": 128,

    "dropout": 0.0,
    "weight_decay": 0.0,
    "label_smoothing": 0.0,

    "mixed_precision": false,
    "pos_encoding": "rotary",
    "attention_type": "GQA",
    
    "use_auxiliary_loss": false,
    "aux_heads": 2,
    "aux_weight": 0.4,
    "loss_type": "cross_entropy"
  },

  "throughput": {
    "max_test_iters": 1000,
    "max_test_time_in_seconds": 20,
    "compute_budget_hours": 0.01
  },

  "training": {
    "val_fraction": 0.01,
    "batch_size": 32,
    "learning_rate": 1e-3,
    "lr_schedule": "cosine",
    "optimizer": "adam",
    "warmup_ratio": "none"
  }
}