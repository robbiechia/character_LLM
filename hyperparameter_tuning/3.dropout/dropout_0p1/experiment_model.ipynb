{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"gpuType":"V5E1","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13788065,"sourceType":"datasetVersion","datasetId":8777561}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"adc4842c","cell_type":"markdown","source":"# Hyperparameter Tuning - Dropout (0.1) Experiment [Kaggle Codespace Version]\n\nAfter completing ablation experiments, we proceed to conduct hyperparameter tuning to further enhance the model's performance. The architecture is defined as per the 'experiment_model.py' file, and we utilise functions created in the 'experiment_utils.py' file to facilitate the training process.\n\nThe flow of this notebook is similar to that of previous components, but with modifications to accommodate the hyperparameter tuning requirements. The loss values are recorded for upcoming analysis, visualization and cross comparison.\n\nThis notebook is also incorporated to be able to run seamlessly in Kaggle's Codespace environment.","metadata":{"id":"adc4842c"}},{"id":"b67f9107","cell_type":"code","source":"################\n# KAGGLE SETUP #\n################\n\ndata_set_name = 'dropout-0p1' # Input this dataset name\nkaggle_dir = f'/kaggle/input/{data_set_name}/'\n\nimport sys\nsys.path.insert(0, kaggle_dir)\n\nimport experiment_utils2 as fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:29.287738Z","iopub.execute_input":"2025-11-19T07:39:29.288516Z","iopub.status.idle":"2025-11-19T07:39:31.769103Z","shell.execute_reply.started":"2025-11-19T07:39:29.288486Z","shell.execute_reply":"2025-11-19T07:39:31.768514Z"}},"outputs":[],"execution_count":1},{"id":"477b41b4","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport optax\nimport sys\nimport jax\nimport time\nimport os\nimport numpy as np\nfrom pathlib import Path","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"477b41b4","outputId":"5447b332-c98d-48e5-9f32-dc33a2170433","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:31.770177Z","iopub.execute_input":"2025-11-19T07:39:31.770957Z","iopub.status.idle":"2025-11-19T07:39:31.774625Z","shell.execute_reply.started":"2025-11-19T07:39:31.770926Z","shell.execute_reply":"2025-11-19T07:39:31.773835Z"}},"outputs":[],"execution_count":2},{"id":"656e4403","cell_type":"markdown","source":"# Load the Experiment Setup","metadata":{"id":"656e4403"}},{"id":"1f18b286","cell_type":"markdown","source":"## Set the relevant directory paths & update.log file\n\nHere, we set the necessary directories and output paths for saving model checkpoints and training logs.","metadata":{"id":"1f18b286"}},{"id":"2168d1a9","cell_type":"code","source":"training_log_file = \"training_results.log\"\nvalidation_log_file = \"validation_results.log\"\ncheckpoint_file = \"checkpoint.pkl\"","metadata":{"id":"2168d1a9","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:35.607246Z","iopub.execute_input":"2025-11-19T07:39:35.607621Z","iopub.status.idle":"2025-11-19T07:39:35.611416Z","shell.execute_reply.started":"2025-11-19T07:39:35.607598Z","shell.execute_reply":"2025-11-19T07:39:35.610585Z"}},"outputs":[],"execution_count":3},{"id":"a89050a0","cell_type":"code","source":"if not os.path.exists(training_log_file):\n    fn.initialize_training_log(training_log_file)\n\nif not os.path.exists(validation_log_file):\n    fn.initialize_validation_log(validation_log_file)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a89050a0","outputId":"942f4710-a759-4a02-8bb6-06874e0968c8","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:37.702080Z","iopub.execute_input":"2025-11-19T07:39:37.702877Z","iopub.status.idle":"2025-11-19T07:39:37.708147Z","shell.execute_reply.started":"2025-11-19T07:39:37.702845Z","shell.execute_reply":"2025-11-19T07:39:37.707274Z"}},"outputs":[{"name":"stdout","text":"[initialize_training_log] Initialized training log file at training_results.log\n[initialize_validation_log] Initialized validation log file at validation_results.log\n","output_type":"stream"}],"execution_count":4},{"id":"da2df4b7","cell_type":"markdown","source":"## Load the experiment configurations\n\nPrior to running this notebook, the experiment configurations will be set in a 'config.json' file, which will be loaded to set the model hyperparameters and training settings.","metadata":{"id":"da2df4b7"}},{"id":"f290991b","cell_type":"code","source":"config = fn.load_config(f\"{kaggle_dir}config.json\")\n\nprint(f\"We will be conducting {config['description']}.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f290991b","outputId":"92cea26f-f947-47d1-e88e-8b979827c139","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:39.458131Z","iopub.execute_input":"2025-11-19T07:39:39.458866Z","iopub.status.idle":"2025-11-19T07:39:39.467732Z","shell.execute_reply.started":"2025-11-19T07:39:39.458840Z","shell.execute_reply":"2025-11-19T07:39:39.467025Z"}},"outputs":[{"name":"stdout","text":"[load_config] Loaded configuration from /kaggle/input/dropout-0p1/config.json\nWe will be conducting  Hyperparameter tuning for dropout value at 0.1.\n","output_type":"stream"}],"execution_count":5},{"id":"31bb4815","cell_type":"code","source":"# Load the seed\nseed = config['seed']\n\n# Model parameters\nvocab_size = config['model']['vocab_size']\nd_model = config['model']['d_model']\nn_heads = config['model']['n_heads']\nn_layers = config['model']['n_layers']\nmlp_ratio = config['model']['mlp_ratio']\nseq_len = config['model']['seq_len']\n\n# Training parameters\nloss_type = config['model']['loss_type']\ndropout_rate = config['model']['dropout']\nweight_decay = config['model']['weight_decay']\nlabel_smoothing = float(config['model']['label_smoothing'])\n\n# Mixed precision and other model settings\nuse_mixed_precision = config['model']['mixed_precision']\npos_encoding = config['model']['pos_encoding']\nattention_type = config['model']['attention_type']\n\n# Auxiliary loss settings\nuse_auxiliary_loss = config['model']['use_auxiliary_loss']\naux_heads = config['model']['aux_heads']\naux_weight = config['model']['aux_weight']","metadata":{"id":"31bb4815","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:42.061557Z","iopub.execute_input":"2025-11-19T07:39:42.062140Z","iopub.status.idle":"2025-11-19T07:39:42.067384Z","shell.execute_reply.started":"2025-11-19T07:39:42.062114Z","shell.execute_reply":"2025-11-19T07:39:42.066739Z"}},"outputs":[],"execution_count":6},{"id":"cf23122e","cell_type":"code","source":"# Throughput test parameters\nmax_test_iters = config['throughput']['max_test_iters']\nmax_test_time_in_seconds = config['throughput']['max_test_time_in_seconds']\ncompute_budget_hours = config['throughput']['compute_budget_hours']\n\n# Training settings\nval_fraction = config['training']['val_fraction']\nbatch_size = config['training']['batch_size']\nlearning_rate = config['training']['learning_rate']\nlr_schedule = config['training']['lr_schedule']\noptimizer_type = config['training']['optimizer']\ngrad_clip = config['training']['grad_clip']","metadata":{"id":"cf23122e","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:44.741673Z","iopub.execute_input":"2025-11-19T07:39:44.741959Z","iopub.status.idle":"2025-11-19T07:39:44.746614Z","shell.execute_reply.started":"2025-11-19T07:39:44.741938Z","shell.execute_reply":"2025-11-19T07:39:44.745766Z"}},"outputs":[],"execution_count":7},{"id":"4030d754","cell_type":"markdown","source":"# Loading the Data\n\nThe same text8 dataset is used, which has 100M characters of text data from Wikipedia articles. It contains only lowercase letters and spaces, and is already pre-split into 90M characters for training and 10M characters for testing.","metadata":{"id":"4030d754"}},{"id":"75d76b31","cell_type":"code","source":"# Read in training text file\nwith open(f\"{kaggle_dir}text8_train.txt\", 'r', encoding='utf-8') as f:\n    train_text = f.read()\nprint(f\"Training text loaded. Length: {len(train_text) :,} characters.\")\n\n# Inspect first 500 characters of training text\nprint(\"First 500 characters of training text:\")\nprint(train_text[:500])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75d76b31","outputId":"f7eef75b-29f8-4ea9-f9e0-7b219821686e","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:46.689337Z","iopub.execute_input":"2025-11-19T07:39:46.689931Z","iopub.status.idle":"2025-11-19T07:39:47.318788Z","shell.execute_reply.started":"2025-11-19T07:39:46.689906Z","shell.execute_reply":"2025-11-19T07:39:47.318109Z"}},"outputs":[{"name":"stdout","text":"Training text loaded. Length: 90,000,000 characters.\nFirst 500 characters of training text:\n anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n","output_type":"stream"}],"execution_count":8},{"id":"7fd37a2d","cell_type":"code","source":"chars = sorted(set(train_text)) # unique characters in training text\nchars_to_int = {ch: i for i, ch in enumerate(chars)} # char to int mapping\nint_to_chars = {i: ch for i, ch in enumerate(chars)} # int to char mapping\n\nprint(f\"Unique characters in training text: {len(chars)}\") # should be 27, including space (sanity check)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fd37a2d","outputId":"0ef808be-a050-47bc-950d-bdb25903339d","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:49.134347Z","iopub.execute_input":"2025-11-19T07:39:49.135078Z","iopub.status.idle":"2025-11-19T07:39:49.738254Z","shell.execute_reply.started":"2025-11-19T07:39:49.135051Z","shell.execute_reply":"2025-11-19T07:39:49.737648Z"}},"outputs":[{"name":"stdout","text":"Unique characters in training text: 27\n","output_type":"stream"}],"execution_count":9},{"id":"032e68f0","cell_type":"markdown","source":"We further split the training data into a training set and a validation set to monitor the model's performance during training, in accordance to the validation fraction specified in our configuration file (10%)","metadata":{"id":"032e68f0"}},{"id":"9a969636","cell_type":"code","source":"train_text, val_text = fn.split_train_val(train_text, val_fraction=val_fraction)\n\nprint(f\"Training text length: {len(train_text) :,} characters.\")\nprint(f\"Validation text length: {len(val_text) :,} characters.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a969636","outputId":"119b07fe-ee8e-46a5-f374-b2ec9a3f6c4f","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:51.519784Z","iopub.execute_input":"2025-11-19T07:39:51.520072Z","iopub.status.idle":"2025-11-19T07:39:51.614346Z","shell.execute_reply.started":"2025-11-19T07:39:51.520051Z","shell.execute_reply":"2025-11-19T07:39:51.613087Z"}},"outputs":[{"name":"stdout","text":"Training text length: 89,099,996 characters.\nValidation text length: 900,004 characters.\n","output_type":"stream"}],"execution_count":10},{"id":"1c28ac68","cell_type":"markdown","source":"# Model Initialisation","metadata":{"id":"1c28ac68"}},{"id":"7fe304ec","cell_type":"markdown","source":"## Model Setup\n\nWe intialise our model with the following parameters in accordance to our configuration file.\nBased on these parameters, our model has approximately ~4.2M parameters.","metadata":{"id":"7fe304ec"}},{"id":"d19f06bd","cell_type":"code","source":"# Define the model params\nrng = jax.random.PRNGKey(seed)\n\nmodel_obj, params, constants = fn.create_train_state(\n        rng,\n        vocab_size = vocab_size,\n        d_model = d_model,\n        n_heads = n_heads,\n        n_layers = n_layers,\n        mlp_ratio = mlp_ratio,\n        seq_len = seq_len,\n        dropout = dropout_rate,\n        aux_loss = use_auxiliary_loss,\n        num_aux_heads = aux_heads,\n        mixed_precision = use_mixed_precision,\n        attention_type = attention_type,\n        pos_encoding = pos_encoding\n)","metadata":{"id":"d19f06bd","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:39:58.197573Z","iopub.execute_input":"2025-11-19T07:39:58.197854Z","iopub.status.idle":"2025-11-19T07:40:07.733817Z","shell.execute_reply.started":"2025-11-19T07:39:58.197835Z","shell.execute_reply":"2025-11-19T07:40:07.733111Z"}},"outputs":[],"execution_count":11},{"id":"91839933","cell_type":"code","source":"total_params = fn.count_parameters(params)\n\nprint(f\"Total number of parameters in the model: {total_params :,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91839933","outputId":"ba0a9b96-0f75-4307-d875-9ec206b1b8e4","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:40:07.734915Z","iopub.execute_input":"2025-11-19T07:40:07.735118Z","iopub.status.idle":"2025-11-19T07:40:07.740175Z","shell.execute_reply.started":"2025-11-19T07:40:07.735100Z","shell.execute_reply":"2025-11-19T07:40:07.739307Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters in the model: 4,160,768\n","output_type":"stream"}],"execution_count":12},{"id":"e709b510","cell_type":"markdown","source":"We perform a sanity check by running a single forward pass with random input data to ensure the model is functioning as expected.","metadata":{"id":"e709b510"}},{"id":"bdc7a187","cell_type":"code","source":"# SANITY CHECK: Test the model forward pass\nB, T = 2, 8  # Batch size and sequence length\nbatch = jax.random.randint(key = rng, shape = (B, T), minval = 0, maxval = vocab_size)\n\nvariables = {\"params\": params, \"constants\": constants}\noutput = model_obj.apply(variables, batch, deterministic=False, rngs={\"dropout\": rng})\nprint(\"Logits shape:\", output[\"logits\"].shape)  # Expected: (B, T, vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdc7a187","outputId":"81237094-534d-4a66-9fad-6faac1bbd600","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:40:08.651074Z","iopub.execute_input":"2025-11-19T07:40:08.651354Z","iopub.status.idle":"2025-11-19T07:40:13.351339Z","shell.execute_reply.started":"2025-11-19T07:40:08.651333Z","shell.execute_reply":"2025-11-19T07:40:13.350679Z"}},"outputs":[{"name":"stdout","text":"Logits shape: (2, 8, 27)\n","output_type":"stream"}],"execution_count":13},{"id":"ec22aeee","cell_type":"markdown","source":"## Initialise the optimizer\n\nIn this section, we set up the optimizer for training our model in accordance to our configuration file.","metadata":{"id":"ec22aeee"}},{"id":"05d04750","cell_type":"code","source":"# Define the learning rate\nlearning_rate = learning_rate\n\n# Create the Optimizer and initialize it\nif optimizer_type == \"adam\":\n    optimizer = optax.adam(learning_rate)\nelif optimizer_type == \"sgd\":\n    optimizer = optax.sgd(learning_rate)\nelif optimizer_type == \"adamw\":\n    optimizer = optax.adamw(\n        learning_rate = learning_rate,\n        weight_decay = weight_decay\n    )\n\n# Add gradient clipping if specified\nif grad_clip is not None and grad_clip != \"none\":\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(grad_clip),\n        optimizer\n    )\n\n\nopt_state = optimizer.init(params)\n\nprint(\"Optimizer initialized:\", optimizer_type, \"with Learning Rate =\", learning_rate)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05d04750","outputId":"fb412264-79b9-4770-e4e6-ac448e4c992e","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:40:16.127197Z","iopub.execute_input":"2025-11-19T07:40:16.127803Z","iopub.status.idle":"2025-11-19T07:40:16.463520Z","shell.execute_reply.started":"2025-11-19T07:40:16.127779Z","shell.execute_reply":"2025-11-19T07:40:16.462804Z"}},"outputs":[{"name":"stdout","text":"Optimizer initialized: adam with Learning Rate = 0.001\n","output_type":"stream"}],"execution_count":14},{"id":"233c7fe3","cell_type":"markdown","source":"## Text encoding\n\nWe then encode the text data into integer format for model training. Each unique character is mapped to a unique integer index.","metadata":{"id":"233c7fe3"}},{"id":"17829ea8","cell_type":"code","source":"# Encode the train, val, test texts\ntrain_data = fn.encode(train_text, chars_to_int)\nval_data = fn.encode(val_text, chars_to_int)","metadata":{"id":"17829ea8","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:40:18.747359Z","iopub.execute_input":"2025-11-19T07:40:18.747986Z","iopub.status.idle":"2025-11-19T07:40:25.916239Z","shell.execute_reply.started":"2025-11-19T07:40:18.747962Z","shell.execute_reply":"2025-11-19T07:40:25.915422Z"}},"outputs":[],"execution_count":15},{"id":"153631ec","cell_type":"markdown","source":"## Determine maximum permissible training steps\n\nTaking into account possible compute limitations, we perform a preliminary calculation to determine the maximum number of training steps we can perform based on the throughput of our model and the total training time available. For this preliminary test, the default maximum training time is 60 seconds, and commpute budget hours is 2 hours.\n\nBased on the throughput calculated from the preliminary test, the estimated maximum no. of training steps we can perform within this compute budget is ~100,000. To ensure we keep within the budget, we set the maximum training steps to be 50,000.","metadata":{"id":"153631ec"}},{"id":"1312e527","cell_type":"code","source":"# Determining how many steps we can run in a reasonable time\nmax_iters = max_test_iters\nmax_time = max_test_time_in_seconds # in seconds\nmax_compute_time = compute_budget_hours # in hours\n\n_ , max_steps = fn.calculate_throughput(\n    max_test_iters = max_iters,\n    max_test_time = max_time,\n    model = model_obj,\n    params = params,\n    opt_state = opt_state,\n    optimizer = optimizer,\n    rng = rng,\n    batch_size = batch_size,\n    seq_len = seq_len,\n    compute_budget = max_compute_time,\n    train_data = train_data,\n    loss_type = loss_type,\n    aux_loss = use_auxiliary_loss,\n    aux_weight = aux_weight,\n    constants = constants,\n    label_smoothing = label_smoothing\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1312e527","outputId":"7961ac93-8c66-4d74-f71a-6787aa4ffad6","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:40:25.917272Z","iopub.execute_input":"2025-11-19T07:40:25.917449Z","iopub.status.idle":"2025-11-19T07:41:25.962281Z","shell.execute_reply.started":"2025-11-19T07:40:25.917434Z","shell.execute_reply":"2025-11-19T07:41:25.961534Z"}},"outputs":[{"name":"stdout","text":"Stopping benchmark at iteration 871 due to time limit.\nBenchmark completed in 60.04 seconds.\nTotal tokens processed: 7143424\nThroughput: 118978.91 tokens/second\nEstimated max steps within compute budget: 104571.0\n","output_type":"stream"}],"execution_count":16},{"id":"ff66f55f","cell_type":"markdown","source":"# Model Training & Evaluation","metadata":{"id":"ff66f55f"}},{"id":"b839549c","cell_type":"markdown","source":"## Training the model\n\nNow, we proceed to train the model over the determined number of training iterations. During training, we monitor the training loss and periodically evaluate the model on the validation set to track its performance. We also make sure to record the time taken for training to ensure it stays within our compute budget.","metadata":{"id":"b839549c"}},{"id":"1262eda3","cell_type":"code","source":"iter_max = 50000\n\n# To track training and validation loss, as well as time taken\ntrain_loss_history = []\nval_loss_history = []\ntrain_step_history = list(range(iter_max))\nval_step_history = []\n\n# Load checkpoint if it exists\nparams, opt_state, constants, start_iter = fn.load_checkpoint(\n    checkpoint_file,\n    params,\n    constants,\n    opt_state\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1262eda3","outputId":"bc8a88d5-d946-4bf7-f70d-4ea509e7045b","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:41:58.274645Z","iopub.execute_input":"2025-11-19T07:41:58.275250Z","iopub.status.idle":"2025-11-19T07:41:58.281306Z","shell.execute_reply.started":"2025-11-19T07:41:58.275226Z","shell.execute_reply":"2025-11-19T07:41:58.280430Z"}},"outputs":[{"name":"stdout","text":"[load_checkpoint] No checkpoint found at checkpoint.pkl\n[load_checkpoint] Starting training as per usual.\n","output_type":"stream"}],"execution_count":17},{"id":"cd2b91d3","cell_type":"markdown","source":"We then train the model and log the training and validation losses for analysis.","metadata":{"id":"cd2b91d3"}},{"id":"c7cbde26","cell_type":"code","source":"if start_iter > 0:\n        print(f\"Resuming training from iteration = {start_iter}.\")\nelse:\n        print(\"Starting training from iteration = 0.\")\n\ntime_start = time.time()\n\nfor it in range(start_iter, iter_max):\n\n    # get a batch of data\n    inputs, targets = fn.get_batch(train_data, batch_size, seq_len)\n\n    # Perform a training step\n    rng, sub = jax.random.split(rng)\n    new_params, new_opt_state, metrics = fn.train_step(\n            model = model_obj,\n            params = params,\n            constants=constants,\n            opt_state = opt_state,\n            x = inputs,\n            y = targets,\n            tx = optimizer,\n            rng = sub,\n            loss_type = loss_type,\n            aux_loss = use_auxiliary_loss,\n            aux_weight = aux_weight,\n            label_smoothing = label_smoothing\n    )\n\n    # Update parameters and optimizer state\n    params = new_params\n    opt_state = new_opt_state\n\n    # Record training metrics\n    acc = metrics['acc']\n    loss = metrics['loss']\n    last_char_acc = metrics['acc_last']\n    train_time = time.time() - time_start\n\n    train_loss_history.append(loss)\n\n    fn.update_training_log(\n        log_path = \"training_results.log\",\n        step = it,\n        train_loss = loss,\n        train_time = train_time,\n        train_acc = acc,\n        last_char_acc = last_char_acc\n        )\n\n    log_every = max(1, iter_max // 100)\n\n    if (it % log_every) == 0 or (it == iter_max - 1): # Print every 1% of iterations\n\n        # Compute the loss on validation set\n        batch_size_val = batch_size\n        seq_len_val = seq_len\n        val_inputs, val_targets = fn.get_batch(val_data, batch_size_val, seq_len_val)\n\n        val_out = model_obj.apply({\"params\": params, \"constants\": constants}, val_inputs, deterministic=True)\n        val_logits = val_out[\"logits\"]\n        val_aux_logits = val_out.get('aux_logits', None)\n\n        val_loss, val_metrics = fn.loss_and_metrics(\n            logits = val_logits,\n            targets = val_targets,\n            loss_type = loss_type,\n            aux_loss = use_auxiliary_loss,\n            aux_logits = val_aux_logits,\n            aux_weight = aux_weight,\n            label_smoothing = label_smoothing\n        )\n\n        # Record validation loss and time\n        val_acc = val_metrics['acc']\n        last_char_acc_val = val_metrics['acc_last']\n        val_loss_history.append(val_loss)\n        time_elapsed = time.time() - time_start\n        val_step_history.append(it)\n\n        fn.update_validation_log(\n            log_path = \"validation_results.log\",\n            step = it,\n            val_loss = val_loss,\n            val_time = time_elapsed,\n            val_acc = val_acc,\n            last_char_val_acc = last_char_acc_val\n        )\n\n        fn.save_checkpoint(\n            checkpoint_path = checkpoint_file,\n            params = params,\n            constants = constants,\n            opt_state = opt_state,\n            step = it,\n            time_elapsed = time_elapsed\n        )\n\n        # Print training and validation metrics\n        print(f\"Iteration {it}, time elapsed: {time_elapsed:.2f} seconds\")\n        print(f\"\\t \\t Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")\n        print(f\"\\t \\t Training Acc: {acc:.4f}, Validation Acc: {val_acc:.4f}\")\n        print(f\"\\t \\t Last Char Training Acc: {last_char_acc:.4f}, Last Char Validation Acc: {last_char_acc_val:.4f}\")\n        print(\"-\" * 50)\n\nprint(f\"Training completed in {time.time() - time_start:.2f} seconds.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7cbde26","outputId":"8478b89e-dafe-4e69-8274-aa483af8ca9e","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:42:01.037404Z","iopub.execute_input":"2025-11-19T07:42:01.038109Z","iopub.status.idle":"2025-11-19T08:30:09.046828Z","shell.execute_reply.started":"2025-11-19T07:42:01.038084Z","shell.execute_reply":"2025-11-19T08:30:09.045942Z"}},"outputs":[{"name":"stdout","text":"Starting training from iteration = 0.\n[save_checkpoint] Saved checkpoint at step 0 to checkpoint.pkl\nIteration 0, time elapsed: 4.32 seconds\n\t \t Training Loss: 3.7987, Validation Loss: 4.5694\n\t \t Training Acc: 0.0469, Validation Acc: 0.1664\n\t \t Last Char Training Acc: 0.0625, Last Char Validation Acc: 0.2188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 500 to checkpoint.pkl\nIteration 500, time elapsed: 33.33 seconds\n\t \t Training Loss: 1.6759, Validation Loss: 1.7217\n\t \t Training Acc: 0.4945, Validation Acc: 0.4854\n\t \t Last Char Training Acc: 0.4062, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 1000 to checkpoint.pkl\nIteration 1000, time elapsed: 62.45 seconds\n\t \t Training Loss: 1.5608, Validation Loss: 1.4118\n\t \t Training Acc: 0.5240, Validation Acc: 0.5590\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 1500 to checkpoint.pkl\nIteration 1500, time elapsed: 91.48 seconds\n\t \t Training Loss: 1.4696, Validation Loss: 1.3494\n\t \t Training Acc: 0.5427, Validation Acc: 0.5767\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 2000 to checkpoint.pkl\nIteration 2000, time elapsed: 120.71 seconds\n\t \t Training Loss: 1.4087, Validation Loss: 1.3889\n\t \t Training Acc: 0.5609, Validation Acc: 0.5668\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 2500 to checkpoint.pkl\nIteration 2500, time elapsed: 149.92 seconds\n\t \t Training Loss: 1.4282, Validation Loss: 1.3691\n\t \t Training Acc: 0.5562, Validation Acc: 0.5719\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.4062\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 3000 to checkpoint.pkl\nIteration 3000, time elapsed: 179.07 seconds\n\t \t Training Loss: 1.3561, Validation Loss: 1.3644\n\t \t Training Acc: 0.5750, Validation Acc: 0.5724\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 3500 to checkpoint.pkl\nIteration 3500, time elapsed: 208.22 seconds\n\t \t Training Loss: 1.3759, Validation Loss: 1.3416\n\t \t Training Acc: 0.5667, Validation Acc: 0.5836\n\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 4000 to checkpoint.pkl\nIteration 4000, time elapsed: 237.29 seconds\n\t \t Training Loss: 1.3583, Validation Loss: 1.3651\n\t \t Training Acc: 0.5736, Validation Acc: 0.5646\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 4500 to checkpoint.pkl\nIteration 4500, time elapsed: 266.49 seconds\n\t \t Training Loss: 1.3324, Validation Loss: 1.3788\n\t \t Training Acc: 0.5779, Validation Acc: 0.5741\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 5000 to checkpoint.pkl\nIteration 5000, time elapsed: 295.64 seconds\n\t \t Training Loss: 1.3516, Validation Loss: 1.3069\n\t \t Training Acc: 0.5822, Validation Acc: 0.5906\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 5500 to checkpoint.pkl\nIteration 5500, time elapsed: 324.88 seconds\n\t \t Training Loss: 1.2945, Validation Loss: 1.2779\n\t \t Training Acc: 0.5917, Validation Acc: 0.5946\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 6000 to checkpoint.pkl\nIteration 6000, time elapsed: 353.94 seconds\n\t \t Training Loss: 1.3596, Validation Loss: 1.2762\n\t \t Training Acc: 0.5789, Validation Acc: 0.6040\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 6500 to checkpoint.pkl\nIteration 6500, time elapsed: 382.93 seconds\n\t \t Training Loss: 1.3423, Validation Loss: 1.2681\n\t \t Training Acc: 0.5826, Validation Acc: 0.6007\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 7000 to checkpoint.pkl\nIteration 7000, time elapsed: 412.12 seconds\n\t \t Training Loss: 1.3309, Validation Loss: 1.2786\n\t \t Training Acc: 0.5846, Validation Acc: 0.6001\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 7500 to checkpoint.pkl\nIteration 7500, time elapsed: 441.26 seconds\n\t \t Training Loss: 1.2917, Validation Loss: 1.2959\n\t \t Training Acc: 0.5968, Validation Acc: 0.6027\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 8000 to checkpoint.pkl\nIteration 8000, time elapsed: 470.42 seconds\n\t \t Training Loss: 1.3296, Validation Loss: 1.2363\n\t \t Training Acc: 0.5876, Validation Acc: 0.6163\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 8500 to checkpoint.pkl\nIteration 8500, time elapsed: 499.51 seconds\n\t \t Training Loss: 1.2657, Validation Loss: 1.2080\n\t \t Training Acc: 0.6030, Validation Acc: 0.6178\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 9000 to checkpoint.pkl\nIteration 9000, time elapsed: 528.62 seconds\n\t \t Training Loss: 1.3069, Validation Loss: 1.2476\n\t \t Training Acc: 0.5944, Validation Acc: 0.6058\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 9500 to checkpoint.pkl\nIteration 9500, time elapsed: 557.72 seconds\n\t \t Training Loss: 1.2873, Validation Loss: 1.2312\n\t \t Training Acc: 0.5917, Validation Acc: 0.6101\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 10000 to checkpoint.pkl\nIteration 10000, time elapsed: 586.87 seconds\n\t \t Training Loss: 1.2483, Validation Loss: 1.2420\n\t \t Training Acc: 0.6071, Validation Acc: 0.6075\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 10500 to checkpoint.pkl\nIteration 10500, time elapsed: 616.06 seconds\n\t \t Training Loss: 1.2680, Validation Loss: 1.2491\n\t \t Training Acc: 0.5963, Validation Acc: 0.6075\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 11000 to checkpoint.pkl\nIteration 11000, time elapsed: 645.04 seconds\n\t \t Training Loss: 1.2827, Validation Loss: 1.2637\n\t \t Training Acc: 0.5967, Validation Acc: 0.6027\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 11500 to checkpoint.pkl\nIteration 11500, time elapsed: 674.11 seconds\n\t \t Training Loss: 1.3169, Validation Loss: 1.1841\n\t \t Training Acc: 0.5878, Validation Acc: 0.6234\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 12000 to checkpoint.pkl\nIteration 12000, time elapsed: 703.23 seconds\n\t \t Training Loss: 1.2701, Validation Loss: 1.2068\n\t \t Training Acc: 0.5986, Validation Acc: 0.6185\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 12500 to checkpoint.pkl\nIteration 12500, time elapsed: 732.27 seconds\n\t \t Training Loss: 1.2425, Validation Loss: 1.2252\n\t \t Training Acc: 0.6105, Validation Acc: 0.6117\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 13000 to checkpoint.pkl\nIteration 13000, time elapsed: 761.30 seconds\n\t \t Training Loss: 1.2743, Validation Loss: 1.2081\n\t \t Training Acc: 0.5948, Validation Acc: 0.6180\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 13500 to checkpoint.pkl\nIteration 13500, time elapsed: 790.32 seconds\n\t \t Training Loss: 1.2135, Validation Loss: 1.3249\n\t \t Training Acc: 0.6118, Validation Acc: 0.5811\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 14000 to checkpoint.pkl\nIteration 14000, time elapsed: 819.36 seconds\n\t \t Training Loss: 1.2585, Validation Loss: 1.2141\n\t \t Training Acc: 0.6005, Validation Acc: 0.6211\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 14500 to checkpoint.pkl\nIteration 14500, time elapsed: 848.44 seconds\n\t \t Training Loss: 1.2339, Validation Loss: 1.2340\n\t \t Training Acc: 0.6139, Validation Acc: 0.6139\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 15000 to checkpoint.pkl\nIteration 15000, time elapsed: 877.48 seconds\n\t \t Training Loss: 1.2387, Validation Loss: 1.2171\n\t \t Training Acc: 0.6151, Validation Acc: 0.6182\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 15500 to checkpoint.pkl\nIteration 15500, time elapsed: 906.53 seconds\n\t \t Training Loss: 1.1756, Validation Loss: 1.2690\n\t \t Training Acc: 0.6312, Validation Acc: 0.6053\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 16000 to checkpoint.pkl\nIteration 16000, time elapsed: 935.63 seconds\n\t \t Training Loss: 1.2492, Validation Loss: 1.1867\n\t \t Training Acc: 0.6125, Validation Acc: 0.6251\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 16500 to checkpoint.pkl\nIteration 16500, time elapsed: 964.72 seconds\n\t \t Training Loss: 1.2468, Validation Loss: 1.2504\n\t \t Training Acc: 0.6130, Validation Acc: 0.6074\n\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 17000 to checkpoint.pkl\nIteration 17000, time elapsed: 993.75 seconds\n\t \t Training Loss: 1.2356, Validation Loss: 1.1519\n\t \t Training Acc: 0.6116, Validation Acc: 0.6340\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 17500 to checkpoint.pkl\nIteration 17500, time elapsed: 1022.95 seconds\n\t \t Training Loss: 1.1856, Validation Loss: 1.1488\n\t \t Training Acc: 0.6182, Validation Acc: 0.6342\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 18000 to checkpoint.pkl\nIteration 18000, time elapsed: 1052.03 seconds\n\t \t Training Loss: 1.2329, Validation Loss: 1.0837\n\t \t Training Acc: 0.6129, Validation Acc: 0.6538\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 18500 to checkpoint.pkl\nIteration 18500, time elapsed: 1081.03 seconds\n\t \t Training Loss: 1.2092, Validation Loss: 1.1846\n\t \t Training Acc: 0.6193, Validation Acc: 0.6299\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 19000 to checkpoint.pkl\nIteration 19000, time elapsed: 1109.96 seconds\n\t \t Training Loss: 1.2804, Validation Loss: 1.1697\n\t \t Training Acc: 0.6063, Validation Acc: 0.6371\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 19500 to checkpoint.pkl\nIteration 19500, time elapsed: 1138.96 seconds\n\t \t Training Loss: 1.2404, Validation Loss: 1.1733\n\t \t Training Acc: 0.6105, Validation Acc: 0.6327\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 20000 to checkpoint.pkl\nIteration 20000, time elapsed: 1168.01 seconds\n\t \t Training Loss: 1.1945, Validation Loss: 1.1754\n\t \t Training Acc: 0.6326, Validation Acc: 0.6357\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 20500 to checkpoint.pkl\nIteration 20500, time elapsed: 1197.03 seconds\n\t \t Training Loss: 1.1778, Validation Loss: 1.1980\n\t \t Training Acc: 0.6307, Validation Acc: 0.6224\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5312\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 21000 to checkpoint.pkl\nIteration 21000, time elapsed: 1226.09 seconds\n\t \t Training Loss: 1.2417, Validation Loss: 1.1301\n\t \t Training Acc: 0.6085, Validation Acc: 0.6389\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 21500 to checkpoint.pkl\nIteration 21500, time elapsed: 1255.19 seconds\n\t \t Training Loss: 1.2186, Validation Loss: 1.0939\n\t \t Training Acc: 0.6187, Validation Acc: 0.6526\n\t \t Last Char Training Acc: 0.5000, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 22000 to checkpoint.pkl\nIteration 22000, time elapsed: 1284.24 seconds\n\t \t Training Loss: 1.2303, Validation Loss: 1.1159\n\t \t Training Acc: 0.6166, Validation Acc: 0.6553\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 22500 to checkpoint.pkl\nIteration 22500, time elapsed: 1313.29 seconds\n\t \t Training Loss: 1.1967, Validation Loss: 1.1679\n\t \t Training Acc: 0.6318, Validation Acc: 0.6384\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 23000 to checkpoint.pkl\nIteration 23000, time elapsed: 1342.46 seconds\n\t \t Training Loss: 1.2013, Validation Loss: 1.1712\n\t \t Training Acc: 0.6235, Validation Acc: 0.6294\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 23500 to checkpoint.pkl\nIteration 23500, time elapsed: 1371.39 seconds\n\t \t Training Loss: 1.1912, Validation Loss: 1.2021\n\t \t Training Acc: 0.6271, Validation Acc: 0.6295\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 24000 to checkpoint.pkl\nIteration 24000, time elapsed: 1400.03 seconds\n\t \t Training Loss: 1.1499, Validation Loss: 1.1724\n\t \t Training Acc: 0.6366, Validation Acc: 0.6343\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 24500 to checkpoint.pkl\nIteration 24500, time elapsed: 1428.66 seconds\n\t \t Training Loss: 1.1624, Validation Loss: 1.1562\n\t \t Training Acc: 0.6376, Validation Acc: 0.6377\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 25000 to checkpoint.pkl\nIteration 25000, time elapsed: 1457.28 seconds\n\t \t Training Loss: 1.1569, Validation Loss: 1.1334\n\t \t Training Acc: 0.6376, Validation Acc: 0.6478\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 25500 to checkpoint.pkl\nIteration 25500, time elapsed: 1485.94 seconds\n\t \t Training Loss: 1.1480, Validation Loss: 1.1159\n\t \t Training Acc: 0.6334, Validation Acc: 0.6544\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 26000 to checkpoint.pkl\nIteration 26000, time elapsed: 1514.62 seconds\n\t \t Training Loss: 1.1729, Validation Loss: 1.1869\n\t \t Training Acc: 0.6313, Validation Acc: 0.6354\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 26500 to checkpoint.pkl\nIteration 26500, time elapsed: 1543.29 seconds\n\t \t Training Loss: 1.1027, Validation Loss: 1.1411\n\t \t Training Acc: 0.6517, Validation Acc: 0.6409\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.4688\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 27000 to checkpoint.pkl\nIteration 27000, time elapsed: 1571.98 seconds\n\t \t Training Loss: 1.1651, Validation Loss: 1.1129\n\t \t Training Acc: 0.6337, Validation Acc: 0.6489\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 27500 to checkpoint.pkl\nIteration 27500, time elapsed: 1600.67 seconds\n\t \t Training Loss: 1.1786, Validation Loss: 1.1073\n\t \t Training Acc: 0.6376, Validation Acc: 0.6525\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 28000 to checkpoint.pkl\nIteration 28000, time elapsed: 1629.35 seconds\n\t \t Training Loss: 1.1709, Validation Loss: 1.1641\n\t \t Training Acc: 0.6266, Validation Acc: 0.6331\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 28500 to checkpoint.pkl\nIteration 28500, time elapsed: 1658.02 seconds\n\t \t Training Loss: 1.1750, Validation Loss: 1.1284\n\t \t Training Acc: 0.6327, Validation Acc: 0.6471\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 29000 to checkpoint.pkl\nIteration 29000, time elapsed: 1686.75 seconds\n\t \t Training Loss: 1.2026, Validation Loss: 1.1367\n\t \t Training Acc: 0.6216, Validation Acc: 0.6415\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 29500 to checkpoint.pkl\nIteration 29500, time elapsed: 1715.38 seconds\n\t \t Training Loss: 1.1859, Validation Loss: 1.0893\n\t \t Training Acc: 0.6238, Validation Acc: 0.6616\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 30000 to checkpoint.pkl\nIteration 30000, time elapsed: 1743.75 seconds\n\t \t Training Loss: 1.1365, Validation Loss: 1.1187\n\t \t Training Acc: 0.6433, Validation Acc: 0.6471\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 30500 to checkpoint.pkl\nIteration 30500, time elapsed: 1771.77 seconds\n\t \t Training Loss: 1.1457, Validation Loss: 1.1147\n\t \t Training Acc: 0.6401, Validation Acc: 0.6523\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 31000 to checkpoint.pkl\nIteration 31000, time elapsed: 1799.84 seconds\n\t \t Training Loss: 1.1556, Validation Loss: 1.1003\n\t \t Training Acc: 0.6390, Validation Acc: 0.6511\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 31500 to checkpoint.pkl\nIteration 31500, time elapsed: 1828.15 seconds\n\t \t Training Loss: 1.1480, Validation Loss: 1.1328\n\t \t Training Acc: 0.6379, Validation Acc: 0.6506\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 32000 to checkpoint.pkl\nIteration 32000, time elapsed: 1856.54 seconds\n\t \t Training Loss: 1.1600, Validation Loss: 1.0755\n\t \t Training Acc: 0.6362, Validation Acc: 0.6637\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 32500 to checkpoint.pkl\nIteration 32500, time elapsed: 1884.89 seconds\n\t \t Training Loss: 1.2128, Validation Loss: 1.0986\n\t \t Training Acc: 0.6271, Validation Acc: 0.6525\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 33000 to checkpoint.pkl\nIteration 33000, time elapsed: 1913.12 seconds\n\t \t Training Loss: 1.1686, Validation Loss: 1.0279\n\t \t Training Acc: 0.6372, Validation Acc: 0.6741\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 33500 to checkpoint.pkl\nIteration 33500, time elapsed: 1941.21 seconds\n\t \t Training Loss: 1.1338, Validation Loss: 1.1010\n\t \t Training Acc: 0.6449, Validation Acc: 0.6506\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 34000 to checkpoint.pkl\nIteration 34000, time elapsed: 1969.66 seconds\n\t \t Training Loss: 1.1073, Validation Loss: 1.0596\n\t \t Training Acc: 0.6487, Validation Acc: 0.6639\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 34500 to checkpoint.pkl\nIteration 34500, time elapsed: 1998.34 seconds\n\t \t Training Loss: 1.1840, Validation Loss: 1.0928\n\t \t Training Acc: 0.6333, Validation Acc: 0.6571\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.8125\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 35000 to checkpoint.pkl\nIteration 35000, time elapsed: 2027.02 seconds\n\t \t Training Loss: 1.1587, Validation Loss: 1.1866\n\t \t Training Acc: 0.6431, Validation Acc: 0.6292\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 35500 to checkpoint.pkl\nIteration 35500, time elapsed: 2055.31 seconds\n\t \t Training Loss: 1.1557, Validation Loss: 1.0969\n\t \t Training Acc: 0.6389, Validation Acc: 0.6588\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 36000 to checkpoint.pkl\nIteration 36000, time elapsed: 2083.87 seconds\n\t \t Training Loss: 1.1571, Validation Loss: 1.1338\n\t \t Training Acc: 0.6382, Validation Acc: 0.6483\n\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 36500 to checkpoint.pkl\nIteration 36500, time elapsed: 2112.42 seconds\n\t \t Training Loss: 1.1510, Validation Loss: 1.1184\n\t \t Training Acc: 0.6418, Validation Acc: 0.6503\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 37000 to checkpoint.pkl\nIteration 37000, time elapsed: 2140.88 seconds\n\t \t Training Loss: 1.1217, Validation Loss: 1.1248\n\t \t Training Acc: 0.6472, Validation Acc: 0.6503\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 37500 to checkpoint.pkl\nIteration 37500, time elapsed: 2169.24 seconds\n\t \t Training Loss: 1.1154, Validation Loss: 1.1100\n\t \t Training Acc: 0.6556, Validation Acc: 0.6519\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 38000 to checkpoint.pkl\nIteration 38000, time elapsed: 2197.66 seconds\n\t \t Training Loss: 1.1787, Validation Loss: 1.1161\n\t \t Training Acc: 0.6318, Validation Acc: 0.6470\n\t \t Last Char Training Acc: 0.6562, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 38500 to checkpoint.pkl\nIteration 38500, time elapsed: 2226.23 seconds\n\t \t Training Loss: 1.1609, Validation Loss: 1.1372\n\t \t Training Acc: 0.6373, Validation Acc: 0.6449\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 39000 to checkpoint.pkl\nIteration 39000, time elapsed: 2254.85 seconds\n\t \t Training Loss: 1.1342, Validation Loss: 1.1519\n\t \t Training Acc: 0.6381, Validation Acc: 0.6385\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 39500 to checkpoint.pkl\nIteration 39500, time elapsed: 2283.63 seconds\n\t \t Training Loss: 1.1400, Validation Loss: 1.1336\n\t \t Training Acc: 0.6462, Validation Acc: 0.6462\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 40000 to checkpoint.pkl\nIteration 40000, time elapsed: 2312.50 seconds\n\t \t Training Loss: 1.1122, Validation Loss: 1.1722\n\t \t Training Acc: 0.6494, Validation Acc: 0.6312\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.8438\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 40500 to checkpoint.pkl\nIteration 40500, time elapsed: 2341.38 seconds\n\t \t Training Loss: 1.2079, Validation Loss: 1.0520\n\t \t Training Acc: 0.6240, Validation Acc: 0.6680\n\t \t Last Char Training Acc: 0.4375, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 41000 to checkpoint.pkl\nIteration 41000, time elapsed: 2370.25 seconds\n\t \t Training Loss: 1.1168, Validation Loss: 1.0881\n\t \t Training Acc: 0.6499, Validation Acc: 0.6552\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 41500 to checkpoint.pkl\nIteration 41500, time elapsed: 2399.13 seconds\n\t \t Training Loss: 1.1707, Validation Loss: 1.1646\n\t \t Training Acc: 0.6312, Validation Acc: 0.6338\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 42000 to checkpoint.pkl\nIteration 42000, time elapsed: 2427.92 seconds\n\t \t Training Loss: 1.0988, Validation Loss: 1.1303\n\t \t Training Acc: 0.6552, Validation Acc: 0.6465\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 42500 to checkpoint.pkl\nIteration 42500, time elapsed: 2456.70 seconds\n\t \t Training Loss: 1.1203, Validation Loss: 1.0504\n\t \t Training Acc: 0.6500, Validation Acc: 0.6699\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 43000 to checkpoint.pkl\nIteration 43000, time elapsed: 2485.55 seconds\n\t \t Training Loss: 1.1609, Validation Loss: 1.0177\n\t \t Training Acc: 0.6360, Validation Acc: 0.6831\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 43500 to checkpoint.pkl\nIteration 43500, time elapsed: 2514.34 seconds\n\t \t Training Loss: 1.0620, Validation Loss: 1.0849\n\t \t Training Acc: 0.6622, Validation Acc: 0.6544\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.7500\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 44000 to checkpoint.pkl\nIteration 44000, time elapsed: 2543.15 seconds\n\t \t Training Loss: 1.1805, Validation Loss: 1.1409\n\t \t Training Acc: 0.6273, Validation Acc: 0.6389\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 44500 to checkpoint.pkl\nIteration 44500, time elapsed: 2571.94 seconds\n\t \t Training Loss: 1.1292, Validation Loss: 1.1008\n\t \t Training Acc: 0.6411, Validation Acc: 0.6604\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 45000 to checkpoint.pkl\nIteration 45000, time elapsed: 2600.75 seconds\n\t \t Training Loss: 1.1885, Validation Loss: 1.0678\n\t \t Training Acc: 0.6305, Validation Acc: 0.6669\n\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.5625\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 45500 to checkpoint.pkl\nIteration 45500, time elapsed: 2629.54 seconds\n\t \t Training Loss: 1.1279, Validation Loss: 1.0600\n\t \t Training Acc: 0.6492, Validation Acc: 0.6602\n\t \t Last Char Training Acc: 0.5625, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 46000 to checkpoint.pkl\nIteration 46000, time elapsed: 2658.30 seconds\n\t \t Training Loss: 1.1631, Validation Loss: 1.1452\n\t \t Training Acc: 0.6353, Validation Acc: 0.6486\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 46500 to checkpoint.pkl\nIteration 46500, time elapsed: 2687.11 seconds\n\t \t Training Loss: 1.1602, Validation Loss: 1.0971\n\t \t Training Acc: 0.6362, Validation Acc: 0.6536\n\t \t Last Char Training Acc: 0.6875, Last Char Validation Acc: 0.6562\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 47000 to checkpoint.pkl\nIteration 47000, time elapsed: 2715.91 seconds\n\t \t Training Loss: 1.1266, Validation Loss: 1.1395\n\t \t Training Acc: 0.6475, Validation Acc: 0.6479\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.5000\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 47500 to checkpoint.pkl\nIteration 47500, time elapsed: 2744.59 seconds\n\t \t Training Loss: 1.1211, Validation Loss: 1.1072\n\t \t Training Acc: 0.6486, Validation Acc: 0.6520\n\t \t Last Char Training Acc: 0.7188, Last Char Validation Acc: 0.5938\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 48000 to checkpoint.pkl\nIteration 48000, time elapsed: 2773.26 seconds\n\t \t Training Loss: 1.2089, Validation Loss: 1.0988\n\t \t Training Acc: 0.6201, Validation Acc: 0.6536\n\t \t Last Char Training Acc: 0.7500, Last Char Validation Acc: 0.6875\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 48500 to checkpoint.pkl\nIteration 48500, time elapsed: 2801.94 seconds\n\t \t Training Loss: 1.1003, Validation Loss: 1.1050\n\t \t Training Acc: 0.6542, Validation Acc: 0.6523\n\t \t Last Char Training Acc: 0.3750, Last Char Validation Acc: 0.7188\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 49000 to checkpoint.pkl\nIteration 49000, time elapsed: 2830.63 seconds\n\t \t Training Loss: 1.1055, Validation Loss: 1.1391\n\t \t Training Acc: 0.6495, Validation Acc: 0.6484\n\t \t Last Char Training Acc: 0.6250, Last Char Validation Acc: 0.6250\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 49500 to checkpoint.pkl\nIteration 49500, time elapsed: 2859.25 seconds\n\t \t Training Loss: 1.1423, Validation Loss: 1.0967\n\t \t Training Acc: 0.6448, Validation Acc: 0.6566\n\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.7812\n--------------------------------------------------\n[save_checkpoint] Saved checkpoint at step 49999 to checkpoint.pkl\nIteration 49999, time elapsed: 2887.84 seconds\n\t \t Training Loss: 1.1577, Validation Loss: 1.1095\n\t \t Training Acc: 0.6390, Validation Acc: 0.6451\n\t \t Last Char Training Acc: 0.7812, Last Char Validation Acc: 0.8438\n--------------------------------------------------\nTraining completed in 2888.00 seconds.\n","output_type":"stream"}],"execution_count":18},{"id":"1f22e0a4","cell_type":"markdown","source":"## Plot the training and validation loss curves\n\nAfter training, we plot the training and validation loss curves to visualize the model's learning progress over time. This plot is saved to the specified output directory, and will be useful to identify if the model converges at first glance. Among all the ablation experiments for that category, we can also compare these curves to see how different configurations affect the learning dynamics.","metadata":{"id":"1f22e0a4"}},{"id":"f4cd9351","cell_type":"code","source":"# Plot training and validation loss curves\nplt.plot(train_step_history, train_loss_history, \"-\",label='Training Loss', color='blue')\nplt.plot(val_step_history, val_loss_history, \"-\", label='Validation Loss', lw = 2, color='red')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend(loc = \"upper right\")\nplt.title(\"Training and Validation Loss over Iterations\")\nplt.legend()\nplt.savefig(\"loss_curve.png\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"f4cd9351","outputId":"2cd068a4-34b3-40f0-de45-d4594dcd51e0","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T08:30:36.008555Z","iopub.execute_input":"2025-11-19T08:30:36.008804Z","iopub.status.idle":"2025-11-19T08:30:36.717184Z","shell.execute_reply.started":"2025-11-19T08:30:36.008787Z","shell.execute_reply":"2025-11-19T08:30:36.716510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB7J0lEQVR4nO3dd1xV9f8H8NcFBESWAwEVxb1XrtAcJYYzsVLza64cqWhZWWalqQ00tUwt04amZa4c/cwR7lTKrThyCw7AHCwH675/f5zugcO97AsH8fV8PM4Dzud8zjmfe+7lnjefdQwiIiAiIiIqJmz0LgARERGRNTG4ISIiomKFwQ0REREVKwxuiIiIqFhhcENERETFCoMbIiIiKlYY3BAREVGxwuCGiIiIihUGN0RERFSsMLihQjV48GD4+vrmad8pU6bAYDBYt0BFzJUrV2AwGLBkyZJCP7fBYMCUKVPU9SVLlsBgMODKlSvZ7uvr64vBgwdbtTz5+awQ5VeHDh3QoUMHvYtBecTghgAoN7acLLt27dK7qI+91157DQaDARcuXMg0z/vvvw+DwYATJ04UYsly78aNG5gyZQqOHTumd1FUpgBz1qxZehel2PD19UX37t3V9fv372PKlCm6f5+cPn0aU6ZMyVEAT48WO70LQEXDsmXLNOtLly5FSEiIWXrdunXzdZ5vv/0WRqMxT/t+8MEHePfdd/N1/uKgf//+mDdvHpYvX47JkydbzPPLL7+gYcOGaNSoUZ7PM2DAALz00ktwcHDI8zGyc+PGDUydOhW+vr5o0qSJZlt+PitUtN2/fx9Tp04FAF1rR06fPo2pU6eiQ4cOZrWEf/zxhz6FIqtgcEMAgJdfflmz/tdffyEkJMQsPaP79+/Dyckpx+cpUaJEnsoHAHZ2drCz40e2VatWqFGjBn755ReLwU1oaCguX76M6dOn5+s8tra2sLW1zdcx8iM/nxUqXCkpKTAajbC3t9e1HPfu3UOpUqWsciy9XwvlD5ulKMc6dOiABg0a4PDhw2jXrh2cnJzw3nvvAQA2bNiAbt26oUKFCnBwcED16tXx0UcfITU1VXOMjP0o0jcBLFq0CNWrV4eDgwNatGiBgwcPava11OfGYDBgzJgxWL9+PRo0aAAHBwfUr18fW7ZsMSv/rl270Lx5czg6OqJ69epYuHBhjvvx/Pnnn+jduzcqV64MBwcH+Pj44I033sCDBw/MXp+zszOuX7+OwMBAODs7w8PDA+PHjze7FjExMRg8eDDc3Nzg7u6OQYMGISYmJtuyAErtzT///IMjR46YbVu+fDkMBgP69euHpKQkTJ48Gc2aNYObmxtKlSqFtm3bYufOndmew1KfGxHBxx9/jEqVKsHJyQlPP/00Tp06ZbbvnTt3MH78eDRs2BDOzs5wdXVFly5dcPz4cTXPrl270KJFCwDAkCFD1KZPU38jS31u7t27h7feegs+Pj5wcHBA7dq1MWvWLIiIJl9uPhd5dfPmTQwdOhSenp5wdHRE48aN8eOPP5rlW7FiBZo1awYXFxe4urqiYcOG+PLLL9XtycnJmDp1KmrWrAlHR0eULVsWTz31FEJCQrItw6VLl9C7d2+UKVMGTk5OePLJJ/H777+r26Ojo2FnZ6fWkqR39uxZGAwGzJ8/X02LiYnBuHHj1Otbo0YNzJgxQ1ODlv5vds6cOerf7OnTp3N03a5cuQIPDw8AwNSpU9X3PX1/r3/++QcvvvgiypQpA0dHRzRv3hy//fab5jimz+fu3bsxevRolC9fHpUqVQIAhIeHY/To0ahduzZKliyJsmXLonfv3prP8pIlS9C7d28AwNNPP23W9G6pz01O3vPcfKdFRUVhyJAhqFSpEhwcHODt7Y2ePXuymcwK+G8w5crt27fRpUsXvPTSS3j55Zfh6ekJQPmicHZ2xptvvglnZ2fs2LEDkydPRlxcHGbOnJntcZcvX474+Hi8+uqrMBgM+Oyzz/D888/j0qVL2f4Hv3fvXqxduxajR4+Gi4sL5s6dixdeeAEREREoW7YsAODo0aPo3LkzvL29MXXqVKSmpmLatGnql2x2Vq9ejfv372PUqFEoW7YsDhw4gHnz5uHatWtYvXq1Jm9qaioCAgLQqlUrzJo1C9u2bcPs2bNRvXp1jBo1CoASJPTs2RN79+7FyJEjUbduXaxbtw6DBg3KUXn69++PqVOnYvny5XjiiSc05161ahXatm2LypUr49atW/juu+/Qr18/DB8+HPHx8fj+++8REBCAAwcOmDUFZWfy5Mn4+OOP0bVrV3Tt2hVHjhzBs88+i6SkJE2+S5cuYf369ejduzeqVq2K6OhoLFy4EO3bt8fp06dRoUIF1K1bF9OmTcPkyZMxYsQItG3bFgDQunVri+cWETz33HPYuXMnhg4diiZNmmDr1q14++23cf36dXzxxRea/Dn5XOTVgwcP0KFDB1y4cAFjxoxB1apVsXr1agwePBgxMTF4/fXXAQAhISHo168fOnbsiBkzZgAAzpw5g3379ql5pkyZguDgYAwbNgwtW7ZEXFwcDh06hCNHjqBTp06ZliE6OhqtW7fG/fv38dprr6Fs2bL48ccf8dxzz2HNmjXo1asXPD090b59e6xatQoffvihZv+VK1fC1tZWvcHfv38f7du3x/Xr1/Hqq6+icuXK2L9/PyZOnIjIyEjMmTNHs//ixYvx8OFDjBgxAg4ODihTpkyOrp2HhwcWLFiAUaNGoVevXnj++ecBQG1CPXXqFNq0aYOKFSvi3XffRalSpbBq1SoEBgbi119/Ra9evTTHGz16NDw8PDB58mTcu3cPAHDw4EHs378fL730EipVqoQrV65gwYIF6NChA06fPg0nJye0a9cOr732GubOnYv33ntPbXLPrOk9p++5SU6+01544QWcOnUKY8eOha+vL27evImQkBBERESwM31+CZEFQUFBkvHj0b59ewEg33zzjVn++/fvm6W9+uqr4uTkJA8fPlTTBg0aJFWqVFHXL1++LACkbNmycufOHTV9w4YNAkD+7//+T0378MMPzcoEQOzt7eXChQtq2vHjxwWAzJs3T03r0aOHODk5yfXr19W08+fPi52dndkxLbH0+oKDg8VgMEh4eLjm9QGQadOmafI2bdpUmjVrpq6vX79eAMhnn32mpqWkpEjbtm0FgCxevDjbMrVo0UIqVaokqampatqWLVsEgCxcuFA9ZmJioma/u3fviqenp7zyyiuadADy4YcfquuLFy8WAHL58mUREbl586bY29tLt27dxGg0qvnee+89ASCDBg1S0x4+fKgpl4jyXjs4OGiuzcGDBzN9vRk/K6Zr9vHHH2vyvfjii2IwGDSfgZx+LiwxfSZnzpyZaZ45c+YIAPnpp5/UtKSkJPHz8xNnZ2eJi4sTEZHXX39dXF1dJSUlJdNjNW7cWLp165ZlmSwZN26cAJA///xTTYuPj5eqVauKr6+vev0XLlwoACQsLEyzf7169eSZZ55R1z/66CMpVaqUnDt3TpPv3XffFVtbW4mIiBCRtOvj6uoqN2/ezFFZq1SponmN//77r9nnzaRjx47SsGFDzfeG0WiU1q1bS82aNdU00+fzqaeeMru+lv5eQ0NDBYAsXbpUTVu9erUAkJ07d5rlb9++vbRv315dz+l7ntPvtLt372b7OaO8Y7MU5YqDgwOGDBlill6yZEn19/j4eNy6dQtt27bF/fv38c8//2R73L59+6J06dLquum/+EuXLmW7r7+/P6pXr66uN2rUCK6uruq+qamp2LZtGwIDA1GhQgU1X40aNdClS5dsjw9oX9+9e/dw69YttG7dGiKCo0ePmuUfOXKkZr1t27aa17Jp0ybY2dmpNTmA0sdl7NixOSoPoPSTunbtGvbs2aOmLV++HPb29up/47a2tmrfAaPRiDt37iAlJQXNmze32KSVlW3btiEpKQljx47VNOWNGzfOLK+DgwNsbJSvl9TUVNy+fRvOzs6oXbt2rs9rsmnTJtja2uK1117TpL/11lsQEWzevFmTnt3nIj82bdoELy8v9OvXT00rUaIEXnvtNSQkJGD37t0AAHd3d9y7dy/LJiZ3d3ecOnUK58+fz3UZWrZsiaeeekpNc3Z2xogRI3DlyhW1mej555+HnZ0dVq5cqeY7efIkTp8+jb59+6ppq1evRtu2bVG6dGncunVLXfz9/ZGamqr5nAFKrUNOaz5z6s6dO9ixYwf69Omjfo/cunULt2/fRkBAAM6fP4/r169r9hk+fLhZ37D0f6/Jycm4ffs2atSoAXd393x9/nLynptk951WsmRJ2NvbY9euXbh7926eykSZY3BDuVKxYkWLHe1OnTqFXr16wc3NDa6urvDw8FA7I8fGxmZ73MqVK2vWTV8KOfmjz7ivaX/Tvjdv3sSDBw9Qo0YNs3yW0iyJiIjA4MGDUaZMGbUfTfv27QGYvz5HR0ezL/305QGUPgHe3t5wdnbW5Ktdu3aOygMAL730EmxtbbF8+XIAwMOHD7Fu3Tp06dJF86X6448/olGjRmp/Dg8PD/z+++85el/SCw8PBwDUrFlTk+7h4aE5H6AEUl988QVq1qwJBwcHlCtXDh4eHjhx4kSuz5v+/BUqVICLi4sm3dSMYCqfSXafi/wIDw9HzZo11QAus7KMHj0atWrVQpcuXVCpUiW88sorZv1+pk2bhpiYGNSqVQsNGzbE22+/naMh/OHh4RY/LxnLUK5cOXTs2BGrVq1S86xcuRJ2dnZqkxAAnD9/Hlu2bIGHh4dm8ff3B6D8HaVXtWrVbMuYWxcuXICIYNKkSWblMDWr5aQcDx48wOTJk9W+Q6bPX0xMTL4+fzl5z02y+05zcHDAjBkzsHnzZnh6eqJdu3b47LPPEBUVlafykRb73FCupP+PyCQmJgbt27eHq6srpk2bhurVq8PR0RFHjhzBhAkTcjScN7NROZKho6i1982J1NRUdOrUCXfu3MGECRNQp04dlCpVCtevX8fgwYPNXl9hjTAqX748OnXqhF9//RVfffUV/u///g/x8fHo37+/muenn37C4MGDERgYiLfffhvly5eHra0tgoODcfHixQIr26effopJkybhlVdewUcffYQyZcrAxsYG48aNK7Th3QX9uciJ8uXL49ixY9i6dSs2b96MzZs3Y/HixRg4cKDaEbVdu3a4ePEiNmzYgD/++APfffcdvvjiC3zzzTcYNmyYVcrx0ksvYciQITh27BiaNGmCVatWoWPHjihXrpyax2g0olOnTnjnnXcsHqNWrVqadUvfBfll+myMHz8eAQEBFvNk/IfEUjnGjh2LxYsXY9y4cfDz84ObmxsMBgNeeumlIvX5GzduHHr06IH169dj69atmDRpEoKDg7Fjxw40bdq0UMpZXDG4oXzbtWsXbt++jbVr16Jdu3Zq+uXLl3UsVZry5cvD0dHR4qR3WU2EZxIWFoZz587hxx9/xMCBA9X0nIxmyUyVKlWwfft2JCQkaGpvzp49m6vj9O/fH1u2bMHmzZuxfPlyuLq6okePHur2NWvWoFq1ali7dq2mKSlj59KclhlQ/sOvVq2amv7vv/+a1YasWbMGTz/9NL7//ntNekxMjOaGmpsZp6tUqYJt27YhPj5eU3tjavY0la8wVKlSBSdOnIDRaNT8J2+pLPb29ujRowd69OgBo9GI0aNHY+HChZg0aZJ6oy5TpgyGDBmCIUOGICEhAe3atcOUKVOyDG6qVKli8fNiqQyBgYF49dVX1aapc+fOYeLEiZr9qlevjoSEBLWmpiBl9r6bPlclSpTIVznWrFmDQYMGYfbs2Wraw4cPzUYj5vbzl9P3PDeqV6+Ot956C2+99RbOnz+PJk2aYPbs2fjpp5/ydDxSsFmK8s30H0r6/0iSkpLw9ddf61UkDVtbW/j7+2P9+vW4ceOGmn7hwgWzfhqZ7Q9oX5+IaIbz5lbXrl2RkpKCBQsWqGmpqamYN29ero4TGBgIJycnfP3119i8eTOef/55ODo6Zln2v//+G6Ghobkus7+/P0qUKIF58+ZpjpdxFI3pvBlrSFavXm3WX8I0J0lOhsB37doVqampmqHLAPDFF1/AYDDkuP+UNXTt2hVRUVGafiwpKSmYN28enJ2d1SbL27dva/azsbFRRwUlJiZazOPs7IwaNWqo27Mqw4EDBzTv5b1797Bo0SL4+vqiXr16arq7uzsCAgKwatUqrFixAvb29ggMDNQcr0+fPggNDcXWrVvNzhUTE4OUlJQsy5MbprmxMr7v5cuXR4cOHbBw4UJERkaa7ffvv//m6PiWPn/z5s0zm44ht5+/nLznOXX//n08fPhQk1a9enW4uLhk+95T9lhzQ/nWunVrlC5dGoMGDVIfDbBs2bJCrf7PzpQpU/DHH3+gTZs2GDVqlHqTbNCgQbZT/9epUwfVq1fH+PHjcf36dbi6uuLXX3/NV9+NHj16oE2bNnj33Xdx5coV1KtXD2vXrs11fwBnZ2cEBgaq/W7SN0kBQPfu3bF27Vr06tUL3bp1w+XLl/HNN9+gXr16SEhIyNW5TPP1BAcHo3v37ujatSuOHj2KzZs3a2pjTOedNm0ahgwZgtatWyMsLAw///yzpsYHUL7M3d3d8c0338DFxQWlSpVCq1atLPaj6NGjB55++mm8//77uHLlCho3bow//vgDGzZswLhx4zSdh61h+/btZjcfQAkoR4wYgYULF2Lw4ME4fPgwfH19sWbNGuzbtw9z5sxRa5aGDRuGO3fu4JlnnkGlSpUQHh6OefPmoUmTJmpfjXr16qFDhw5o1qwZypQpg0OHDmHNmjUYM2ZMluV799138csvv6BLly547bXXUKZMGfz444+4fPkyfv31V7O+IX379sXLL7+Mr7/+GgEBAXB3d9dsf/vtt/Hbb7+he/fuGDx4MJo1a4Z79+4hLCwMa9aswZUrV8ze57wqWbIk6tWrh5UrV6JWrVooU6YMGjRogAYNGuCrr77CU089hYYNG2L48OGoVq0aoqOjERoaimvXrmnmSspM9+7dsWzZMri5uaFevXoIDQ3Ftm3bzKYAaNKkCWxtbTFjxgzExsbCwcEBzzzzDMqXL292zJy+5zl17tw5dOzYEX369EG9evVgZ2eHdevWITo6Gi+99FKujkUW6DBCix4BmQ0Fr1+/vsX8+/btkyeffFJKliwpFSpUkHfeeUe2bt1qNswys6HgloZDIsNQ0cyGggcFBZntW6VKFc3QZBGR7du3S9OmTcXe3l6qV68u3333nbz11lvi6OiYyVVIc/r0afH39xdnZ2cpV66cDB8+XB1anH4Y86BBg6RUqVJm+1sq++3bt2XAgAHi6uoqbm5uMmDAADl69GiOh4Kb/P777wJAvL29zYZfG41G+fTTT6VKlSri4OAgTZs2lY0bN5q9DyLZDwUXEUlNTZWpU6eKt7e3lCxZUjp06CAnT540u94PHz6Ut956S83Xpk0bCQ0NNRteK6IMka1Xr546LN/02i2VMT4+Xt544w2pUKGClChRQmrWrCkzZ87UDE03vZacfi4yMn0mM1uWLVsmIiLR0dEyZMgQKVeunNjb20vDhg3N3rc1a9bIs88+K+XLlxd7e3upXLmyvPrqqxIZGanm+fjjj6Vly5bi7u4uJUuWlDp16sgnn3wiSUlJWZZTROTixYvy4osviru7uzg6OkrLli1l48aNFvPGxcVJyZIlzYYzpxcfHy8TJ06UGjVqiL29vZQrV05at24ts2bNUsuTk6HyGWUcCi4isn//fmnWrJnY29ubffYuXrwoAwcOFC8vLylRooRUrFhRunfvLmvWrFHzmD6fBw8eNDvf3bt31ffG2dlZAgIC5J9//rH4/n/77bdSrVo1sbW11XxfWfqs5uQ9z+l32q1btyQoKEjq1KkjpUqVEjc3N2nVqpWsWrUq64tJOWIQKUL/XhMVssDAwDwNwyUioqKLfW7osZHxUQnnz5/Hpk2bdH1wHxERWR9rbuix4e3tjcGDB6NatWoIDw/HggULkJiYiKNHj5rN3UJERI8udiimx0bnzp3xyy+/ICoqCg4ODvDz88Onn37KwIaIqJhhzQ0REREVK+xzQ0RERMUKgxsiIiIqVh67PjdGoxE3btyAi4tLrqbeJiIiIv2ICOLj41GhQgWzSSozeuyCmxs3bsDHx0fvYhAREVEeXL16FZUqVcoyz2MX3JimyL569SpcXV11Lg0RERHlRFxcHHx8fHL0qIvHLrgxNUW5uroyuCEiInrE5KRLCTsUExERUbHC4IaIiIiKFQY3REREVKw8dn1uiIgo/1JTU5GcnKx3MaiYsbe3z3aYd04wuCEiohwTEURFRSEmJkbvolAxZGNjg6pVq8Le3j5fx2FwQ0REOWYKbMqXLw8nJydOhkpWY5pkNzIyEpUrV87XZ4vBDRER5Uhqaqoa2JQtW1bv4lAx5OHhgRs3biAlJQUlSpTI83HYoZiIiHLE1MfGyclJ55JQcWVqjkpNTc3XcRjcEBFRrrApigqKtT5bDG6IiIioWGFwQ0RElEu+vr6YM2dOjvPv2rULBoOBo8wKCYMbIiIqtgwGQ5bLlClT8nTcgwcPYsSIETnO37p1a0RGRsLNzS1P58spBlEKjpaylm3bgCNHgORkYOhQwMtL7xIRET32IiMj1d9XrlyJyZMn4+zZs2qas7Oz+ruIIDU1FXZ22d8aPTw8clUOe3t7ePG+UGhYc2Mt69cDEyYAH3wAXLumd2mIiAiAl5eXuri5ucFgMKjr//zzD1xcXLB582Y0a9YMDg4O2Lt3Ly5evIiePXvC09MTzs7OaNGiBbZt26Y5bsZmKYPBgO+++w69evWCk5MTatasid9++03dnrFGZcmSJXB3d8fWrVtRt25dODs7o3PnzppgLCUlBa+99hrc3d1RtmxZTJgwAYMGDUJgYGCer8fdu3cxcOBAlC5dGk5OTujSpQvOnz+vbg8PD0ePHj1QunRplCpVCvXr18emTZvUffv37w8PDw+ULFkSNWvWxOLFi/NcloLE4MZa0o/HT0nRrxxERIVEBLh3T59FxHqv491338X06dNx5swZNGrUCAkJCejatSu2b9+Oo0ePonPnzujRowciIiKyPM7UqVPRp08fnDhxAl27dkX//v1x586dTPPfv38fs2bNwrJly7Bnzx5ERERg/Pjx6vYZM2bg559/xuLFi7Fv3z7ExcVh/fr1+XqtgwcPxqFDh/Dbb78hNDQUIoKuXbuqw/yDgoKQmJiIPXv2ICwsDDNmzFBrtyZNmoTTp09j8+bNOHPmDBYsWIBy5crlqzwFhc1S1pK+GpPBDRE9Bu7fB9K16hSqhASgVCnrHGvatGno1KmTul6mTBk0btxYXf/oo4+wbt06/PbbbxgzZkymxxk8eDD69esHAPj0008xd+5cHDhwAJ07d7aYPzk5Gd988w2qV68OABgzZgymTZumbp83bx4mTpyIXr16AQDmz5+v1qLkxfnz5/Hbb79h3759aN26NQDg559/ho+PD9avX4/evXsjIiICL7zwAho2bAgAqFatmrp/REQEmjZtiubNmwNQaq+KqiJTczN9+nQYDAaMGzcu0zxLliwx6wzm6OhYeIXMSvrghg+TIyJ6ZJhu1iYJCQkYP3486tatC3d3dzg7O+PMmTPZ1tw0atRI/b1UqVJwdXXFzZs3M83v5OSkBjYA4O3treaPjY1FdHQ0WrZsqW63tbVFs2bNcvXa0jtz5gzs7OzQqlUrNa1s2bKoXbs2zpw5AwB47bXX8PHHH6NNmzb48MMPceLECTXvqFGjsGLFCjRp0gTvvPMO9u/fn+eyFLQiUXNz8OBBLFy4UPPByIyrq6umM1iRmUyKzVJE9JhxclJqUPQ6t7WUylAFNH78eISEhGDWrFmoUaMGSpYsiRdffBFJSUlZHifj4wIMBgOMRmOu8os129vyYNiwYQgICMDvv/+OP/74A8HBwZg9ezbGjh2LLl26IDw8HJs2bUJISAg6duyIoKAgzJo1S9cyW6J7zU1CQgL69++Pb7/9FqVLl842f/rOYF5eXvD09CyEUuYAm6WI6DFjMChNQ3osBfl/7b59+zB48GD06tULDRs2hJeXF65cuVJwJ7TAzc0Nnp6eOHjwoJqWmpqKI0eO5PmYdevWRUpKCv7++2817fbt2zh79izq1aunpvn4+GDkyJFYu3Yt3nrrLXz77bfqNg8PDwwaNAg//fQT5syZg0WLFuW5PAVJ95qboKAgdOvWDf7+/vj444+zzZ+QkIAqVarAaDTiiSeewKeffor69etnmj8xMRGJiYnqelxcnFXKbYbNUkRExULNmjWxdu1a9OjRAwaDAZMmTcqyBqagjB07FsHBwahRowbq1KmDefPm4e7duzlqsQgLC4OLi4u6bjAY0LhxY/Ts2RPDhw/HwoUL4eLignfffRcVK1ZEz549AQDjxo1Dly5dUKtWLdy9exc7d+5E3bp1AQCTJ09Gs2bNUL9+fSQmJmLjxo3qtqJG1+BmxYoVOHLkiCYyzUrt2rXxww8/oFGjRoiNjcWsWbPQunVrnDp1CpUqVbK4T3BwMKZOnWrNYlvGmhsiomLh888/xyuvvILWrVujXLlymDBhQsH9Y5yFCRMmICoqCgMHDoStrS1GjBiBgIAA2NraZrtvu3btNOu2trZISUnB4sWL8frrr6N79+5ISkpCu3btsGnTJrWJLDU1FUFBQbh27RpcXV3RuXNnfPHFFwCUuXomTpyIK1euoGTJkmjbti1WrFhh/RduBQbRqYHv6tWraN68OUJCQtS+Nh06dECTJk1yPKV1cnIy6tati379+uGjjz6ymMdSzY2Pjw9iY2Ph6uqa79eh+uIL4M03ld9XrgT69LHesYmIioCHDx/i8uXLqFq1atEZzPEYMRqNqFu3Lvr06ZPpPe9Rl9VnLC4uDm5ubjm6f+tWc3P48GHcvHkTTzzxhJqWmpqKPXv2YP78+UhMTMw2Oi1RogSaNm2KCxcuZJrHwcEBDg4OVit3Zu4m2EHtMcSaGyIiyqfw8HD88ccfaN++PRITEzF//nxcvnwZ//vf//QuWpGnW3DTsWNHhIWFadKGDBmCOnXqYMKECTmqdktNTUVYWBi6du1aUMXMsWNhdnjatMI+N0RElE82NjZYsmQJxo8fDxFBgwYNsG3btiLbz6Uo0S24cXFxQYMGDTRppUqVQtmyZdX0gQMHomLFiggODgagTLT05JNPokaNGoiJicHMmTMRHh6OYcOGFXr5MzLacig4ERFZj4+PD/bt26d3MR5Juo+WykpERARsbNJGq9+9exfDhw9HVFQUSpcujWbNmmH//v2aIWx6MdqyQzEREVFRUKSCm127dmW5/sUXX6i9tosaseFQcCIioqJA90n8igujDWtuiIiIigIGN1bCPjdERERFA4MbK2HNDRERUdHA4MZKxJZ9boiIiIoCBjdWkmrDZikiouKqQ4cOGDdunLru6+ub7Wz6BoMB69evz/e5rXWcxwmDGysRDgUnIipyevTogc6dO1vc9ueff8JgMODEiRO5Pu7BgwcxYsSI/BZPY8qUKWjSpIlZemRkJLp06WLVc2W0ZMkSuLu7F+g5ChODGythsxQRUdEzdOhQhISE4Nq1a2bbFi9ejObNm6vPN8wNDw8PODk5WaOI2fLy8iqUxwgVJwxurIQdiomIip7u3bvDw8MDS5Ys0aQnJCRg9erVGDp0KG7fvo1+/fqhYsWKcHJyQsOGDfHLL79kedyMzVLnz59Hu3bt4OjoiHr16iEkJMRsnwkTJqBWrVpwcnJCtWrVMGnSJCT/98/wkiVLMHXqVBw/fhwGgwEGg0Etc8ZmqbCwMDzzzDMoWbIkypYtixEjRiAhIUHdPnjwYAQGBmLWrFnw9vZG2bJlERQUpJ4rLyIiItCzZ084OzvD1dUVffr0QXR0tLr9+PHjePrpp+Hi4gJXV1c0a9YMhw4dAqA8I6tHjx4oXbo0SpUqhfr162PTpk15LktOFKlJ/B5lHApORFT02NnZYeDAgViyZAnef/99GAwGAMDq1auRmpqKfv36ISEhAc2aNcOECRPg6uqK33//HQMGDED16tXRsmXLbM9hNBrx/PPPw9PTE3///TdiY2M1/XNMXFxcsGTJElSoUAFhYWEYPnw4XFxc8M4776Bv3744efIktmzZgm3btgEA3NzczI5x7949BAQEwM/PDwcPHsTNmzcxbNgwjBkzRhPA7dy5E97e3ti5cycuXLiAvn37okmTJhg+fHiur6HRaFQDm927dyMlJQVBQUHo27evOtlu//790bRpUyxYsAC2trY4duwYSpRQ7otBQUFISkrCnj17UKpUKZw+fRrOzs65LkduMLixEtbcENFjqXlzICqq8M/r5QX8VzOQnVdeeQUzZ87E7t270aFDBwBKk9QLL7wANzc3uLm5Yfz48Wr+sWPHYuvWrVi1alWOgptt27bhn3/+wdatW1GhQgUAwKeffmrWT+aDDz5Qf/f19cX48eOxYsUKvPPOOyhZsiScnZ1hZ2cHLy+vTM+1fPlyPHz4EEuXLkWpUqUAAPPnz0ePHj0wY8YMeHp6AgBKly6N+fPnw9bWFnXq1EG3bt2wffv2PAU327dvR1hYGC5fvgwfHx8AwNKlS1G/fn0cPHgQLVq0QEREBN5++23UqVMHAFCzZk11/4iICLzwwgto2LAhAKBatWq5LkNuMbixEva5IaLHUlQUcP263qXIUp06ddC6dWv88MMP6NChAy5cuIA///wT06ZNAwCkpqbi008/xapVq3D9+nUkJSUhMTExx31qzpw5Ax8fHzWwAQA/Pz+zfCtXrsTcuXNx8eJFJCQkICUlBa6urrl6LWfOnEHjxo3VwAYA2rRpA6PRiLNnz6rBTf369WFra6vm8fb2RlhYWK7Olf6cPj4+amADAPXq1YO7uzvOnDmDFi1a4M0338SwYcOwbNky+Pv7o3fv3qhevToA4LXXXsOoUaPwxx9/wN/fHy+88EKe+jnlBvvcWAmHghPRY8nLC6hYsfCXLGo3LBk6dCh+/fVXxMfHY/HixahevTrat28PAJg5cya+/PJLTJgwATt37sSxY8cQEBCApKQkq12m0NBQ9O/fH127dsXGjRtx9OhRvP/++1Y9R3qmJiETg8EAo9FYIOcClJFep06dQrdu3bBjxw7Uq1cP69atAwAMGzYMly5dwoABAxAWFobmzZtj3rx5BVYWgDU3VsOh4ET0WMph05De+vTpg9dffx3Lly/H0qVLMWrUKLX/zb59+9CzZ0+8/PLLAJQ+JufOnUO9evVydOy6devi6tWriIyMhLe3NwDgr7/+0uTZv38/qlSpgvfff19NCw8P1+Sxt7dHampqtudasmQJ7t27p9be7Nu3DzY2Nqhdu3aOyptbptd39epVtfbm9OnTiImJ0VyjWrVqoVatWnjjjTfQr18/LF68GL169QIA+Pj4YOTIkRg5ciQmTpyIb7/9FmPHji2Q8gKsubEaI58KTkRUZDk7O6Nv376YOHEiIiMjMXjwYHVbzZo1ERISgv379+PMmTN49dVXNSOBsuPv749atWph0KBBOH78OP78809NEGM6R0REBFasWIGLFy9i7ty5as2Gia+vLy5fvoxjx47h1q1bSExMNDtX//794ejoiEGDBuHkyZPYuXMnxo4diwEDBqhNUnmVmpqKY8eOaZYzZ87A398fDRs2RP/+/XHkyBEcOHAAAwcORPv27dG8eXM8ePAAY8aMwa5duxAeHo59+/bh4MGDqFu3LgBg3Lhx2Lp1Ky5fvowjR45g586d6raCwuDGSlhzQ0RUtA0dOhR3795FQECApn/MBx98gCeeeAIBAQHo0KEDvLy8EBgYmOPj2tjYYN26dXjw4AFatmyJYcOG4ZNPPtHkee655/DGG29gzJgxaNKkCfbv349JkyZp8rzwwgvo3Lkznn76aXh4eFgcju7k5IStW7fizp07aNGiBV588UV07NgR8+fPz93FsCAhIQFNmzbVLD169IDBYMCGDRtQunRptGvXDv7+/qhWrRpWrlwJALC1tcXt27cxcOBA1KpVC3369EGXLl0wdepUAErQFBQUhLp166Jz586oVasWvv7663yXNysGEZECPUMRExcXBzc3N8TGxua6I1dWfgiOxivv/dcG3LMnwKmyiaiYefjwIS5fvoyqVavC0dFR7+JQMZTVZyw392/W3FgJh4ITEREVDQxurIRDwYmIiIoGBjdWwhmKiYiIigYGN1bCZikiIqKigcGNlXAoOBE9Lh6zcShUiKz12WJwYy02NjBCmRCKNTdEVByZZr29f/++ziWh4so0Y3P6R0fkBWcotqJklIADkhjcEFGxZGtrC3d3d9y8eROAMueKaZZfovwyGo34999/4eTkBDu7/IUnDG6sxGAAUmDH4IaIijXTE6tNAQ6RNdnY2KBy5cr5DpoZ3FhRiulyss8NERVTBoMB3t7eKF++PJL5XUdWZm9vDxub/PeYYXBjRcn4bzg4a26IqJiztbXNd78IooLCDsVWpNbcMLghIiLSDYMbK2KzFBERkf4Y3FiJCGtuiIiIigIGN1bEPjdERET6Y3BjRay5ISIi0h+DGytinxsiIiL9MbixEoOBzVJERERFAYMbK2KzFBERkf4Y3FiRGtwYjcpCREREhY7BjRWlpJ/wmbU3REREuigywc306dNhMBgwbty4LPOtXr0aderUgaOjIxo2bIhNmzYVTgFzQO1zAzC4ISIi0kmRCG4OHjyIhQsXolGjRlnm279/P/r164ehQ4fi6NGjCAwMRGBgIE6ePFlIJc0aa26IiIj0p3twk5CQgP79++Pbb79F6dKls8z75ZdfonPnznj77bdRt25dfPTRR3jiiScwf/78Qipt1jTBDYeDExER6UL34CYoKAjdunWDv79/tnlDQ0PN8gUEBCA0NLSgipcrbJYiIiLSn132WQrOihUrcOTIERw8eDBH+aOiouDp6alJ8/T0RFRUVKb7JCYmIjExUV2Pi4vLW2FzgM1SRERE+tOt5ubq1at4/fXX8fPPP8PR0bHAzhMcHAw3Nzd18fHxKZDzGAxsliIiIioKdAtuDh8+jJs3b+KJJ56AnZ0d7OzssHv3bsydOxd2dnZITU0128fLywvR0dGatOjoaHh5eWV6nokTJyI2NlZdrl69avXXYsKaGyIiIv3p1izVsWNHhIWFadKGDBmCOnXqYMKECbC1tTXbx8/PD9u3b9cMFw8JCYGfn1+m53FwcICDg4PVyp0V9rkhIiLSn27BjYuLCxo0aKBJK1WqFMqWLaumDxw4EBUrVkRwcDAA4PXXX0f79u0xe/ZsdOvWDStWrMChQ4ewaNGiQi+/Jay5ISIi0p/uo6WyEhERgcjISHW9devWWL58ORYtWoTGjRtjzZo1WL9+vVmQpBf2uSEiItKfrqOlMtq1a1eW6wDQu3dv9O7du3AKlEtsliIiItJfka65edSwWYqIiEh/DG6siM1SRERE+mNwY0VsliIiItIfgxsrMZvEj8ENERGRLhjcWBGDGyIiIv0xuLEi9rkhIiLSH4MbK2KfGyIiIv0xuLEiNksRERHpj8GNFbFZioiISH8MbqyIzVJERET6Y3BjRWyWIiIi0h+DGyticENERKQ/BjdWYjaJH/vcEBER6YLBjZU4OLDPDRERUVHA4MZKmjZlsxQREVFRwODGitgsRUREpD8GN1bEZikiIiL9MbixIjZLERER6Y/BjRUxuCEiItIfgxsrYp8bIiIi/TG4sSL2uSEiItIfgxsrMZvEj8ENERGRLhjcWBGbpYiIiPTH4MaK2CxFRESkPwY3VsRmKSIiIv0xuLEiBjdERET6Y3BjRexzQ0REpD8GN1bEPjdERET6Y3BjRWyWIiIi0h+DGytisxQREZH+GNxYicHAZikiIqKigMGNFbFZioiISH8MbqyIwQ0REZH+GNxYEfvcEBER6Y/BjVUZkAJb5VfW3BAREemCwY2VqbU3DG6IiIh0weDGylIN/wU3bJYiIiLSha7BzYIFC9CoUSO4urrC1dUVfn5+2Lx5c6b5lyxZAoPBoFkcHR0LscTZU4eDs+aGiIhIF3bZZyk4lSpVwvTp01GzZk2ICH788Uf07NkTR48eRf369S3u4+rqirNnz6rrBoOhsIqbI2yWIiIi0peuwU2PHj0065988gkWLFiAv/76K9PgxmAwwMvLqzCKlyumGCvVYAcIGNwQERHppMj0uUlNTcWKFStw7949+Pn5ZZovISEBVapUgY+PD3r27IlTp05ledzExETExcVploKk1tywzw0REZEudA9uwsLC4OzsDAcHB4wcORLr1q1DvXr1LOatXbs2fvjhB2zYsAE//fQTjEYjWrdujWvXrmV6/ODgYLi5uamLj49PQb0UAEAK+9wQERHpyiAiomcBkpKSEBERgdjYWKxZswbfffcddu/enWmAk15ycjLq1q2Lfv364aOPPrKYJzExEYmJiep6XFwcfHx8EBsbC1dXV6u9jpMngYYNgQu2tVA99TxQpgxw+7bVjk9ERPQ4i4uLg5ubW47u37r2uQEAe3t71KhRAwDQrFkzHDx4EF9++SUWLlyY7b4lSpRA06ZNceHChUzzODg4wMHBwWrlzQ6bpYiIiPSle7NURkajUVPTkpXU1FSEhYXB29u7gEuVc2yWIiIi0peuNTcTJ05Ely5dULlyZcTHx2P58uXYtWsXtm7dCgAYOHAgKlasiODgYADAtGnT8OSTT6JGjRqIiYnBzJkzER4ejmHDhun5MjRSDBwKTkREpCddg5ubN29i4MCBiIyMhJubGxo1aoStW7eiU6dOAICIiAjY2KRVLt29exfDhw9HVFQUSpcujWbNmmH//v056p9TWDjPDRERkb5071Bc2HLTISk3TB2K/7Zrg5Yp+5XElBTA1tZq5yAiInpc5eb+XeT63DyqTJP4JRtKpCWy9oaIiKjQMbixstT0LX0MboiIiAodgxsrS0kf3HA4OBERUaFjcGNlbJYiIiLSF4MbK2OzFBERkb4Y3FhZCoMbIiIiXTG4sTL2uSEiItIXgxsrY58bIiIifTG4sTL2uSEiItIXgxsrMU3ix2YpIiIifTG4sbIUNksRERHpisGNlXG0FBERkb4Y3FgZgxsiIiJ9MbixshQD+9wQERHpicGNlaWAfW6IiIj0xODGytgsRUREpC8GN1bGoeBERET6YnBjZZyhmIiISF8MbqzENIkfZygmIiLSF4MbK2OfGyIiIn0xuLEyDgUnIiLSF4MbK+NQcCIiIn0xuLEyNksRERHpi8GNlbFZioiISF8MbqwsWdgsRUREpCcGN1amqblhcENERFToGNxYGee5ISIi0heDGysxTeKXnH60FPvcEBERFToGN1bG0VJERET6YnBjZansc0NERKQrBjdWxqeCExER6YvBjZUlc4ZiIiIiXTG4sTI2SxEREemLwY2VsUMxERGRvhjcWFmScCg4ERGRnhjcWBmbpYiIiPSla3CzYMECNGrUCK6urnB1dYWfnx82b96c5T6rV69GnTp14OjoiIYNG2LTpk2FVNqsmSbxY7MUERGRvnQNbipVqoTp06fj8OHDOHToEJ555hn07NkTp06dsph///796NevH4YOHYqjR48iMDAQgYGBOHnyZCGXPHMcCk5ERKQvg4iI3oVIr0yZMpg5cyaGDh1qtq1v3764d+8eNm7cqKY9+eSTaNKkCb755pscHT8uLg5ubm6IjY2Fq6ur1cp99ixQpw7QwO0qwmIrK4m9ewOrVlntHERERI+r3Ny/i0yfm9TUVKxYsQL37t2Dn5+fxTyhoaHw9/fXpAUEBCA0NDTT4yYmJiIuLk6zFCQ2SxEREelL9+AmLCwMzs7OcHBwwMiRI7Fu3TrUq1fPYt6oqCh4enpq0jw9PREVFZXp8YODg+Hm5qYuPj4+Vi1/RgxuiIiI9KV7cFO7dm0cO3YMf//9N0aNGoVBgwbh9OnTVjv+xIkTERsbqy5Xr1612rEt4VPBiYiI9GWXfZaCZW9vjxo1agAAmjVrhoMHD+LLL7/EwoULzfJ6eXkhOjpakxYdHQ0vL69Mj+/g4AAHBwfrFjoLHApORESkL91rbjIyGo1ITEy0uM3Pzw/bt2/XpIWEhGTaR0cPbJYiIiLSl641NxMnTkSXLl1QuXJlxMfHY/ny5di1axe2bt0KABg4cCAqVqyI4OBgAMDrr7+O9u3bY/bs2ejWrRtWrFiBQ4cOYdGiRXq+DI1k4VBwIiIiPeka3Ny8eRMDBw5EZGQk3Nzc0KhRI2zduhWdOnUCAERERMDGJq1yqXXr1li+fDk++OADvPfee6hZsybWr1+PBg0a6PUSVKZJ/FJhm5bImhsiIqJCV+TmuSloBTXPzblzQO3agLs7cDehhBLYNGsGHDpktXMQERE9rh7JeW6KFbv/KsRYc0NERFTo8hTcXL16FdeuXVPXDxw4gHHjxhWpvi+FzVT/FRMDoMR/w8HZ54aIiKjQ5Sm4+d///oedO3cCUCbW69SpEw4cOID3338f06ZNs2oBHxWHD6dbYc0NERGRbvIU3Jw8eRItW7YEAKxatQoNGjTA/v378fPPP2PJkiXWLN8jIzY23QqDGyIiIt3kKbhJTk5WJ8bbtm0bnnvuOQBAnTp1EBkZab3SPUJMo6UApAU3bJYiIiIqdHkKburXr49vvvkGf/75J0JCQtC5c2cAwI0bN1C2bFmrFvCRZOpzw5obIiKiQpen4GbGjBlYuHAhOnTogH79+qFx48YAgN9++01trnrcaJ7nyWYpIiIi3eRpEr8OHTrg1q1biIuLQ+nSpdX0ESNGwMnJyWqFe5RoYjoGN0RERLrJU83NgwcPkJiYqAY24eHhmDNnDs6ePYvy5ctbtYCPClOfG1tbcCg4ERGRjvIU3PTs2RNLly4FAMTExKBVq1aYPXs2AgMDsWDBAqsW8FFhekqECFhzQ0REpKM8BTdHjhxB27ZtAQBr1qyBp6cnwsPDsXTpUsydO9eqBXxUmGpujEYwuCEiItJRnoKb+/fvw8XFBQDwxx9/4Pnnn4eNjQ2efPJJhIeHW7WAjwqLQ8FTUtKmLiYiIqJCkafgpkaNGli/fj2uXr2KrVu34tlnnwWgPOXbmg+jfJSke3g5xNTnBgBSUwu/MERERI+xPAU3kydPxvjx4+Hr64uWLVvCz88PgFKL07RpU6sW8FGhqbmxTTcIjU1TREREhSpPQ8FffPFFPPXUU4iMjFTnuAGAjh07olevXlYr3KMkfXAjdnZQVxncEBERFao8BTcA4OXlBS8vL/Xp4JUqVXpsJ/ADtM1SSN8sxeHgREREhSpPzVJGoxHTpk2Dm5sbqlSpgipVqsDd3R0fffQRjEajtcv4SNDU3LBZioiISDd5qrl5//338f3332P69Olo06YNAGDv3r2YMmUKHj58iE8++cSqhXwUsM8NERFR0ZCn4ObHH3/Ed999pz4NHAAaNWqEihUrYvTo0Y9lcKMZLWWX7rKyWYqIiKhQ5alZ6s6dO6hTp45Zep06dXDnzp18F+pRpJ3nJl2fG9bcEBERFao8BTeNGzfG/PnzzdLnz5+PRo0a5btQjyL2uSEiIioa8tQs9dlnn6Fbt27Ytm2bOsdNaGgorl69ik2bNlm1gI+KTJulGNwQEREVqjzV3LRv3x7nzp1Dr169EBMTg5iYGDz//PM4deoUli1bZu0yPhK089xwKDgREZFe8jzPTYUKFcw6Dh8/fhzff/89Fi1alO+CPWo4WoqIiKhoyFPNDZnTNEsxuCEiItINgxsrybRDMZuliIiIChWDGyvJtM8Na26IiIgKVa763Dz//PNZbo+JiclPWR5pGR+cqWJwQ0REVKhyFdy4ubllu33gwIH5KtCjShPc2DC4ISIi0kuugpvFixcXVDmKBYMBEOFQcCIiIj2xz40VmUZMGVlzQ0REpBsGN1ZU4r8KGwY3RERE+mFwY0WmfsSpBg4FJyIi0guDGytSgxsbDgUnIiLSC4MbKzI1S2lqbhjcEBERFSoGN1ZkqrlJAYMbIiIivega3AQHB6NFixZwcXFB+fLlERgYiLNnz2a5z5IlS2AwGDSLo6NjIZU4a2pwY+BQcCIiIr3oGtzs3r0bQUFB+OuvvxASEoLk5GQ8++yzuHfvXpb7ubq6IjIyUl3Cw8MLqcRZY7MUERGR/nI1iZ+1bdmyRbO+ZMkSlC9fHocPH0a7du0y3c9gMMDLy6ugi5drbJYiIiLSX5HqcxMbGwsAKFOmTJb5EhISUKVKFfj4+KBnz544depUYRQvWxaDGzZLERERFaoiE9wYjUaMGzcObdq0QYMGDTLNV7t2bfzwww/YsGEDfvrpJxiNRrRu3RrXrl2zmD8xMRFxcXGapaCcPq38jIjkUHAiIiK9FJngJigoCCdPnsSKFSuyzOfn54eBAweiSZMmaN++PdauXQsPDw8sXLjQYv7g4GC4ubmpi4+PT0EUX2PuAjZLERER6aVIBDdjxozBxo0bsXPnTlSqVClX+5YoUQJNmzbFhQsXLG6fOHEiYmNj1eXq1avWKHKW2OeGiIhIP7p2KBYRjB07FuvWrcOuXbtQtWrVXB8jNTUVYWFh6Nq1q8XtDg4OcHBwyG9RcyUZHApORESkF12Dm6CgICxfvhwbNmyAi4sLoqKiAABubm4oWbIkAGDgwIGoWLEigoODAQDTpk3Dk08+iRo1aiAmJgYzZ85EeHg4hg0bptvryIg1N0RERPrRNbhZsGABAKBDhw6a9MWLF2Pw4MEAgIiICNjYpLWe3b17F8OHD0dUVBRKly6NZs2aYf/+/ahXr15hFTtbDG6IiIj0o3uzVHZ27dqlWf/iiy/wxRdfFFCJrINDwYmIiPRTJDoUFzeaPjesuSEiIipUDG4KAJuliIiI9MPgpgAwuCEiItIPg5sCwKHgRERE+mFwY0Xz5ys/WXNDRESkHwY3VnTokPKTwQ0REZF+GNxY0X9zEHIoOBERkY4Y3FiRwaD85FBwIiIi/TC4sSJTcMNmKSIiIv0wuCkADG6IiIj0w+DGiu7dU35yKDgREZF+GNxY0Y0byk+BDURto2LNDRERUWFicGNFEyakW7H7r2mKwQ0REVGhYnBjRU5Oab+L3X9NU2yWIiIiKlQMbqzIwSHtd7FlzQ0REZEeGNxYkY9P2u+pBgY3REREemBwY0UNGqT9/jCFwQ0REZEeGNxYUcmSab+zzw0REZE+GNwUEKMNa26IiIj0wOCmgDC4ISIi0geDmwIS94DNUkRERHpgcFNAEh6y5oaIiEgPDG4KiPrwTAY3REREhYrBTQFhcENERKQPBjcFRH0yeGoqIKJvYYiIiB4jDG4KiFpzA7D2hoiIqBAxuCkgDG6IiIj0weCmgKjNUgCHgxMRERUiBjcFhDU3RERE+mBwU0AY3BAREemDwU0BYXBDRESkDwY3BYR9boiIiPTB4KaAsOaGiIhIHwxuCgiDGyIiIn0wuCkgbJYiIiLSB4ObAsKaGyIiIn0wuCkgDG6IiIj0oWtwExwcjBYtWsDFxQXly5dHYGAgzp49m+1+q1evRp06deDo6IiGDRti06ZNhVDa3GFwQ0REpA9dg5vdu3cjKCgIf/31F0JCQpCcnIxnn30W9+7dy3Sf/fv3o1+/fhg6dCiOHj2KwMBABAYG4uTJk4VY8uyxzw0REZE+DCIiehfC5N9//0X58uWxe/dutGvXzmKevn374t69e9i4caOa9uSTT6JJkyb45ptvsj1HXFwc3NzcEBsbC1dXV6uV3cRgUH5+gvfwHoKVlR07gKeftvq5iIiIHhe5uX8XqT43sbGxAIAyZcpkmic0NBT+/v6atICAAISGhlrMn5iYiLi4OM1SkGrXVn6yWYqIiEgfRSa4MRqNGDduHNq0aYMGDRpkmi8qKgqenp6aNE9PT0RFRVnMHxwcDDc3N3Xx8fGxarkz+vZb5SebpYiIiPRRZIKboKAgnDx5EitWrLDqcSdOnIjY2Fh1uXr1qlWPn5EpdmLNDRERkT7sss9S8MaMGYONGzdiz549qFSpUpZ5vby8EB0drUmLjo6Gl5eXxfwODg5wcHCwWlmzY+pzw+CGiIhIH7rW3IgIxowZg3Xr1mHHjh2oWrVqtvv4+flh+/btmrSQkBD4+fkVVDFzxea/K8rghoiISB+61twEBQVh+fLl2LBhA1xcXNR+M25ubihZsiQAYODAgahYsSKCg5WRR6+//jrat2+P2bNno1u3blixYgUOHTqERYsW6fY60jMFN+xzQ0REpA9da24WLFiA2NhYdOjQAd7e3uqycuVKNU9ERAQiIyPV9datW2P58uVYtGgRGjdujDVr1mD9+vVZdkIuTKYWMNbcEBER6UPXmpucTLGza9cus7TevXujd+/eBVCi/CtXTvnJ4IaIiEgfRWa0VHHDZikiIiJ9MLgpIKy5ISIi0geDmwLC4IaIiEgfDG4KCIMbIiIifTC4KSDsc0NERKQPBjcFhDU3RERE+mBwU0AY3BAREemDwU0BYbMUERGRPhjcFBDW3BAREemDwU0BeOklBjdERER6YXBTAIxGBjdERER6YXBTAIxG9rkhIiLSC4ObAjB2rLbm5vwZ1twQEREVFgY3BaBdO21ws3c3gxsiIqLCwuCmgKRvlioBNksREREVFgY3BSR9zY0dWHNDRERUWBjcFBAGN0RERPpgcFNAMgY3Fy4AIjoWiIiI6DHB4KaAZOxzU7Mm8PzzOhaIiIjoMcHgpoA8QEn1d3fEAADWr9enLERERI8TBjcF5CFK4joqAABq4ZzOpSEiInp8MLgpIO3aAf+gDgDAA7dQBrd1LhEREdHjgcFNARk4EDiL2up6bZzVsTRERESPDwY3BaRevbSaGyAtuOFjpoiIiAoWg5sC4uenrbmpg38AAJUr61UiIiKixwODmwJkqVkqKgq4cAGYPx9ITNSrZERERMWXXfZZKK8qt6mMB/scURIPNX1uatZUfp44ASxapFPhiIiIiinW3BSg4Bk2OIdaAIAauAC7DA/Q/PZb4PRpPUpGRERUfDG4KUA2NmlNUyWQgqq4bJZnzZrCLhUREVHxxuCmANWsaXnEVHp83hQREZF1MbgpQOXKWR4xld6ePYVZIiIiouKPwU0By24ivx07CrM0RERExR+DmwKWm1mK164FqlUDDh4s6FIREREVXwxuClgCXNQHaFpqlgKA8HDAaAReeAG4fBl47rnCLCEREVHxwuCmgDk5pdXeZPYATV9f4H//S1t/8KCQCkdERFQMMbgpYAZD9iOmAGDlysIqERERUfGma3CzZ88e9OjRAxUqVIDBYMD69euzzL9r1y4YDAazJSoqqnAKnAcGQ96eDi4CbN4MJCQUVMmIiIiKJ12Dm3v37qFx48b46quvcrXf2bNnERkZqS7ly5cvoBLm39SpuQ9uYmMBd3ega1fAxUV5TMPu3QVYSCIiomJE12dLdenSBV26dMn1fuXLl4e7u7v1C1QA3ngDSDhZB1isrGfWqTijuLi03xs3Vn5GRAA+PlYuIBERUTHzSPa5adKkCby9vdGpUyfs27cvy7yJiYmIi4vTLIXJYAAmf1cZD+AIIOfNUpaMHw+cPw/cvKk8WZyIiIjMPVLBjbe3N7755hv8+uuv+PXXX+Hj44MOHTrgyJEjme4THBwMNzc3dfHRo+rDJu0BmtVx0ewBmjm1ahVQqxbg6ak82sFgAFJSrFlQIiKiR59BpGg83chgMGDdunUIDAzM1X7t27dH5cqVsWzZMovbExMTkZiYqK7HxcXBx8cHsbGxcHV1zU+Rc2WVoQ/6YDUAoBbO4vx/wU5+/fgjMHCgVQ5FRERUZMXFxcHNzS1H9+9HqubGkpYtW+JCFm00Dg4OcHV11Sx6yMuIqZwYNAh4+22gXj3gzTeBkyeVDslERESPq0c+uDl27Bi8vb31Lka20s91k9NOxTk1axZw5gzwxRdAw4bKSKv0Vq9WHu2Q0blzwODByk8iIqLiQtfRUgkJCZpal8uXL+PYsWMoU6YMKleujIkTJ+L69etYunQpAGDOnDmoWrUq6tevj4cPH+K7777Djh078Mcff+j1EnKsoGpuMvPOO8CMGcCHHwIffaSk3b8P7N8P7NoF9OgBtGqlpIeEAKdPA8uWAS++CHh5mR8vORn4v/8DnnoKKMIj74mIiPQNbg4dOoSnn35aXX/zzTcBAIMGDcKSJUsQGRmJiIgIdXtSUhLeeustXL9+HU5OTmjUqBG2bdumOUZRFXqrFlBO+b0wgpuZM4GzZ4HffktLS0oC/P2V3z/+OC39xg1gxAilw/KCBUoQM3EiMHo0MGWKEgQ5Oipz9lSpAly5kvW5o6OBZ54Bhg5VmsqIiIgKU5HpUFxYctMhyeoqVgRu3MC/KIfy+Ldwz21FCxYogdCGDcokgxkFBQFff638/nh9uoiIqKA8Vh2KHym10x6gWRp3dC5M3o0aBezcCXz+OZCYaB7APHxovo8IsGULUFSflJGUpDS9ERHRo4/BTWGqk9apeDS+1rEg1nHqlNJcZWMDLFyodGg+fFgb7MTGKk1c770HdOkCVKtm+VipqUrApMeztFJSlLmDfHwAo7Hwz09ERNbFZqnCFBoKtGkDiMBosIG/hGAnntFkcUY8RuNrxMEVv6AfYuFeuGW0gl69gHXrlN9r1lRmVU7P9Ik7exa4exd48kmlz8+332q359etW8DixcDLLwNZDagLDwd8fZXfExKAUqWsc34iIrIeNksVVX5+Sq9cADZixG+l+sEbN9TNvriM/WiNGXgXCzAa11ERCzECTXA036eujgsYg3mYjTdRFZfyfbyspG96yhjYpFenjnJJduxIC2wApYkoOw8fAtOnKw8VzUy/fsqosYAApWZo1Chg6VJlLiBLQ+OJiKh4YM1NYTMalcd9b90KAPgTT+EZ7IAfQvErXoAHblnc7SCa4wQa4QYq4AYqIBLeOIxmuIbMHicheAY70AP/h67YhFpIizIuoDoa4zjuo2hWUVStClzKJv6aOlUZyQUoNUBVqwIlSmjzGAxpv69ZowxzT2/PHqBtW9bcEBE9ClhzU5TZ2AA//QRUqgQAaIu9uNqoG7bBXw1szqIWvsYoxCFtKFILHMJQ/IBJ+BgLMBrr0QtX4ItleBn1cVLNZ4ARgViHw2iG7fDHOHypCWwAoAYu4hO8XwgvNm8uX7acvmuXEsT8/jtw6FBaeu3a5hMXZpRuRgFVVrU+OSWiNK0REVHRweBGD+XKKdMG2ynTDHmdCIH9fw/T3Ipn8ST+QhC+RkVcx6v4BsfRyOJhbGHEy/gZJ9EQ6xCI4ViEo2iKdXgeT6RrykqBLXahPT7EFPXp5K9hLtpgbwG/0Lx7+FB58vmSJUon5QsXgKefVubY6d4d2LhRm//+faWmZsgQ5ZEU0dHa7RnXgcz79mzdqrw9lkZ9ZWRjA5Qpo7Y2WrR3r9JZecMG4OpV4JVXgOPHsz5uaqrymoiIKA/kMRMbGysAJDY2Vu+iiHz5pYhyjxUB5HOME1skp09Sl3K4KY1wTAKwWYbge/kU78pNlDPPmG45iGbSF7+IK2LU5DcwW91+FjWlJO5ldYhHdgkM1K5/8EHmeTdtSvv91i3ttt69RTZsUN6uhASRpUtFgoJEnn5a5MIFbd4HD5T9mzQRWb067W0uWdLyeU1SU0X27FGOb9KkiZLn7l3LH53UVJFXXlE+Qjlx9arIkiUiiYm5+oQSERUZubl/M7jRk9EoMnmySOPGIosX5/oG7oQEeR1fyFVU1GwIRSvpgt8FMJrtY4MU2Qc/NWEW3tQ9EHkUlrffVn6WwS0Zg7lSFRez3Sc6WuTff0UMBsvbjx5VPgaff56WtmuXNub9+GORlSvNPzpbt5oHSVkpXVrJ+8ILIg8fWusDTERUeHJz/2aH4iKkTh2lc2xu2SMRL+MnPIm/sAYv4g88C8CQaf5aOIvjaAxHJMIIA57CXoSidd4L/hjZiQ7ogN0IR2XUwjkkwSFfxxPRdnzOyosvKs1lIsAnnwCTJqUdw5LTp4H69ZVh8JGRaelPPKE09V25AvTsqTwiY9Ag7b6JiUrzWefOwIABuX5ZRERWl6v7d4GHWkVMkaq5ySA1VeS558z/w580yfo1EePxmbpyFjWlGi7oXjtS1JemOKxJ6I9l+T7m9u25y3/7tsi332rT9u5VKgHj4pSmrfr1RcaNE/H2zvw4Fy6IdOmStm6SmCjy6qsiHTuabyMi0hObpbJQlIMbkzlzlJuKnZ3IX3+JJCVZ/0ZtgxQJRSs14QEcZDKmiAMe5PvYdXFKPsW78ju6SCds1T0osdayEMM1CQfQXCw1/emxVKyYu/wff6xdN5k/3zyvtfz0k8hLLyl9k0REUlJEFi8WOXtWCbaSk613rvSMxoI5LhEVrtzcvzlaqgh67TXl0QYPHihP5C5RQhnK3KUL0KkT0LRp/s9hhC0GYBki/psnxxGJmIopOIkG6IJNcEY8bJGS4+N54wbG4QscQjOcRn1MxHR0xWZsQE80xZH8F1hnrojF/7Bck9YCh9AKf+tUIq3r13OX39IzvuLjlSfEZyYlBRg3TvtE+b//VkayZebSJaBvX6UZ7OWXgRUr0h6qumSJMrqtdm2gRg3lcx4YCNy+nfPXcetW1qPali1TmuUOHsz5MQvKL78of8+WpiUAlJF0IoVbpqzweWv0SCuEYKtIeRRqbrJjNIo0bGid//idESefYbwkw9ZihkSUkLtwkzOoLWvwvEzBZOmNldIGf8oYzJWf8D+5iKpZnuQKKks53NS9diM/y2ikVWlcRhX195/RL4/HNEpdnBJH3Nf9tWW3iIiMGKFNW7dO5Pz5tPUjR0RatRIZPVrZZmLpeBMnKtuGDLG8fdiwtP0TE0VCQ5VanowiI5X85cqZb/v0U+0xK1fO/O/p2jVlVNyePbn8Q/zPkSNKJ/DU1MzzxMSkleW557TbjMa0be+9l7cy5NWJEyIBASIHDmjTU1JEypQRKV8+69dFVJjYLJWF4hDciChV+5Mni+zfLxIWJtK0af5uYPURJrvR1ip3wwNoLmPxpabZazueznSYuzUXV8RIH6yQHzBYvsDrVhrqbpQw1FcTmuOAOgw/CXbijeu5Op4NUuQX9BUBZB/8CuW65Ge5dCn3+4SGKjdFS9vc3ZXPr4uL5e2mm390dFrahAkin30m8t13aX8DK1akbTcZP17Jm/GY3t7mf0P//KMEFp07mx/HJCdfE6Z9f/hBWY+PF/n5Z+0wfmfntHwdOmj3T0jQlrUwlStn+bzXr6el37mT++MmJVluDgwNFalRQ+T//i/3xzx2TLmujyqjkU2k+cXgJgvFJbixJGM/irzcxPvhZ/kVveQP+MtetJbDaCr/oJbch2OmO96Ho+zBUzINH0htnFE3VcA1uQEvNd9svJFtGcojSrxwI8dlLoNb4od9MgZzZSs6SSJKaDLk5JzZLW3wp7qyF60FEPkI76tpUzEpV9d4EYZpEkfhK4t5SyFeXsF3Uh3n8/0aCnspl/UUTNkuu3aJVK9ueVtEhHma0Shy+HDWx9y/X6RrV5HWrUX+97+09Fq10n4/flwZKn/5skhIiJLWqVPWf3emfV97TVnv109Zf/ppZT39PEqm9BkzRD75RNkeH6/dnlfx8crggxMncpY/fQADKPudPats++STtPTbt3NXjjt3lGCuZ0/zbW5ueXud6Wu3QkLS0hMTldrCX34RuXkz58e7e1cJju/dyzxPbKwyr1VMjPZ8mbl9WyQ8PPPyd+wo0rZtwQc4EREiAweKHDpUsOfRA4ObLDwuwc3gwda9WdkgRWrgnPTEOnkPH8tXGCVBmCfNcFDskJTpfq2xVxNwZDXCaDgWqnn3wU/G4st0gY5RauOMDMH38i2Gyn48KbdQJtuCJ8NW6iMsX6/9J/zPrPwVcE1tyouGh9jjYY6O9QkmmiXeQhkpjdua5BJIVGu+bsBLSiHequ9ncVvSBygFscTHKzclOztl/csvlRqYa9e0+eLitOs//pj1cW/fNt/HdPNLTFRqK27fVprwPvxQxMtLaZI23chNeZOTlUkdTccw2b5d6SRuNCr7GI0iV64o8y9lVqYHD7Tr4eEi33yjHN/URJXVDfqrr9L2XbtWZM2atLmVTNfPVMYtW5QmsVu3lEDCFFyld/u2iI9P2n6ff562LcM8qHLkiHbfS5eUySszdlZv107JP3hw5q+jRw8lT5cuyvqZM8r6qFGW85vKEBWlTU9OVgIl0/bNm0V2707bbjSaB03x8SJffy1y40bm5cvMU0+Zfw5Mtm4VOX06Z8dJTlY+35k5dUpk5EhlgtDCwuAmC8U5uFm2LO1Dfe+ettoeEOnbV58bz6tYoK48gIO8g+lSAonqdlsky5cYa3HnVBjkAJrnKJARKP1h5uA1zcimXWgnOR3VZIMUTd5yuCkPYS8CyL8oqxlNZmpaEkAG4EcBRBrhmCzEcDmD2vIbuktf/KI2jY3D55rXdRhpbYlzMUZTjll4U1OwtzBTl/eOS8Eun35quZbr5ZeVfkKZ7TdokDLSzN5eZPly8+Du3j1tbYe1lnXrlEklS5cWmT3bcn+czPZ9882cneP6dWWCS1NANHWqdvvs2WnnGj/efH+TO3fS0rp3t1xGg8Hyd2lKivkxBw40P8epU0p6WFjatp9+UoJKo1Fk40YlLf1EnaZl61aR/v3TyhERkXZcU3+06tVz9t2fXpl0X5UPHihTRaSkKIFfxvJnxRQk7dunBFsXLyr97FJTRXbuTDuWn592v5AQ5T0qiBoqBjdZKM7BTUqKyPvvK/8NmSQlKf91nT+vrGf2hRccrPxHVzBf4uZNMf+gljyLLeKOO7IVnTTbsuugbFrC4SMh6ChfYZS8hjnSACfEFJg44IFcQDU1bz/8nOmh3HBXRuJrOYDmatk+wDSpiovyNmaoGT/DeM1+ftinrpxFTfkTbSyeIA7O8hu6a9JG4SupgGuSACcRaGuYemCD2TFuwOuR6HzMpegs1apZ/5iW+vYZjcp3TP/+yn/y1jyfp6d536zy5ZXvq0OHLO+TnKwENmPGaNN//10JlE6d0qb376/UyhiNSq1LtWrmHegzLgkJ5p3WMy6rVmW9Pf08U4DSVywsTFtLBSjf2xERIs2bizz/fOZNaffvi/j7a/dt1Cjt2OnnS8tKVJQ2MBwyRMTVNW0947QTjo7a/U3p6e9D1sLgJgvFObjJqaVLlQ9f+pqe+fOVbRn/ALN6JlNuFns8lLkYIymw0Wy4jdLq74koIa/gOwFE6uGkTMUkOYcaIlBqTTagh0xAsLTFbnFCQrbn7IqN6sp1eIsLYtNtN0pb7JaleDnL/kQP4KD+XgPnMmw2ykE0y/XFmIwp6up7SGtLDEFHqYwrmmsSgUrq76Mx3yrvBRcuxXlp3Fj/MgDKo05yk3/UKCVwy5g+bpx2ffRopalu5kyRSpWU/mEi2o7x2S1Go1ITA4i0b6/sf+OGiK2FQbMZn9NnaTl4UKmtuX07La1lS+vfuxjcZIHBjcJU5Wv6T8zUtnrwoBLQfPGF0jEyY/t7fpcmOCJ70dpsw02Uk7bYbWEfo7jhruR1srz1SJvyWXmOllG64zfZjyct7nAJvhbTt6KTxeO/jKWahJOoJ6PwlbjhrnTADvkWQ+Uu3NTt8xCkeS2OuK85ZzjS/m1bg+elMY5qtqVvzivspQouy1h8KU1wpIDPZZSXsVSOo6H8gr7ig3DdXjMXLo/Ckr6jdk6W9H2fCnKxNgY3WWBwo5WSkv1wV9MHtW5dpTq2Zk2lDduUnpKitJPv2KF0CMz+Q6/cvCLhKQJIGOqLLy4VyB+XLy6pNTPJsJUTaGCW6TZKy5cYK41xVAARH4TLO5iuDv9Ogp10wI5MX8tEfCIL8Kq0x06xFIQ54IF0x2/yHNaLAalm25/HGrMDX0TV/4I6kQ3ooaabarZytxilFv4x67Sc06UuTsmPGKB2oH4Ah3x30s5sqYwrsgnaf0HjUUrewGyrDZlvgBPyCr5jJ+18LOVwU/6Hn8QVMbqXhUvRXayNwU0WGNzk3uTJygd12zZt+tmzStVmRufOWf6gV62a1iY8ZIhIx1bx0hp7NbURqalK1ebEidarXp6MKRY3nEAD6Y9lWTxyQgkKCv65W0bZjqfVhESUkGY4qG5vib/UbedRPdc3+UlQemQmw1Z2oIO8hjlSBZfV7Q54IBVwTRriuLTGXumErdIT66Q/lsmv6GXxoMfQKMcjxHKyGJAqQZgn8SiVaaYjaCIt8He+zlMd59VzHEZTKYt/C/i9LX6LLZLVfxL2wc9iwF5UFz1rPh/HxdoY3GSBwU3e3L+f+32MxrTh6UuXKmkpKWnPy7p3TxkqOmuWMtzU1FRmcvKkdf7AHHFfziNt0pRQtJLu+K1IfSk3wAn1pmupb036Ttf/w085Pm55RGXap+gGvNQOzTlZbqGMZnbmjB2sc7pUxFWZhyD5A/5yCE/IJfhKHJw1ma6iovTGSpmHIEmFQbNtD56SIMwTT0Tm6rwGpMoePKVJPIEGuT6OaXFCgrTA3/+NsNP/M1RYy2D8oEl4Eat0L1N2iwMeyCIMk2TY/tc0nPN96yNMnsIeKSrPkXuUFmtjcJMFBjeFL/0kWLn10UfKxFfp/2BatFA60u3Zk/M/soq4Kqf6fyKnv9pRZL+kquCyNMIxi9vaYZe6cgp1pSz+zXJ+IdOSfl6dGLjmqWDX4S1vYLaUQrw0xHF1aHwqDFk011lePBGpGcVmaVmAVzXNHS3xlxxFY7N8KbCR7XhaBuDHHF2L1/GFxQ3/oJZUxNVcvY4KuCZXoIzV3ge/x6ZfkB2SzPqlnUXNHF3/glwMSJXyiLIYaFbANfkbLTSJOa2NbY29kgSlg8oqvCjuuKP7e/AoLdbG4CYLDG4eTWvWKPMpmEYGpGea2n77duUZQYBIvXrKfBxP/tdvOCAgLf+mTSIeHmm1SsOHK0uL/77/TM8hSj/9v/6L0azWQaD0R7mCyvIhPjTbxxUxakCTiBJSEVelDk7Lu/hUQtFKbsBLTqKe7EZb+RW9ZBGGyWy8IdPwgUxAsIzBXOmFX82an97ELHUlApVy/IXvhrtmQUoybCUaHnIGtWUzAv7rt2S+ry2S5TXMkdOoY/HgV1BZRmN+psPla+KspgZrMH5QgxOB0scpfVNdVktJ3JNDeEKTeBulpTt+KwKfk4JdhmGRxQ3DsKjQy+KAB9IVG2UhhqszoUegkkzDB1IVFwVQpmtIP0u6afkEE7M9fgkkyknU0ySGwyeTgQ95W6zxeBhXxMibmCVNcVj3z0fGxdoY3GSBwU3xc/u2+YP/smOaYCrjJGT37mlnM71+XXmYoemP9exZ5fk4f/9tPk9F+qVOHWXuDGsGSJ2wNcsMGZurJiBYXfkWQ61WDgNSNX2EcvLwUEfcl11opyZcQeX/bkC5rUUzSgOckI/wvjpNQPolEp7yDqZrHtRqgxTNCL05eE0ApfNy+ubKKJSXl7E0yzIZkCqrkfkY31l4U7xwQ9pjpwRhnizAq7ICfQpghJlRmuNAoT6awx4PNQHha5ij/n4NFcxu1BVwTYZj4X83XWvVlhrFD/tkOV7Ksn+WAPIn2mhmR7+MKmotTCQ8s61teheWJ7JJgY1Mwwf5rq2ahKmSBDvZifa5euRMxsXUAf8h7KUrNlrMMxTfylVUlHXoKZUQkedrPwyL5Cxqyo8YkKMO+dbG4CYLDG4oLx48UKbJz2j48LRp6c+cUeYL2rpVm2fxYvM/+uwmCQOUEWk//JD29O0uXUQGYbH8hP/J/6Gb7EVrTU1GHJzVm50j7quj0VJhkJo4a6Wbi7JUQoTcgbuaMAlTM+3DZItkzZD8mygntfCPFcphlDb4Uzaiq9nGJNjJOvSUXvhVc5M6hxqaOZK8cd2sNmgvWmf6X/A0pE38FAsXaYM/ZQ2ez7awMXCVVgjN8rXk9HV74YY6KWQS7KQzNln1vc1sGYmv1ZX/QzcBRNahp5r2Nmaoebtio+bzcQm+Mgtvih/2STnclBb4W/riF3kPH8sXeF3+h5+yrAG0RbL0wQrNw3jTL/dQUkLRSh3Rl3HZjqelLP7VBKaBWJvp+arhglrTlwIb6Y7fZAc6aDL9hu65et/SL8OxUJMQgUrqaM3cLAHYrEl4CPsMnwej5jl4ps/iICzOVdnL4JasRaAm8SCaSXlEZbmftTG4yQKDG9LD1nSVLlOmKGm//y7Sq5cy6yhg/jwwS3bs0D70ERBZgrR54Q+guZRAouZGtBK9rXJzy7i8hOWahN/RRcrgliaPD8JlJXqrCXFw1owEs9bSBEdkJXqbdT5Ov6TCIG3wp9mmcripuUmb8i7CMHkJy8UP+6QirmrmNEqBjXTB7+oNZAzmmj20NeMSCxd5Evs1yTVwTv4P3SQRJeQO3OUf1JI/0UZ+RS/5CO9LK4RqgsbeWGn2KJIYuEodnC6Q99i0OOCBXEXa1LRP4JAAymSbpmt+B+5SBrdkKibl6SRJsJNteEbGYK4EYLOMwDfyKd6VX9BXM/+TafkXZeV7DJHnsF6tNfLGdZmAYDmLmmq+LzFWrWVJX/u5CZ0zKYpRtuBZNcH08F0bpMgEBKu1P4KsZz7PbOmIEItBWDxKSQ9syPFxbJFs1mwmUKZqCMBmsUOSLMagTA+wAT1yVGP0DLbJNVSwuPEiqlr4x8ko1XFemUTVyhjcZIHBDell/HilP0/GpywnJioP0ktMVDpKm74ksmLKExAgcu5IvDL50H+JX+B1zSMssmoSSUrK3xO838PHmoAiHD7SCqHSEMdlKV7WfIk/hL08g215PldOllr4R6bjHYtfxtk9If5ZbJF/kLMncL6OL8ySm+GgrMHzsh1Py5cYK8OxUNrgTwlBRzVTLFzED/vEAQ9kMqZoZsDObIlCefkeQzRBogCa2b7Po7pZYGm6AWY3/PkJHJK6OJVlnrFIe0LlOvTUbEt/AzXVFpqWTegsf8A/0xqVvCxH0VgGYkk2UxEYpQX+luY4oEk3IFXtEJ0Kg1TGFbN9++IXzec5Y/PLc1ivrlyHtzgjLsfFr4tTmkk9F+BVTW1UKgwyHp9JTmpVRuAbdSUUrWQF+qjrD+Ag++CnOe5bmKn5R8j0efwD/jILb8ogLJbmOCAdsENewXfyEd6XNXhe8/f9L8rKKHylCTb/RVnphK3SGytlIYar1zcepbJ+jHoeMLjJAoMb0lN2D5Ob9V9f3a5ds8534IDIL7+kSzh8WKSEec3BZgQIoOQ9ckTpmN27t8irr4ocP6495unTypD/1FSRu3eVOYkmTBD55x/tYRs0EPH2TlvvhK1yE2kRkqUbWQKcpBd+zdP9rFTWXSssLjZIkU7YKsvQX26jtGxFpxx13iyBRBmPz8yGpqdfvsGIHN18TEtJ3JM/kPbQnzg4m/UXikJ5OY/qOR7RthK9pTKuyBE0UdN2oIMayPjikizAq/IQ9nIRVTOdJPN/+Em9+VkK2ABlyHv6TrkZm08q44o6gi79Z0B52KtynUrjtgzCYlmD52UrOskCvCpvY4Y8jzXyLLbIbLyR5Si6FNjI/6GbPI3tubr2lpb0jzyZikmabW64qwnQnsN6i8dIP7FmTqdE8EC05p+O9XhObJAijrgvy/GSJvN6PCceiM70WC6IlSikPavBD/vEFsmyCi+aZX4AB83f3nNYbxaE5mT5A/7ijesCKP2pjqNh9vvt2ZOXr8lMMbjJAoMbKspSU5XAJeOcPzkyZ47Zl8vN1bvMJl/MixUrRKZPV5rUEhNFbt1SJnVMSvovw7VrkuJnPprrtqGMfIgP5fTum9IwB9+FGRdTh+/Ll5Vaqvzc1HKzlMNN6Y2V8hZmypcYK+vQUw6imXyNkXnqSOqI+2YPiBUoTTEz8LamdsABD8QXl+QVfCfr0FMzF9FtlJaXsFxMN3gfhGtuVEvxsizGILMAcwc6mPWJ8kSk5jlmAqWzdfrh1E1wRNP0sRovWHx9n2OcuhIJT2mHXXm47kapjzCZgGD5BBNlOBZKJ2yVGjhn1QkjvXFdvT5XUVGdFNMVMfJ/6KZmXIvATI9RFRfVWrck2GVb82WPh5pO7YfRNEONkNFsstFoeGQaXH2MtFEO6Zud7ZCk6QN2G6UtNsWWxb/yHV7R/FOS2RKPUvIWZpp9flwRoxlYYFoewl624Rmlr1t4eP6/fNJhcJMFBjdUbBmN2udi+PllX1VkTUlJcuDpdyQJdspkf3PnKs/rSKd9e/Pvz4yzXJvS1641P8XPP+f9ppazR4OIjB0r8tln1rmRpl8ccV82Iy1C+xNt/nuSffb7dcHv8jq+sNhH4knsN6s5sbSMwVxNUmYdodfjOXFBrLyLTzX9iOJRKtO+PS6IlZ/RT5ahf75G/hTWkr6PVXf8Jk1wRDNyLg7O2Y4q+hAfqish6ChZ1SjNwNvqyjVUkAq4ZjFfD2yQaHhoEr/HkP9qTNICWlNn54ewV4e9mxY7JMk0fCC/oG8O+mIZxROR0glb5U3MkkUYJjPxlozGfOmMTVIbZ7IMLO3xUObgNdmL1vIZxsuz2KKpIbU2BjdZYHBDxdq//4o88YSIu7syFXQhe/hQZOSABFn7a+ZB1T//KM1fKSmWtx87JrJ3b9bnCQ1N+4JNTFRqkbK7oYkonbdr1dKOVjt0SGm227tXOw2ApSc0W1qa/fdgeD8/kU8/VeJJo1Gbx8NDZN48kffHP5TkrxbKAOe1FkeXDRqkHKeC5f6bmS79sUyTcAfuMhlTNM8tu4eS6pPtX8BqNT0aHvI6vtB0lM3YLHcYTbOtnXiUlq7YqK78g1qavk934C4B2JztMTI+9DazmZqfxna130oiSqidsTNbPBCtGV1oWm6hjOzBU5qmyJl4S/drmdVibQxussDghoq91NTCrbHRybRpIj/9lLZ+9aoSnJw5o0zQeP68kvbyy+bzICUkKE1sGfsdpRcWlvYl/fnnyjPTjEZlxm1T+vTpme+f/kve0uNLjh8XOXVKpEMHyzeDBw8yv2k895wyNYHRKPLjj0raCHwjx93bysKqwdKrY6wAIq1bi6wqH6Tu+CfaSMuqNzWR25S6KwRQRvGkOGv7/KQabOQTTFRG4I1UZgvfvdu8PIcPm6etW5f2u+t/h61bV+TPP/W/6dogRSJQyXxD8+YSUCvnD/FN37k4ApXEBbGa7aVxWzPK7E3MytFxp001yhB8n2Xfr1soU+RnTLY2BjdZYHBDRDm1ebPIwYPm6RcuiHz7bbo+Rxb066d8wb/6atbnSEhQgpXFi823nTwp8vbbIleuKJ281Zvqc2l5jEaltivj7N3h4f/VRMXHi1RL66xr9PXVHsholE2bRL75RkROnBCjjzISJsXHV+TPP2X1aiWgyejOHaWmaf16Zf2vv5Trkt7MmUptWfoaMRGliXDtWu0EmYDyGmNjlcAtNVVkUYYJkUeNMr+BurmJXLumvBcZt1WrptTITZ2alvbMMyLBwSLn/vehJvPGqmPUzm7p/zcwGkV+/TUt65Ah6Xczap5ifw41xA/71G3pO/iGoKOUcc/6eXbDhyv92Q4dUtZ9cUk+w3jZik6aIEkAGYpvdQ9eAJGdO9Nme2dw85/du3dL9+7dxdvbWwDIunXrst1n586d0rRpU7G3t5fq1avLYkvfCFlgcENEheHBA6W5zJqjYev917d31apc7rhnj4ghwxxAbm7KFNwZxcWJhISY9ZcqKHPnZn0zzDiLuIhyTU37TJuWlp4++ImM1B5jxQqRS5fSHeTmTWUKBQ8Pub9kZZaVnUajEryZnpN3+3ZazVQNnJPEkmk1XqbarineaUO1b6O0EoFJWvm6d1dq/kxB26JFaUHVxYtp+TTTQ8TEiOzfL79+cETKlFEC79Gj07b/+afIU+n69V+7JrJ6tUjZsiJVqqSlz5ih1BxaCkgMBpFJk5SPwc2bStD4wn9zH44Zo9SILlgg4umprTkVUf4ZePbZrN/P/HhkgptNmzbJ+++/L2vXrs1RcHPp0iVxcnKSN998U06fPi3z5s0TW1tb2bJlS47PyeCGiB5VCQnKf/V5anV84w3tXey776xevrxIThaZPVvk6NHc7bd0qUjPntoY7OFDZZLLZctyeBCjMfPOXzkQFaUscvZs2oPsLCzX565R9/n9d2XkX0SEsn7njuVjL1qkBFB9/pu+xsbGvOgiIkFB2mDCaFRef8batoQE5Xr9+GNaWni4yMKFIvv2KX3RgMxj2rt3c3BBRAn8XnxR5P/+L2f5cyM392+DiAiKAIPBgHXr1iEwMDDTPBMmTMDvv/+OkydPqmkvvfQSYmJisGXLlhydJy4uDm5uboiNjYWrq2t+i01E9Gh48ABo2RI4eRLo2hXYuBEwGPQuVfGRkgLMnAl8+CGQnJyW/sorwPff5/mwd+4As2YBAwYAdeuab//qK2DMGOX3/NzNTfsW5Y9Ebu7fNoVUJqsIDQ2Fv7+/Ji0gIAChoaE6lYiI6BFRsiSwbx8QEgKsW1e072KPIjs7YOJE4OBBoGFDJa1+feDLL/N12DJlgE8/tRzYAMCIEcCUKcDevfk6DQyG4vWRsNO7ALkRFRUFT09PTZqnpyfi4uLw4MEDlCxZ0myfxMREJCYmqutxcXEFXk4ioiLJ1RXI8A8iWVnjxsDhw8Dx40CDBoCjY4GerkQJpbKItB6pmpu8CA4Ohpubm7r4+PjoXSQiIirOSpQAmjcv8MCGMvdIBTdeXl6Ijo7WpEVHR8PV1dVirQ0ATJw4EbGxsepy9erVwigqERER6eSRapby8/PDpk2bNGkhISHw8/PLdB8HBwc4ODgUdNGIiIioiNC15iYhIQHHjh3DsWPHAACXL1/GsWPHEBERAUCpdRk4cKCaf+TIkbh06RLeeecd/PPPP/j666+xatUqvPHGG3oUn4iIiIogXYObQ4cOoWnTpmjatCkA4M0330TTpk0xefJkAEBkZKQa6ABA1apV8fvvvyMkJASNGzfG7Nmz8d133yEgIECX8hMREVHRU2TmuSksnOeGiIjo0VNs57khIiIiyg6DGyIiIipWGNwQERFRscLghoiIiIoVBjdERERUrDC4ISIiomKFwQ0REREVKwxuiIiIqFh5pJ4tZQ2mOQvj4uJ0LgkRERHllOm+nZO5hx+74CY+Ph4A4OPjo3NJiIiIKLfi4+Ph5uaWZZ7H7vELRqMRN27cgIuLCwwGg1WPHRcXBx8fH1y9epWPdihAvM6Fg9e5cPA6Fx5e68JRUNdZRBAfH48KFSrAxibrXjWPXc2NjY0NKlWqVKDncHV15R9OIeB1Lhy8zoWD17nw8FoXjoK4ztnV2JiwQzEREREVKwxuiIiIqFhhcGNFDg4O+PDDD+Hg4KB3UYo1XufCwetcOHidCw+vdeEoCtf5setQTERERMUba26IiIioWGFwQ0RERMUKgxsiIiIqVhjcEBERUbHC4MZKvvrqK/j6+sLR0RGtWrXCgQMH9C5SkbJnzx706NEDFSpUgMFgwPr16zXbRQSTJ0+Gt7c3SpYsCX9/f5w/f16T586dO+jfvz9cXV3h7u6OoUOHIiEhQZPnxIkTaNu2LRwdHeHj44PPPvvMrCyrV69GnTp14OjoiIYNG2LTpk1Wf716CA4ORosWLeDi4oLy5csjMDAQZ8+e1eR5+PAhgoKCULZsWTg7O+OFF15AdHS0Jk9ERAS6desGJycnlC9fHm+//TZSUlI0eXbt2oUnnngCDg4OqFGjBpYsWWJWnuL8N7FgwQI0atRInaTMz88PmzdvVrfzOlvf9OnTYTAYMG7cODWN19k6pkyZAoPBoFnq1Kmjbn8kr7NQvq1YsULs7e3lhx9+kFOnTsnw4cPF3d1doqOj9S5akbFp0yZ5//33Ze3atQJA1q1bp9k+ffp0cXNzk/Xr18vx48flueeek6pVq8qDBw/UPJ07d5bGjRvLX3/9JX/++afUqFFD+vXrp26PjY0VT09P6d+/v5w8eVJ++eUXKVmypCxcuFDNs2/fPrG1tZXPPvtMTp8+LR988IGUKFFCwsLCCvwaFLSAgABZvHixnDx5Uo4dOyZdu3aVypUrS0JCgppn5MiR4uPjI9u3b5dDhw7Jk08+Ka1bt1a3p6SkSIMGDcTf31+OHj0qmzZtknLlysnEiRPVPJcuXRInJyd588035fTp0zJv3jyxtbWVLVu2qHmK+9/Eb7/9Jr///rucO3dOzp49K++9956UKFFCTp48KSK8ztZ24MAB8fX1lUaNGsnrr7+upvM6W8eHH34o9evXl8jISHX5999/1e2P4nVmcGMFLVu2lKCgIHU9NTVVKlSoIMHBwTqWqujKGNwYjUbx8vKSmTNnqmkxMTHi4OAgv/zyi4iInD59WgDIwYMH1TybN28Wg8Eg169fFxGRr7/+WkqXLi2JiYlqngkTJkjt2rXV9T59+ki3bt005WnVqpW8+uqrVn2NRcHNmzcFgOzevVtElGtaokQJWb16tZrnzJkzAkBCQ0NFRAlCbWxsJCoqSs2zYMECcXV1Va/rO++8I/Xr19ecq2/fvhIQEKCuP45/E6VLl5bvvvuO19nK4uPjpWbNmhISEiLt27dXgxteZ+v58MMPpXHjxha3ParXmc1S+ZSUlITDhw/D399fTbOxsYG/vz9CQ0N1LNmj4/Lly4iKitJcQzc3N7Rq1Uq9hqGhoXB3d0fz5s3VPP7+/rCxscHff/+t5mnXrh3s7e3VPAEBATh79izu3r2r5kl/HlOe4vhexcbGAgDKlCkDADh8+DCSk5M1r79OnTqoXLmy5jo3bNgQnp6eap6AgADExcXh1KlTap6sruHj9jeRmpqKFStW4N69e/Dz8+N1trKgoCB069bN7FrwOlvX+fPnUaFCBVSrVg39+/dHREQEgEf3OjO4yadbt24hNTVV86YCgKenJ6KionQq1aPFdJ2yuoZRUVEoX768ZrudnR3KlCmjyWPpGOnPkVme4vZeGY1GjBs3Dm3atEGDBg0AKK/d3t4e7u7umrwZr3Ner2FcXBwePHjw2PxNhIWFwdnZGQ4ODhg5ciTWrVuHevXq8Tpb0YoVK3DkyBEEBwebbeN1tp5WrVphyZIl2LJlCxYsWIDLly+jbdu2iI+Pf2Sv82P3VHCix0FQUBBOnjyJvXv36l2UYqt27do4duwYYmNjsWbNGgwaNAi7d+/Wu1jFxtWrV/H6668jJCQEjo6OehenWOvSpYv6e6NGjdCqVStUqVIFq1atQsmSJXUsWd6x5iafypUrB1tbW7Oe49HR0fDy8tKpVI8W03XK6hp6eXnh5s2bmu0pKSm4c+eOJo+lY6Q/R2Z5itN7NWbMGGzcuBE7d+5EpUqV1HQvLy8kJSUhJiZGkz/jdc7rNXR1dUXJkiUfm78Je3t71KhRA82aNUNwcDAaN26ML7/8ktfZSg4fPoybN2/iiSeegJ2dHezs7LB7927MnTsXdnZ28PT05HUuIO7u7qhVqxYuXLjwyH6eGdzkk729PZo1a4bt27eraUajEdu3b4efn5+OJXt0VK1aFV5eXpprGBcXh7///lu9hn5+foiJicHhw4fVPDt27IDRaESrVq3UPHv27EFycrKaJyQkBLVr10bp0qXVPOnPY8pTHN4rEcGYMWOwbt067NixA1WrVtVsb9asGUqUKKF5/WfPnkVERITmOoeFhWkCyZCQELi6uqJevXpqnqyu4eP6N2E0GpGYmMjrbCUdO3ZEWFgYjh07pi7NmzdH//791d95nQtGQkICLl68CG9v70f385zrLshkZsWKFeLg4CBLliyR06dPy4gRI8Td3V3Tc/xxFx8fL0ePHpWjR48KAPn888/l6NGjEh4eLiLKUHB3d3fZsGGDnDhxQnr27GlxKHjTpk3l77//lr1790rNmjU1Q8FjYmLE09NTBgwYICdPnpQVK1aIk5OT2VBwOzs7mTVrlpw5c0Y+/PDDYjMUfNSoUeLm5ia7du3SDOm8f/++mmfkyJFSuXJl2bFjhxw6dEj8/PzEz89P3W4a0vnss8/KsWPHZMuWLeLh4WFxSOfbb78tZ86cka+++srikM7i/Dfx7rvvyu7du+Xy5cty4sQJeffdd8VgMMgff/whIrzOBSX9aCkRXmdreeutt2TXrl1y+fJl2bdvn/j7+0u5cuXk5s2bIvJoXmcGN1Yyb948qVy5stjb20vLli3lr7/+0rtIRcrOnTsFgNkyaNAgEVGGg0+aNEk8PT3FwcFBOnbsKGfPntUc4/bt29KvXz9xdnYWV1dXGTJkiMTHx2vyHD9+XJ566ilxcHCQihUryvTp083KsmrVKqlVq5bY29tL/fr15ffffy+w112YLF1fALJ48WI1z4MHD2T06NFSunRpcXJykl69eklkZKTmOFeuXJEuXbpIyZIlpVy5cvLWW29JcnKyJs/OnTulSZMmYm9vL9WqVdOcw6Q4/0288sorUqVKFbG3txcPDw/p2LGjGtiI8DoXlIzBDa+zdfTt21e8vb3F3t5eKlasKH379pULFy6o2x/F62wQEcl9fQ8RERFR0cQ+N0RERFSsMLghIiKiYoXBDRERERUrDG6IiIioWGFwQ0RERMUKgxsiIiIqVhjcEBERUbHC4IaIHju+vr6YM2eO3sUgogLC4IaICtTgwYMRGBgIAOjQoQPGjRtXaOdesmQJ3N3dzdIPHjyIESNGFFo5iKhw2eldACKi3EpKSoK9vX2e9/fw8LBiaYioqGHNDREVisGDB2P37t348ssvYTAYYDAYcOXKFQDAyZMn0aVLFzg7O8PT0xMDBgzArVu31H07dOiAMWPGYNy4cShXrhwCAgIAAJ9//jkaNmyIUqVKwcfHB6NHj0ZCQgIAYNeuXRgyZAhiY2PV802ZMgWAebNUREQEevbsCWdnZ7i6uqJPnz6Ijo5Wt0+ZMgVNmjTBsmXL4OvrCzc3N7z00kuIj48v2ItGRHnC4IaICsWXX34JPz8/DB8+HJGRkYiMjISPjw9iYmLwzDPPoGnTpjh06BC2bNmC6Oho9OnTR7P/jz/+CHt7e+zbtw/ffPMNAMDGxgZz587FqVOn8OOPP2LHjh145513AACtW7fGnDlz4Orqqp5v/PjxZuUyGo3o2bMn7ty5g927dyMkJASXLl1C3759NfkuXryI9evXY+PGjdi4cSN2796N6dOnF9DVIqL8YLMUERUKNzc32Nvbw8nJCV5eXmr6/Pnz0bRpU3z66adq2g8//AAfHx+cO3cOtWrVAgDUrFkTn332meaY6fvv+Pr64uOPP8bIkSPx9ddfw97eHm5ubjAYDJrzZbR9+3aEhYXh8uXL8PHxAQAsXboU9evXx8GDB9GiRQsAShC0ZMkSuLi4AAAGDBiA7du345NPPsnfhSEiq2PNDRHp6vjx49i5cyecnZ3VpU6dOgCU2hKTZs2ame27bds2dOzYERUrVoSLiwsGDBiA27dv4/79+zk+/5kzZ+Dj46MGNgBQr149uLu748yZM2qar6+vGtgAgLe3N27evJmr10pEhYM1N0Skq4SEBPTo0QMzZsww2+bt7a3+XqpUKc22K1euoHv37hg1ahQ++eQTlClTBnv37sXQoUORlJQEJycnq5azRIkSmnWDwQCj0WjVcxCRdTC4IaJCY29vj9TUVE3aE088gV9//RW+vr6ws8v5V9Lhw4dhNBoxe/Zs2NgoldCrVq3K9nwZ1a1bF1evXsXVq1fV2pvTp08jJiYG9erVy3F5iKjoYLMUERUaX19f/P3337hy5Qpu3boFo9GIoKAg3LlzB/369cPBgwdx8eJFbN26FUOGDMkyMKlRowaSk5Mxb948XLp0CcuWLVM7Gqc/X0JCArZv345bt25ZbK7y9/dHw4YN0b9/fxw5cgQHDhzAwIED0b59ezRv3tzq14CICh6DGyIqNOPHj4etrS3q1asHDw8PREREoEKFCti3bx9SU1Px7LPPomHDhhg3bhzc3d3VGhlLGjdujM8//xwzZsxAgwYN8PPPPyM4OFiTp3Xr1hg5ciT69u0LDw8Psw7JgNK8tGHDBpQuXRrt2rWDv78/qlWrhpUrV1r99RNR4TCIiOhdCCIiIiJrYc0NERERFSsMboiIiKhYYXBDRERExQqDGyIiIipWGNwQERFRscLghoiIiIoVBjdERERUrDC4ISIiomKFwQ0REREVKwxuiIiIqFhhcENERETFCoMbIiIiKlb+H+kDiUaBCG1DAAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":19},{"id":"11644d30","cell_type":"markdown","source":"## Testing the model on a given prompt\n\nWe finally test the trained model by generating text based on a given prompt. The model successfully generates a sequence of characters that continue from the prompt, and we save the generated text to a file for review.","metadata":{"id":"11644d30"}},{"id":"3767fd8e","cell_type":"code","source":"# Test the model on a given prompt\nprompt = \"the meaning of life is\"\nencoded_prompt = fn.encode(prompt, chars_to_int)\ncontext = encoded_prompt[None, :]\n\nB = 1\nseed = seed\ngenerate_len = 1000\nrng = jax.random.PRNGKey(seed)\n\noutput_indices = fn.generate_tokens(\n    model=model_obj,\n    params=params,\n    constants=constants,\n    rng=rng,\n    context=context,\n    length=generate_len,\n    block_size=64,\n    temperature=0.8,\n    sample=True,\n    pad_id=None,\n    deterministic=True\n)\n\noutput_indices = np.array(output_indices)  # Convert from JAX array to NumPy array\ngenerated_text = fn.decode(output_indices, int_to_chars)\n\nprint(\"Generated ID Shape:\", output_indices.shape)\nprint(\"Generated Text:\")\nprint(prompt + generated_text)\n\ngenerated_text_file = \"generated_text.txt\"\n\nwith open(generated_text_file, \"w\") as f:\n    f.write(prompt + generated_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3767fd8e","outputId":"0e84baab-cecb-4ecb-91fd-2232306e9d16","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T08:30:41.518260Z","iopub.execute_input":"2025-11-19T08:30:41.518559Z","iopub.status.idle":"2025-11-19T08:30:44.622788Z","shell.execute_reply.started":"2025-11-19T08:30:41.518536Z","shell.execute_reply":"2025-11-19T08:30:44.622156Z"}},"outputs":[{"name":"stdout","text":"Generated ID Shape: (1, 1000)\nGenerated Text:\nthe meaning of life is told to solve a governor of the last site that could contain since the population of hell the second majority faction bread was then the short man who was in hope that did not allow the effect of receiving the previous from each current general on the leading leader of the motor leader and is true it is also often used for example is the topic of cantor this realizes a text seems to require his downtown cantor jerusalem a film the first appointment of bernard white petition polish man carporators or of the bernard polish court of fame polish in one nine eight seven he became a member of the pipe david saint est extremely important the concept of death terms of the equations are strong concepts that he can include the most important list of microton based on tour center center lists design of information at the country however it is an anchor state research like many akihara the mystery externally recognize the united states in the country who spoke a to the formation of some scholars \n","output_type":"stream"}],"execution_count":20}]}