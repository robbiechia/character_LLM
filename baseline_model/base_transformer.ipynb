{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc4842c",
   "metadata": {},
   "source": [
    "In this notebook, we create a baseline transformer model based on our decided model architecture framework. We will create functions accordingly for the training process, and train this first model as a sanity check to ensure our foundational understanding is correct... \n",
    "\n",
    "after this, we will assess our baseline transformer model and move into the second phase which is tuning of ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import base_functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030d754",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d76b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text loaded. Length: 90,000,000 characters.\n",
      "Test text loaded. Length: 5,000,000 characters.\n"
     ]
    }
   ],
   "source": [
    "# Read in training text file\n",
    "with open('./data/text8_train.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "print(f\"Training text loaded. Length: {len(train_text) :,} characters.\")\n",
    "\n",
    "# Read in test text file\n",
    "with open('./data/text8_test.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.read()\n",
    "print(f\"Test text loaded. Length: {len(test_text) :,} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585d4176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters of training text:\n",
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "# Inspect first 500 characters of training text\n",
    "print(\"First 500 characters of training text:\")\n",
    "print(train_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd37a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters in training text: 27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(train_text)) # unique characters in training text\n",
    "chars_to_int = {ch: i for i, ch in enumerate(chars)} # char to int mapping\n",
    "int_to_chars = {i: ch for i, ch in enumerate(chars)} # int to char mapping\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Unique characters in training text: {vocab_size}\") # should be 27, including space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a969636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 80,999,996 characters.\n",
      "Validation text length: 9,000,004 characters.\n"
     ]
    }
   ],
   "source": [
    "train_text, val_text = fn.split_train_val(train_text, val_fraction=0.1)\n",
    "\n",
    "print(f\"Training text length: {len(train_text) :,} characters.\")\n",
    "print(f\"Validation text length: {len(val_text) :,} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28ac68",
   "metadata": {},
   "source": [
    "# Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19f06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model params\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "mlp_ratio = 4\n",
    "seq_len = 128\n",
    "dropout_rate = 0.0\n",
    "\n",
    "m1, params = fn.create_train_state(rng, \n",
    "        vocab_size = vocab_size,\n",
    "        d_model = d_model,\n",
    "        n_heads = n_heads,\n",
    "        n_layers = n_layers,\n",
    "        mlp_ratio = mlp_ratio,\n",
    "        seq_len = seq_len,\n",
    "        dropout = dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91839933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 419,072\n"
     ]
    }
   ],
   "source": [
    "total_params = fn.count_parameters(params)\n",
    "\n",
    "print(f\"Total number of parameters in the model: {total_params :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc7a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (2, 8, 27)\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK: Test the model forward pass\n",
    "B, T = 2, 8  # Batch size and sequence length\n",
    "batch = jax.random.randint(key = rng, shape = (B, T), minval = 0, maxval = vocab_size)\n",
    "\n",
    "logits = m1.apply({\"params\": params}, batch, deterministic=False)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected: (B, T, vocab)_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22aeee",
   "metadata": {},
   "source": [
    "## Initialise the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d04750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized: Adam with Learning Rate = 0.001\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Create the Optimizer and initialize it\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "print(\"Optimizer initialized: Adam with Learning Rate =\", learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839549c",
   "metadata": {},
   "source": [
    "## Train the model over n_iter iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100000\n",
    "batch_size = 64\n",
    "seq_len = 32\n",
    "\n",
    "# Encode the train, val, test texts\n",
    "train_data = fn.encode(train_text, chars_to_int)\n",
    "val_data = fn.encode(val_text, chars_to_int)\n",
    "test_data = fn.encode(test_text, chars_to_int)\n",
    "\n",
    "# To track training and validation loss, as well as time taken\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_time_history = []\n",
    "val_time_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7cbde26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, time elapsed: 4.19 seconds\n",
      "\t \t Training Loss: 3.8638, Validation Loss: 3.3623\n",
      "\t \t Training Acc: 0.0293, Validation Acc: 0.1606\n",
      "\t \t Last Char Training Acc: 0.0156, Last Char Validation Acc: 0.1699\n",
      "--------------------------------------------------\n",
      "Iteration 1000, time elapsed: 69.62 seconds\n",
      "\t \t Training Loss: 1.7792, Validation Loss: 1.7297\n",
      "\t \t Training Acc: 0.4717, Validation Acc: 0.4747\n",
      "\t \t Last Char Training Acc: 0.4062, Last Char Validation Acc: 0.4775\n",
      "--------------------------------------------------\n",
      "Iteration 2000, time elapsed: 134.43 seconds\n",
      "\t \t Training Loss: 1.6033, Validation Loss: 1.6436\n",
      "\t \t Training Acc: 0.5039, Validation Acc: 0.4966\n",
      "\t \t Last Char Training Acc: 0.5938, Last Char Validation Acc: 0.5078\n",
      "--------------------------------------------------\n",
      "Iteration 3000, time elapsed: 205.59 seconds\n",
      "\t \t Training Loss: 1.5133, Validation Loss: 1.5662\n",
      "\t \t Training Acc: 0.5234, Validation Acc: 0.5212\n",
      "\t \t Last Char Training Acc: 0.4688, Last Char Validation Acc: 0.5137\n",
      "--------------------------------------------------\n",
      "Iteration 4000, time elapsed: 270.61 seconds\n",
      "\t \t Training Loss: 1.6274, Validation Loss: 1.5494\n",
      "\t \t Training Acc: 0.5142, Validation Acc: 0.5226\n",
      "\t \t Last Char Training Acc: 0.4375, Last Char Validation Acc: 0.5479\n",
      "--------------------------------------------------\n",
      "Iteration 5000, time elapsed: 335.48 seconds\n",
      "\t \t Training Loss: 1.4677, Validation Loss: 1.5211\n",
      "\t \t Training Acc: 0.5415, Validation Acc: 0.5289\n",
      "\t \t Last Char Training Acc: 0.7031, Last Char Validation Acc: 0.5176\n",
      "--------------------------------------------------\n",
      "Iteration 6000, time elapsed: 401.33 seconds\n",
      "\t \t Training Loss: 1.4916, Validation Loss: 1.5571\n",
      "\t \t Training Acc: 0.5366, Validation Acc: 0.5231\n",
      "\t \t Last Char Training Acc: 0.5312, Last Char Validation Acc: 0.5195\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# get a batch of data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m fn\u001b[38;5;241m.\u001b[39mget_batch(train_data, batch_size, seq_len) \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Perform a training step\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     rng, sub \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n",
      "File \u001b[0;32m~/Documents/NUS/AY25_26/Year4Sem1/DSA4212/character_LLM/base_functions.py:145\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(text_int, B, T)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# targets are text from i+1 to i+T+1\u001b[39;00m\n\u001b[1;32m    143\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([text_int[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mT\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39marray(x, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32), jnp\u001b[38;5;241m.\u001b[39marray(y, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/array_constructors.py:268\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m out_array: Array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39m_convert_element_type(\n\u001b[1;32m    269\u001b[0m     out, dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type, sharding\u001b[38;5;241m=\u001b[39msharding)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndmin \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndim(out_array):\n\u001b[1;32m    271\u001b[0m   out_array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mexpand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mndim(out_array)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py:1725\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type, sharding, warn_on_complex_to_real_cast)\u001b[0m\n\u001b[1;32m   1723\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m operand\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1725\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_element_type_p\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m   1726\u001b[0m       operand, new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(weak_type),\n\u001b[1;32m   1727\u001b[0m       sharding\u001b[38;5;241m=\u001b[39msharding)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/core.py:612\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    611\u001b[0m   args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[0;32m--> 612\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_true_bind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/core.py:628\u001b[0m, in \u001b[0;36mPrimitive._true_bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    626\u001b[0m trace_ctx\u001b[38;5;241m.\u001b[39mset_trace(eval_trace)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 628\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(prev_trace, args, params)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m   trace_ctx\u001b[38;5;241m.\u001b[39mset_trace(prev_trace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py:4934\u001b[0m, in \u001b[0;36m_convert_element_type_bind_with_trace\u001b[0;34m(trace, args, params)\u001b[0m\n\u001b[1;32m   4932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_element_type_bind_with_trace\u001b[39m(trace, args, params):\n\u001b[1;32m   4933\u001b[0m   sharding \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 4934\u001b[0m   operand \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mPrimitive\u001b[38;5;241m.\u001b[39mbind_with_trace(convert_element_type_p, trace, args, params)\n\u001b[1;32m   4935\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sharding\u001b[38;5;241m.\u001b[39m_is_concrete:\n\u001b[1;32m   4936\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mset_current_trace(trace):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/core.py:638\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_lojax(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, args, params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/core.py:1162\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, args, params)\u001b[0m\n\u001b[1;32m   1160\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[1;32m   1161\u001b[0m check_eval_args(args)\n\u001b[0;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/dispatch.py:90\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     88\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m   outs \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "for it in range(n_iter):\n",
    "\n",
    "    # get a batch of data\n",
    "    inputs, targets = fn.get_batch(train_data, batch_size, seq_len) \n",
    "\n",
    "    # Perform a training step\n",
    "    rng, sub = jax.random.split(rng)\n",
    "    new_params, new_opt_state, metrics = fn.train_step(m1, params, opt_state, inputs, targets, optimizer, rng=sub)\n",
    "\n",
    "    # Update parameters and optimizer state\n",
    "    params = new_params\n",
    "    opt_state = new_opt_state\n",
    "    acc = metrics['acc']\n",
    "    loss = metrics['loss']\n",
    "    last_char_acc = metrics['acc_last']\n",
    "\n",
    "    train_loss_history.append(loss)\n",
    "    train_time_history.append(time.time() - time_start)\n",
    "\n",
    "    if it % (n_iter // 50) == 0 or it == n_iter - 1: # Print every 2% of iterations\n",
    "\n",
    "        # Compute the loss on validation set\n",
    "        batch_size_val = 1024\n",
    "        seq_len_val = 32\n",
    "        val_inputs, val_targets = fn.get_batch(val_data, batch_size_val, seq_len_val)\n",
    "        val_logits = m1.apply({\"params\": params}, val_inputs, deterministic=True)\n",
    "        val_loss, val_metrics = fn.loss_and_metrics(val_logits, val_targets)\n",
    "\n",
    "        # Record validation loss and time\n",
    "        val_acc = val_metrics['acc']\n",
    "        last_char_acc_val = val_metrics['acc_last']\n",
    "        val_loss_history.append(val_loss)\n",
    "        time_elapsed = time.time() - time_start\n",
    "        val_time_history.append(time_elapsed)\n",
    "\n",
    "        # Print training and validation metrics\n",
    "        print(f\"Iteration {it}, time elapsed: {time_elapsed:.2f} seconds\")\n",
    "        print(f\"\\t \\t Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"\\t \\t Training Acc: {acc:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "        print(f\"\\t \\t Last Char Training Acc: {last_char_acc:.4f}, Last Char Validation Acc: {last_char_acc_val:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(f\"Training completed in {time.time() - time_start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22e0a4",
   "metadata": {},
   "source": [
    "## Plot the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "plt.plot(train_time_history, train_loss_history, \"-\",label='Training Loss', color='blue')\n",
    "plt.plot(val_time_history, val_loss_history, \"-\", label='Validation Loss', lw = 2, color='red')\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.title(\"Training and Validation Loss over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11644d30",
   "metadata": {},
   "source": [
    "## Testing the model on a given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given prompt\n",
    "prompt = \"the meaning of life is\"\n",
    "encoded_prompt = fn.encode(prompt, chars_to_int)\n",
    "context = encoded_prompt[None, :]\n",
    "\n",
    "B = 1\n",
    "seed = 42\n",
    "generate_len = 1000\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "output_indices = fn.generate_tokens(\n",
    "    model=m1,\n",
    "    params=params,\n",
    "    rng=rng,\n",
    "    context=context,\n",
    "    length=generate_len,\n",
    "    block_size=128,\n",
    "    temperature=0.8,\n",
    "    sample=True,\n",
    "    pad_id=None,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "output_indices = np.array(output_indices)  # Convert from JAX array to NumPy array\n",
    "generated_text = fn.decode(output_indices, int_to_chars)\n",
    "\n",
    "print(\"Generated ID Shape:\", output_indices.shape)\n",
    "print(\"Generated Text:\")\n",
    "print(prompt + generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
