{
  "name": "Final Model",
  "description": "Training the final character-level LLM with chosen architecture and hyperparameters.",
  "seed": 0,

  "model": {
    "vocab_size": 27,
    "d_model": 256,
    "n_heads": 4,
    "n_layers": 6,
    "mlp_ratio": 4,
    "seq_len": 256,

    "dropout": 0.0,
    "pos_encoding": "rotary",
    "attention_type": "GQA",

    "use_auxiliary_loss": false,
    "aux_heads": 2,
    "aux_weight": 0.0,
    "loss_type": "cross_entropy",
    "mixed_precision": false
  },

  "throughput": {
    "max_test_iters": 2000,
    "max_test_time_in_seconds": 60,
    "compute_budget_hours": 4
  },

  "training": {
    "val_fraction": 0.01,
    "batch_size": 32,

    "learning_rate": 0.0009632654721866886,
    "weight_decay": 0.05,
    "lr_schedule": "warmup_decay",
    "optimizer": "adamw",
    "warmup_ratio": 0.1306142633759742,
    "label_smoothing": 0.0,
    "grad_clip": "none"
  }
}