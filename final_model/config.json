{
  "name": "Final Model",
  "description": "Training the final character-level LLM with chosen architecture and hyperparameters.",
  "seed": 0,

  "model": {
    "vocab_size": 27,
    "d_model": 256,
    "n_heads": 4,
    "n_layers": 6,
    "mlp_ratio": 4,
    "seq_len": 128,

    "dropout": 0.0,
    "pos_encoding": "rotary",
    "attention_type": "GQA",

    "use_auxiliary_loss": false,
    "aux_heads": 2,
    "aux_weight": 0.4,
    "loss_type": "cross_entropy",
    "mixed_precision": false
  },

  "throughput": {
    "max_test_iters": 2000,
    "max_test_time_in_seconds": 60,
    "compute_budget_hours": 5
  },

  "training": {
    "val_fraction": 0.01,
    "batch_size": 32,

    "learning_rate": 1e-3,
    "weight_decay": 0.0,
    "lr_schedule": "constant",
    "optimizer": "adam",
    "warmup_ratio": "none",
    "label_smoothing": 0.0,
    "grad_clip": "none"
  }
}